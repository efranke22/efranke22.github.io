[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erin Franke",
    "section": "",
    "text": "Hi, thanks for visiting my website!\nI am a first-year Statistics PhD student at Carnegie Mellon University in Pittsburgh, PA. I graduated from Macalester College in 2023, majoring in Statistics and minoring Computer Science and Economics.\nI am passionate about using statistics as a tool to better understand and solve complex interdisciplinary problems for societal good. I am currently working with Weijing Tang (CMU Stats & DS) and Phoebe Lam (CMU Psychology) to integrate existing studies in order to understand the influence of psychological stress on disease vulnerability, and whether social relationships, at what age, buffer this association.\nAs both a student and teaching assistant, I enjoy thinking about effective ways to learn and teach statistics and data science. Visit my Teaching page to learn more about some recent projects.\n\n\n\n\n\nAbout Me\nIn addition to statistics & data science, I also enjoy running, volunteering at my local food pantry in Chicago, hiking, going on walks while listening to podcasts, following women’s pro running & the MLB, and spending time with friends and family. This fall, I will be racing the 2025 Bank of America Chicago Marathon for charity, raising money for Nourishing Hope Food Pantry.\n\n\n\n\n\n\n\n\n\nMy siblings and I on a hike in Washington\n\n\n\n\n\n\n\nMy first marathon!\n\n\n\n\n\n\n\n\n\nPirates game with cohortmates\n\n\n\n\n\n\n\nSchenely Park: My favorite place in Pittsburgh!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Erin Franke",
    "section": "",
    "text": "BA in Statistics, 2023\nMacalester College; Saint Paul, MN\nResume"
  },
  {
    "objectID": "projects/statGen/index.html",
    "href": "projects/statGen/index.html",
    "title": "Statistical Genetics Summary",
    "section": "",
    "text": "In Fall 2022 I took Statistical Genetics, which introduced me to the field and got me really interested in pursuing a career in health/genetics related data science. I had a great time learning how to use both R and PLINK to analyze genomic data - you can check out everything I learned here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Undergrad Projects",
    "section": "",
    "text": "I am very grateful to have had many opportunities to work with real data during my undergrad! I personally find (a) applying methodology to data, and (b) making an effort to explain this work in writing or orally, to be the best way to process what I learn and understand what I don’t know. I worked on the projects below at Macalester. I like to keep these projects on my website to refer back to for coding tricks or concept refreshers, and as a reflection of my development as a statistician :)\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGentrification and Crime in the Twin Cities: Insights and Challenges through a Statistical Lens\n\n\nAn independent honors project in statistics completed over my senior at Macalester College.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Oxygen Help? A Causal Analysis\n\n\nAn indepedent project to apply causal inference techniques to understand oxygen use when ascending the Himalayan Mountains.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Genetics Summary\n\n\nA summary of what I learned in Stat 494 Statistical Genetics in Fall 2022.\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounders & Omitted Variable Bias in Linear Regression\n\n\nMy final group project in my Mathematical Statistics class in Spring 2022.\n\n\n\n\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Spatial Analysis of Elevated Blood Lead Levels in the Twin Cities Metropolitan Region\n\n\nResearch using spatial techniques including SAR models, the SF package, and Matern Random Effects Modeling to study where children in the Twin Cities are testing with…\n\n\n\n\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 and Residential Property Crime in Chicago, IL\n\n\nMy econometrics project seeking to understand the impact of the COVID-19 pandemic on residential property crime in Chicago’s 77 community areas.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTommy John surgery and its Relationship to MLB Pitcher Career Trajectory\n\n\nResearch using Bayesian techniques aimed to understand the impacts of Tommy John surgery.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing wOBA with Launch Angle and Exit Velocity\n\n\nSummer research project with the goal to understanding how individual MLB players should swing to maximize their weighted OBP.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUS Universities and the SVD\n\n\nHow we identified a few very unique colleges using the SVD and Forbe’s data set on America’s Top Colleges of 2019.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA survival analysis of draft to debut date for MLB players\n\n\nA survival analysis project completed by Erin Franke and Corey Pieper in March 2021.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy first experience with data analysis for the community\n\n\nWhat I learned during my one month internship working with Common Pantry of Chicago, IL.\n\n\n\n\n\n\nJan 31, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/lead/index.html",
    "href": "projects/lead/index.html",
    "title": "A Spatial Analysis of Elevated Blood Lead Levels in the Twin Cities Metropolitan Region",
    "section": "",
    "text": "In Spring 2022 I got to take Correlated Data (STAT 452) and learned about how to work with time series, longitudinal, and spatially correlated data. I enjoyed combining my passion of mapping with modeling techniques that account for spatial correlation in order to learn more about the critical issue of childhood lead exposure in the Twin Cities, which can cause damage to a child’s brain and nervous system, slowed growth and development, and even comas or death with severe exposure. To learn more about what might cause elevated blood lead levels in children and where this issue is most apparent, check out our writeup on github."
  },
  {
    "objectID": "projects/econometrics/index.html",
    "href": "projects/econometrics/index.html",
    "title": "COVID-19 and Residential Property Crime in Chicago, IL",
    "section": "",
    "text": "In Fall 2021 I took Econometrics (ECON 381) at Macalester College. With the COVID-19 pandemic still looming and consistently and feeling like doing a project related to my hometown, I decided to learn more about COVID-19 and residential property crime on the neighborhood level. Over the course of my analysis (March 2019 - Feb 2021), I found one neighborhood to have a significant increase in residential property crime (Lincoln Square) and 15 neighborhoods to have a significant decrease in residential property crime. Overall, I found significant evidence that property crime decreased for Chicago as a whole with the onset of the pandemic. My statistical methods included fixed and random effects modeling as well as Granger causality. Please check out my paper to learn more!"
  },
  {
    "objectID": "projects/woba/index.html",
    "href": "projects/woba/index.html",
    "title": "Maximizing wOBA with Launch Angle and Exit Velocity",
    "section": "",
    "text": "In summer 2021 I was lucky enough to be part of a 15 person research cohort with a focus on sports analytics through Carnegie Mellon. Not only did I get to learn so many awesome statistical techniques, but I also got to apply them to my favorite topic (baseball) with people who were equally as passionate as me. My research was done in partnership with Sarah Sult (Washington University in St. Louis) and Brooke Coneeny (Swarthmore College) and advised by Adam Brodie of the Houston Astros.\nSome of my main takeaways from this project were how deeply you have to think as a statistician about underlying assumptions. So many times we thought were proud of a model we had developed and ready to apply it, and then realized how it doesn’t take into account something crucial to the game of baseball (like how if a batter changes their swing they will be thrown different pitches). This was also my first time conducting formal team research and I learned how lucky I was to be able to work with people with different strengths than my own. Sarah and Brooke are both computer science majors and were able to use their skills to debug some of our more complicated functions while I focused on data visualization and modeling.\nPlease check out our project repository and most recent presentation to learn more about the complex relationship between launch angle/attack angle, exit velocity, and wOBA!"
  },
  {
    "objectID": "projects/bayes/index.html",
    "href": "projects/bayes/index.html",
    "title": "Tommy John surgery and its Relationship to MLB Pitcher Career Trajectory",
    "section": "",
    "text": "In Fall 2021 I had the opportunity to take Bayesian Statistics (STAT 454) and complete a capstone project on a topic of choice. Hearing so much about the use of Bayesian techniques in sports, I decided to learn more about career trajectory for pitchers in the MLB, with a focus especially on Tommy John surgery. Please check out this website to learn more and feel free to reach out to me with questions and/or suggestions!"
  },
  {
    "objectID": "projects/survival/index.html",
    "href": "projects/survival/index.html",
    "title": "A survival analysis of draft to debut date for MLB players",
    "section": "",
    "text": "In this project we conducted a survival analysis of the time from draft date to debut date for major league baseball players. Our analysis specifically focuses on the variables of fielding position and whether a player was drafted in high school or college, and how these differences change the length of a player’s time between being drafted and making their MLB debut. For the complete analysis, please use the following link to visit our website."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Erin Franke",
    "section": "About Me",
    "text": "About Me\nIn addition to statistics & data science, I also enjoy all things running, hiking, volunteering at my local food pantry in Chicago, following Major League Baseball, and spending time with friends and family.\n\n\n\n\n\n\n\n\n\nMy siblings and I on a hike in Washington\n\n\n\n\n\n\n\nMy first marathon!\n\n\n\n\n\n\n\n\n\nMacalester commencement with cross country teammates\n\n\n\n\n\n\n\nManitou Incline: 2,744 steps of fun!"
  },
  {
    "objectID": "projects/svd/index.html",
    "href": "projects/svd/index.html",
    "title": "US Universities and the SVD",
    "section": "",
    "text": "This past spring, two of my classmates at Macalester College (Vivian Powell and Pippa Gallagher) and I decided to use the SVD to analyze United States colleges. The SVD - Singular Value Decomposition - is a matrix factorization that can be used for data reduction in machine learning, similarity analysis, image compression, and least squares regression among other uses. In this project, we used the SVD and a data set from Forbes 2019 about “America’s Top Colleges” to identify some of America’s most unique colleges and universities, at least in the sense of some of their basic demographics.\n\n\nGeneral Analysis\nUsing a data set with the 523 colleges Forbes selected, we took information on each school’s undergraduate population, student population, net price, average grant aid, total annual cost, alumni salary, acceptance rate, and mean SAT and ACT scores to build an original SVD plot. Giving each variable an equal weighing on a 0 to 1 scale, we got the following plot. \n\nA first step in the analysis of the SVD plot of the entire matrix was to look at the first two left singular vectors in order to understand which variables were most strongly associated with the horizontal axis and vertical axis in the plot of the first two right singular vectors. Below is a table that displays the left singular vectors U1 and U2, and which variable each entry represents:\n\nU1 and U2, which are the two most important left singular vectors of our matrix, essentially tell us which variables are the most influential on a college’s placement in the plot of V1 and V2. By analyzing U1, we can see which colleges are going to be closest to the direction of V1 (the negative horizontal axis in our case) and which will be furthest (the positive horizontal direction). We can see that the largest positive value in U1 is for Average Grant Aid, indicating that colleges with more grant aid will be closer to the direction of V1 and colleges with less grant aid will be farther from that direction. Total Annual Cost is similarly influential, with a slightly lower positive value. The largest negative value in U1 is for Acceptance Rate, which indicates that colleges with the lowest acceptance rates will be more negative on the x-axis, while colleges with high acceptance rates will be more positive on the x-axis. Overall, this tells us that colleges closer to the left side of the plot are likely to have a higher cost, higher grant aid, and a lower acceptance rate. We can expect this to include schools like the Ivies and “prestigious” liberal arts colleges with substantial endowments. On the other hand, colleges closer to the right side of the plot are likely to have a lower cost and less aid, as well as a higher acceptance rate. We could expect to see state schools with lower tuition in this category.\n\nLooking at U2, which tells us which variables are the most influential in a college’s placement on the vertical axis, we see that the largest positive value is for Acceptance Rate. This means that colleges with a higher acceptance rate will tend to have more positive y values, while colleges with lower acceptance rates will tend to have more negative y values. U2 also has two very large negative values, for Student Population and Undergraduate Population. This indicates that schools with a very large number of students will tend to be closer to the negative y-axis, while schools with few students will be closer to the positive y-axis. Overall, colleges near the top of the plot are likely to have a higher acceptance rate and/or smaller student populations, and colleges near the bottom of the plot are likely to be more selective and/or have larger student populations.\n\nClearly, this analysis isn’t perfect, because there will be schools with very low acceptance rates and tiny populations, and according to our analysis these schools won’t have a specific place on the plot. However, the information we gain from analyzing these first two columns of U is mostly to help us understand what the main factors are for grouping colleges in this way, and why two colleges might be plotted as opposites even if they are similar in some ways. Knowing that cost, financial aid, size, and acceptance rate are the most important variables for our SVD analysis will allow us to understand what makes some of these colleges unique.\n\n\n\nOutliers\n\nIn order to apply what we now know about U and V, we can look at the SVD plot and consider our outliers. Above is a plot of the SVD with labels on a few of the major outlier schools that we noticed in our analysis (BYU, University of Central Florida, and Liberty University). The major outlier we saw in almost all plots was BYU. Upon further research, we found that BYU has an incredibly low annual total cost of 18,370 dollars in comparison to almost all other universities. As a result of this, it also has a very low grant aid ($4843 annually). To really understand quite how low these numbers are in comparison to other universities, take a look at the density plots of total annual cost and grant aid below.\n\nThat’s pretty crazy! However, this information actually lines up with our SVD plots. If you remember, when we discussed singular vectors we said that colleges with more grant aid and a higher total annual cost would be much more on the negative side of the x axis and those with lower costs and financial aid would be on the right, positive side. BYU definitely follows this trend, being a huge right horizontal outlier on all of our plots.\nNow let’s see what might be going on at the University of Central Florida. Digging into the data, we found that the University of Central Florida has a large student population of 66,059 students. Looking at the student population density plot, there appear to be hardly any schools of this size.\n\nAdditionally, UCF’s average grant aid is $5757, which we know from the density plot of average grant aid is certainly on the lower end of the spectrum. The low grant aid explains this university’s positive V1 value on the SVD plot. The university’s large student population explains why UCF is such a noticeable vertical outlier.\nFinally, let’s investigate our third major outlier, Liberty University. Based on Liberty’s negative vertical location on the plot we might predict Liberty has a large student population or is a more selective university. From the positive V1 value we guess that Liberty tends to be a cheaper university and/or gives out less financial aid, though not quite to the degree as BYU or even UCF. Looking into the numbers, we find our size prediction confirmed - Liberty has the largest student population at 75,735 students! Furthermore, our cost prediction is correct as well as Liberty’s average grant aid is 10,400 dollars and their annual total cost is $38,364. Wow, our SVD plot seems pretty reliable!\n\n\nSingle Variable Analysis\n\nACT and SAT scores\nAfter completing the original plot, we decided it would be best to start looking at different variables and assessing how they appeared on the plot. For example, we took the ACT variable and divided it into three categories - low average ACT (&lt;24), medium average ACT (24-30), and high average ACT (30+). We then created plots with the universities colored by these groups - one plot without the ACT/SAT variables included in our main matrix (below left) and one with them included (below right). Green indicates universities with high ACT scores, blue indicates medium scores of 24-30, and red indicates low scores of less than 24.\n\nThere are subtle differences between the two plots, but you really have to look closely! This means that the ACT/SAT variable is highly correlated with another variable in the data set that was plotted originally. It makes most logical sense that this would be the acceptance rate variable, which we will analyze next. However, it is important to note that based on the somewhat distinct red, blue, and green groups in these plots, ACT score and potentially acceptance rate are pretty good indicators of what makes colleges similar and different.\n\n\nAcceptance Rate\nNext, we analyzed Acceptance Rate, as it seems to be one of the most influential variables based on our conclusions from U1 and U2. In order to do this, we defined low acceptance rates as &lt; 35%, medium acceptance rates as &gt; 35% and &lt; 70%, and high acceptance rates as &gt; 70%. We then repeated the same process used for SAT/ACT scores by creating two plots, both color-coded by acceptance rate: one where acceptance rate was included in the SVD, and one where it was not included. In the plots below, green represents the low acceptance rates, blue represents medium, and red represents high.\n\nThe first plot, with acceptance rate, looks very similar to the plots color coded for ACT/SAT score above (which makes sense given that colleges with similar acceptance rates will likely accept students with proportionally similar scores). There is a fairly clear distinction on the plot between colleges in different categories. Consistent with our conclusions from the left singular vectors earlier, we see that colleges with the lowest acceptance rates (green) tend to be towards the left and bottom sides of the plot, while colleges with high acceptance rates (red) tend to be along the top and right sides of the plot (and medium rates fall somewhere in between). However, when acceptance rate is removed from the matrix, the clarity of this pattern collapses significantly, indicating that the data of acceptance rate cannot be accurately represented as a linear combination of the other variables in the data set. So we can conclude that as we found earlier, acceptance rate is a fairly strong tool to group similar colleges by.\n\n\nPublic versus Private\nFinally, we chose to analyze the predictability of private vs public institutions. We did not include the private/public variable in our SVD as it held far too much weight due to being a binary variable with values of only 0 and 1. Instead, we will investigate whether other variables could accurately depict a college as private or public. In the figure below, blue represents public universities and red represents private universities. The separation between the two categories is definite with minimal mixing of red and blue near the top of the plot. This demonstrates that relying on other factors - including size, cost and acceptance rate - can fairly accurately encompass the information in the public vs private variable.\n\nThere are several visible outliers on this plot, including Liberty University and Brigham Young University-Idaho. While these universities are both private, they are plotted far on the positive side of the x-axis, far from any other private colleges. As we discussed earlier, this is due to their unique qualities. This graph demonstrates that private universities are more likely to have a higher tuition cost (falling on the negative x-axis), which is a common and generally accurate stereotype of private universities. However, Liberty and BYU have uncharacteristically low tuition and therefore could not be easily recognized as a private institution without the private/public factor included. If we were to include the private/public variable we can see a significantly clearer separation of the two categories, where the outliers are even easier to spot, located between the two groups of universities.\n\nIf this graph were not color coded a viewer might assume that Liberty University and Brigham Young University are public rather than private. This emphasizes the uniqueness of the two colleges and their unpredictability. While determining if a college is more likely to be private or public appears to be easy given other variables, there will always be outliers that don’t fall into the stereotypes of alike colleges.\n\n\n\nConclusion\nSVD analysis is not a perfect tool, but this paper has demonstrated that it carries great value in the ability to reduce a very large data set to something plottable in two dimensions that is visually digestible to the average reader. By using only the first two right singular vectors of the data, we can extract a much simpler representation of the vast majority of the information contained in the data set, and use it to understand colleges in a way that comparing schools by a single variable at a time simply would not achieve. Overall, the SVD is a good way to draw general and overarching conclusions rather than specific and pointed ones. The SVD takes advantage of having the ability to pull from a large data set that otherwise would take far too much storage, time, and machine power to analyze."
  },
  {
    "objectID": "projects/commonpantry/index.html",
    "href": "projects/commonpantry/index.html",
    "title": "My first experience with data analysis for the community",
    "section": "",
    "text": "Brief reflection\nDuring January of 2021 I was fortunate enough to have the opportunity to complete a one credit internship analyzing data with Common Pantry, my local food pantry in Chicago, IL. This internship was very rewarding as not only did I become much more comfortable in R and learn quicker and easier ways to analyze data, but in the process I was able to help out to my local community with information that could play a part of feeding more families in the coming months.\nOne of my main takeaways from this internship was the importance of good data. As a small nonprofit, a lot of Common Pantry’s data entry is done by hand which makes sense. Throughout my analysis, I encountered problems such as mismatched or missing units within the data, inconsistencies with data being recorded as characters or numeric entries, and simply missing information among other issues. This gave me a lot of practice cleaning the data, and learned new techniques to do so including using functions such as str_replace_all(), str_extract(), as.numeric(), and several others. I also got good practice with joining data sets horizontally and vertically, renaming columns, exporting and importing data from a variety of sources, and more.\n\n\nResults\nWhen COVID-19 hit in March 2020 and demand at Common Pantry spiked to about 2.11x their normal levels, Common Pantry could understandably no longer collect in depth information on clients such as their names, addresses, family size, and more. However, one piece of information they were able to collect was client zip code. I used this to create a basic density plot of the distribution of clients in Chicago. The Common Pantry service area, outlined in red, was removed once COVID-19 put many out of work.\n\nThe map above shows an important point, which is that a large portion of Common Pantry’s clients during COVID are coming from out of their service area and perhaps this service area needs to be enlarged for the long term (if Common Pantry can get adequate funding to do so). Four months after the conclusion of this project, one super exciting thing to note is that Common Pantry has recently announced the purchase of their own new and larger building only three blocks from the current pantry that will allow them to serve a greater amount of clients!\n\n\n\nCommon Pantry’s new location\n\n\nOutside of the zip code analysis, as a result of the limited 2020 data a large portion of my work was done with the 2019-2020 data on the donor appeal instead of information on the clients coming to the pantry. However, I was able to create the following plot which demonstrates the sheer number of clients visiting the pantry each month. Notice the spike in numbers as COVID begins and people are laid off of work in March and April.\n\nThankfully, while much of the community was in need of increased support for the majority of 2020, other members were able to step up and support Common Pantry to a greater level than before. In 2019, Common Pantry received 1,429 total donations, with 55.7% of them being financial. Total donations more than doubled in 2020 at 3,415, with 74.8% being financial. While the average donation amount fell in 2020, it was not by much, falling from 304.23 dollars in 2019 to $275.96 in 2020. For a visual summary of how donations changed throughout 2020 itself, see below.\n\n\n\nGrowth in number of donations throughout 2020\n\n\n\n\n\nA comparison of the value of monetary donations in 2019 versus 2020,by month\n\n\nAnother interesting thing I found when investigating the data was that despite monetary donations skyrocketing when COVID-19 hit, food donations stayed relatively constant. This makes sense - in the virtual environment, schools and businesses that may have been doing food drives in 2019 transitioned over to monetary fundraisers. This comparison of food versus monetary donations overtime can be seen below.\n\nThe remainder of the work I did with Common Pantry is more private, as I identified their top 75 donors of 2019-2020 to better inform them who to potentially thank and/or target for additional funds in the future. I also used R to filter for frequent donors and was able to develop a list of people giving a certain amount monthly. Additionally, I identified some of Common Pantry’s most successful appeals and campaigns for raising money (unsurprisingly, the COVID-19 appeal and the I am Your Neighbor fundraisers were incredibly successful).\n\n\nConclusion\nOverall, I am so happy I got to have this experience and give back in a new way to a group that I have been volunteering with since middle school. It is my hope that as I continue to learn more data science and R that I can help Common Pantry and other nonprofits in ways that I haven’t even imagined yet."
  },
  {
    "objectID": "projects/causal/index.html",
    "href": "projects/causal/index.html",
    "title": "Does Oxygen Help? A Causal Analysis",
    "section": "",
    "text": "In Spring 2023 I had the opportunity to take Causal Inference (STAT 451) and complete a capstone project on a topic of choice. As a hiker, I was interested to learn about the value of oxygen when ascending the Himalayan Mountains. Please check out this website to learn more about causal inference and this project."
  },
  {
    "objectID": "projects/honors/index.html",
    "href": "projects/honors/index.html",
    "title": "Gentrification and Crime in the Twin Cities: Insights and Challenges through a Statistical Lens",
    "section": "",
    "text": "Over the course of my senior year, I completed an independent research project in correspondence with my advisor, Victor Addona. Given that gentrification is highly contested phenomenon in today’s society and the neighborhood just north of Macalester has seen rapid redevelopment of the past several years, I chose to study the relationship between gentrification and crime in the Twin Cities. Using mapping, Poisson generalized linear models, and spatial modeling techniques, we did not find gentrification to yield meaningful decreases in violent crime or theft. To learn more, please download the completed paper here."
  },
  {
    "objectID": "projects/mathstat/index.html",
    "href": "projects/mathstat/index.html",
    "title": "Confounders & Omitted Variable Bias in Linear Regression",
    "section": "",
    "text": "In Spring 2022, I took Mathematical Statistics (STAT 455) where I deepened my understanding of theoretical statistics. Two of my classmates, Vivian Powell and Cheikh Fall, and I completed a final project on a topic of our choice which we taught to the remainder of our class. Given the importance of linear regression, we chose to dive deeper in a key assumption of ordinary least squares, exogeneity, and the consequences of omitted variable bias if this assumption does not hold. Please check out our our paper to learn more!"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Erin Franke",
    "section": "",
    "text": "Hi, my name is Erin (she/her). Thanks for visiting my page! \nI am a Statistics PhD student at Carnegie Mellon University. Previously, I graduated from Macalester College in May 2023 (Statistics major, Computer Science and Economics minors) and spent time at Mayo Clinic in a Statistical Programmer role. I am passionate about working with data to solve problems for the greater benefit of society, and have enjoyed applying statistical methodology to a variety of fields, including healthcare, sports, and urban studies/society. Please check out my Projects tab to learn more about my skills and see examples of my work."
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Tidy Tuesday!",
    "section": "",
    "text": "Tidy Tuesday is a weekly data project from the Data Science Learning Community. The data is shared every Monday here and on these social media platforms. If you are new to working with data, it is a great way to get practice! There are clear instructions for how to load each dataset into R, Python, and Julia. Tidy Tuesday also has a supportive online presence and it is encouraged to share what you create on social media—just follow these guidelines.\nI first started doing Tidy Tuesday when I took my first data science class in 2021. Challenging myself to find creative ways to visualize data and tell a story remains one of my favorite aspects of statistics. This page includes all my Tidy Tuesdays from most recent to oldest. Feel free to click on the image to see the corresponding code. While I often don’t have time to participate, I enjoy following #TidyTuesday on Bluesky to get inspiration. One of my favorite Tidy Tuesday creators is Nicola Rennie, check out her respository to see some pretty cool visualizations!"
  },
  {
    "objectID": "tidytuesday.html#section",
    "href": "tidytuesday.html#section",
    "title": "Tidy Tuesday!",
    "section": "2023",
    "text": "2023"
  },
  {
    "objectID": "tidytuesday.html#section-1",
    "href": "tidytuesday.html#section-1",
    "title": "Tidy Tuesday!",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "tidytuesday.html#section-2",
    "href": "tidytuesday.html#section-2",
    "title": "Tidy Tuesday!",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "My Reflections from USCOTS 2025\n\n\n\nJul 21, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/rtricks/index.html",
    "href": "blog/rtricks/index.html",
    "title": "A small collection of various R tricks",
    "section": "",
    "text": "Below is a collection of R code & tricks I have found helpful to have on hand. These are the smaller, more obscure pieces of code I forget and then repeatedly search online for. I hope to continue to add to this list overtime!\n\nData Wrangling\n\n1. coalesce()\nSource: post from appliedepi@bsky.social\nReturns the first non-missing value from a set of columns based on the order that you specify. This is great for preventing a long series of case_when() calls.\n\n\n\n2. Text\n\na. Removing accents\nIn a situation where we are working with strings with accents (e.g. cities, names, etc), we likely want all strings to be formatted in the same. The following shows an example where some cities have accents and some do not. We can remove all accents using stringi::stri_trans_general(), as shown below.\n\nhead(cities) %&gt;% gt() # gt() just makes table look nice :) \n\n\n\n\n\n\n\nnames\n\n\n\n\nBogotá\n\n\nBogota\n\n\nQuébec\n\n\nQuebec\n\n\nÎle de la Cité\n\n\nIle de la Cite\n\n\n\n\n\n\n\n\ncities %&gt;%\n  mutate(names = stringi::stri_trans_general(names, \"Latin-ASCII\")) %&gt;% \n  count(names) %&gt;% gt()\n\n\n\n\n\n\n\nnames\nn\n\n\n\n\nBogota\n2\n\n\nIle de la Cite\n2\n\n\nQuebec\n2\n\n\n\n\n\n\n\n\n\nb. unnest_tokens()\nThe unnest_tokens() is helpful for analyzing word counts of text, specifically that might be split up across many lines. I first used this function to analyze historical markers data. The example below is taken from the unnest_tokens() help page.\n\nlibrary(janeaustenr) # for this example\nlibrary(tidytext) # for unnest_tokens()\n\nnovel &lt;- tibble(txt = prideprejudice)\nhead(novel, 15) %&gt;%\n  gt()\n\n\n\n\n\n\n\ntxt\n\n\n\n\nPRIDE AND PREJUDICE\n\n\n\n\n\nBy Jane Austen\n\n\n\n\n\n\n\n\n\n\n\nChapter 1\n\n\n\n\n\n\n\n\nIt is a truth universally acknowledged, that a single man in possession\n\n\nof a good fortune, must be in want of a wife.\n\n\n\n\n\nHowever little known the feelings or views of such a man may be on his\n\n\nfirst entering a neighbourhood, this truth is so well fixed in the minds\n\n\nof the surrounding families, that he is considered the rightful property\n\n\n\n\n\n\n\n\nnovel %&gt;%\n  tidytext::unnest_tokens(output = word, input = txt) %&gt;%\n  head(20) %&gt;%\n  gt()\n\n\n\n\n\n\n\nword\n\n\n\n\npride\n\n\nand\n\n\nprejudice\n\n\nby\n\n\njane\n\n\nausten\n\n\nchapter\n\n\n1\n\n\nit\n\n\nis\n\n\na\n\n\ntruth\n\n\nuniversally\n\n\nacknowledged\n\n\nthat\n\n\na\n\n\nsingle\n\n\nman\n\n\nin\n\n\npossession\n\n\n\n\n\n\n\nWe could then go on to count the number of times each word is used!\n\n\nc. Removing punctuation\nLike accents, we might also want to remove punctuation. We can do this with the [:punct:] regular expression!\n\ndata %&gt;% gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy :)\n\n\nexcited!!\n\n\n**excited**\n\n\nsad :(\n\n\nangry,\n\n\nupset?\n\n\n#mad\n\n\n\n\n\n\ndata %&gt;%\n  mutate(feelings = str_replace_all(feelings, \"[:punct:]\", \"\")) %&gt;% \n  gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy\n\n\nexcited\n\n\nexcited\n\n\nsad\n\n\nangry\n\n\nupset\n\n\nmad\n\n\n\n\n\n\n\n\n\nd. Separate list of words in a column!\nMany times I have ran into there being a list of words separated by a comma in a column of dataset. For example, asking participants to list the most memorable characteristics of a chocolate that they tasted (analyzed in this tidy tuesday).\nThis is some code to count how many time each word is used.\n\nhead(chocolate) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\n\n\n\n\n2454\nrich cocoa, fatty, bready\n\n\n2458\ncocoa, vegetal, savory\n\n\n2454\ncocoa, blackberry, full body\n\n\n2542\nchewy, off, rubbery\n\n\n2546\nfatty, earthy, moss, nutty,chalky\n\n\n2546\nmildly bitter, basic cocoa, fatty\n\n\n\n\n\n\nlist_of_adjectives &lt;- chocolate %&gt;%\n  mutate(id = row_number(), # gives each reviewer/chocolate combination a unique id\n         most_memorable_characteristics = strsplit(as.character(most_memorable_characteristics), \",\")) %&gt;% # split on the comma to create a list of characteristics for each individual\n  unnest(most_memorable_characteristics) # unlists the list, one in each column\n\nhead(list_of_adjectives) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\nid\n\n\n\n\n2454\nrich cocoa\n1\n\n\n2454\nfatty\n1\n\n\n2454\nbready\n1\n\n\n2458\ncocoa\n2\n\n\n2458\nvegetal\n2\n\n\n2458\nsavory\n2\n\n\n\n\n\n\n\n\n# can then count the number of each adjective, or separate into separate columns:\nlist_of_adjectives %&gt;%\n  group_by(id) %&gt;%\n  mutate(adjnum = paste0(\"adj\", row_number(id))) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = c(reviewer, id), names_from = adjnum, values_from = most_memorable_characteristics) %&gt;%\n  head() %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nid\nadj1\nadj2\nadj3\nadj4\nadj5\n\n\n\n\n2454\n1\nrich cocoa\nfatty\nbready\nNA\nNA\n\n\n2458\n2\ncocoa\nvegetal\nsavory\nNA\nNA\n\n\n2454\n3\ncocoa\nblackberry\nfull body\nNA\nNA\n\n\n2542\n4\nchewy\noff\nrubbery\nNA\nNA\n\n\n2546\n5\nfatty\nearthy\nmoss\nnutty\nchalky\n\n\n2546\n6\nmildly bitter\nbasic cocoa\nfatty\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n3. Make things faster\n\na. across()\nacross() lets us apply one function to many columns at once, such as below getting the average of each respective penguin measurement column.\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\npenguins %&gt;% \n  summarise(across(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g), ~mean(.x, na.rm=T))) %&gt;%\n  gt()\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n43.92193\n17.15117\n200.9152\n4201.754\n\n\n\n\n\n\n\n\n\nb. mutate if\nUse mutate_if() to apply a particular function under certain conditions.\n\npenguins %&gt;% \n  mutate_if(is.double, as.integer) %&gt;%  # IF is numeric, make AS integer\n  head(3) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39\n18\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39\n17\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40\n18\n195\n3250\nfemale\n2007\n\n\n\n\n\n\n\n\n\nc. data.table\nThe data.table package’s fread() function loads data into R more efficiently than many of the read_X() functions. When loading a large dataset, try replacing the read_X() with fread()!\nExample of loading in tidy tuesday data:\n\nlibrary(data.table)\n\nhistorical_markers &lt;- fread('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-04/historical_markers.csv')\n\n\n\n\n\nData Visualization\n\n1. Title placement\n\na. Move plot title left\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nb. Center title/subtitle\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n2. Color\n\na. Change color of part of title\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", \n       title = \"Penguin &lt;strong&gt;&lt;span style='color:navy'&gt; flipper&lt;/span&gt;&lt;/strong&gt;&lt;/b&gt; vs. &lt;strong&gt;&lt;span style='color:goldenrod3'&gt; bill &lt;/span&gt;&lt;/strong&gt;&lt;/b&gt;length\")+\n  theme_classic()+\n  theme(plot.title = ggtext::element_markdown()) # must have element_markdown() to make this work!!\n\n\n\n\n\n\n\n\n\n\nb. Color palette finder\nThis website is a super fun way to visualize a bunch of different color palettes! You can choose a type (qualitative, diverging, essential), a target color, and the palette length. You can also select for different types of color blindness.\n\n\nc. Change plot background color\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.background = element_rect(fill = \"lightblue3\"), \n        panel.background = element_rect(fill = \"lightblue3\"))\n\n\n\n\n\n\n\n\n\n\n\n3. Annotate plot\n\na. Text\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  annotate(geom=\"text\", x=220, y=3500, label = \"There is a \\nlinear relationship\",\n           color = \"navy\", size=3)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nb. Add an arrow\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  geom_curve(aes(x = 220, xend = 211, y = 3500, yend = 4000),arrow = arrow(length = unit(0.03, \"npc\")), curvature = 0.4, color = \"navy\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n4. Fonts\nI like the showtext package to change fonts. Look here for fonts.\n\nlibrary(showtext)\n\nfont_add_google(\"Shadows Into Light\") # choose fonts to add\nfont_add_google(\"Imprima\")\nfont_add_google(\"Gudea\")\nshowtext_auto() # turns on the automatic use of showtext\n\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(family = \"Imprima\"))\n\n\n\n\n\n\n\n\n\n\n5. Images\n\na. general\n\nlibrary(png)\npenguin_pic &lt;- readPNG(\"images/penguin.png\", native=TRUE)\n\n\nplot &lt;- penguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()\n\nplot +                  \n  patchwork::inset_element(p = penguin_pic,\n                left = 0.87,\n                bottom = 0.5,\n                right = 1,\n                top = 0.7)\n\n\n\n\n\n\n\n\n\n\nb. aes\nYou might not want to do this for a ton of points, but you can replace a typical geom_point() with geom_image(), or think of other fun ways to use this (e.g. putting images at the end of bar chart).\n\npenguins %&gt;%\n  head(10) %&gt;% # select 10 points\n  mutate(img = \"images/penguin.png\") %&gt;%\n  ggplot()+\n  ggimage::geom_image(aes(x = flipper_length_mm, y = body_mass_g, image=img), size=0.06) +\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\n\nOther R tricks\n\n1. Rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑\n\n\n2. Custom Default Theme\nDuring college (and still now) I liked to modify the theme() of a plot a lot. This ended up with a lot of repeated code, so my advisor, Brianna Heggeseth, taught me how to setup a custom default theme. Thanks, Brianna! :) Here are the steps.\n\nOpen you .Rprofile file by pasting file.edit(file.path(\"~\", \".Rprofile\")) in the console\nModify this file to load your theme when a specific package is loaded (for example, ggplot2). Then, put your desired theme in theme_set(). This could be something as simple as ggplot2::theme_classic(). Below, I put a very simple example of a custom theme.\n\n\nsetHook(packageEvent(\"ggplot2\", \"onLoad\"), \n        function(...) ggplot2::theme_set(ggplot2::theme_classic()+\n                                           ggplot2::theme(plot.title.position = \"plot\",\n                                                 plot.title = ggplot2::element_text(family = \"mono\"))))\n\nA few notes:\n- You must call each function you use with the appropriate library in order for this to work.\n- When you are done, save the .Rprofile and quit RStudio in order for the changes to be saved.\n\nHere is a source I used to help refresh my memory on doing this.\n\n\n3. Highlighting code\nWhile making this blog post, I wanted to be able to highlight the lines of code that corresponded to the topic I was discussing. To do this, I installed the line-highlight extension for Quarto and followed the directions.\n\n\n\nThanks!\nThanks for reading my first blog post! If you have any R tricks you know of or feedback on this post, please email me at efranke@andrew.cmu.edu; I’d love to hear :)"
  },
  {
    "objectID": "blog/rtricks/index.html#data-wrangling",
    "href": "blog/rtricks/index.html#data-wrangling",
    "title": "My favorite R Tricks",
    "section": "Data Wrangling",
    "text": "Data Wrangling"
  },
  {
    "objectID": "blog/rtricks/index.html#data-visualization",
    "href": "blog/rtricks/index.html#data-visualization",
    "title": "My favorite R Tricks",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nOther R tricks\n\nSet up rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑"
  },
  {
    "objectID": "blog/rtricks/index.html#other-r-tricks",
    "href": "blog/rtricks/index.html#other-r-tricks",
    "title": "My favorite R Tricks",
    "section": "Other R tricks",
    "text": "Other R tricks\n\nSet up rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "As a statistician, my goal is to contribute research that helps understand and solve complex interdisciplinary problems.\nFor my year-long advanced data analysis project, I am working with Weijing Tang (CMU Stats & DS) and Phoebe Lam (CMU Psychology). Our goal is to integrate five existing studies in order to understand the influence of psychological stress on disease vulnerability, and whether social relationships, at what age, buffer this association. Combining the studies creates a blockwise missingness pattern in the data, a challenge that we address using moderated nonlinear factor analysis (MNLFA) with full information maximum likelihood.\nPrior to starting my PhD, I interned for the Minnesota Twins and worked at Mayo Clinic on the Chronic Lymphocytic Leukemia team. Collaborating with experts in other fields is my favorite part of being a statistician. Throughout my PhD, I look forward to doing methodological work that can help support these types of collaborations.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Throughout my PhD, I am excited to gain teaching experience and conduct teaching-related research. This summer, I am enjoying TAing for the Carnegie Mellon Sports Analytics Camp, which I was a student at in 2021 (full-circle moment!). I also attended my first conference, the United States Conference on Teaching Statistics, where I learned lots and presented two projects with my cohortmate Sara Colando."
  },
  {
    "objectID": "tidytuesday.html#what-is-tidy-tuesday",
    "href": "tidytuesday.html#what-is-tidy-tuesday",
    "title": "Tidy Tuesday!",
    "section": "",
    "text": "Tidy Tuesday is a weekly data project from the Data Science Learning Community. The data is shared every Monday here and on these social media platforms. If you are new to working with data, it is a great way to get practice! There are clear instructions for how to load each dataset to R, Python, and Julia. Tidy Tuesday also has a supportive online presence and it is encouraged to share what you create on social media—just follow these guidelines.\nI first started doing Tidy Tuesday when I took my first data science class in 2021. Challenging myself to find creative ways to visualize data and tell a story remains one of my favorite aspects of statistics. This page includes all my Tidy Tuesdays from most recent to oldest. Feel free to click on the image to see the corresponding code. I have also compiled a blog post with some obscure pieces of data wrangling and visualization code that have come in handy for many of my Tidy Tuesdays. While I often don’t have time to participate, I enjoy following #TidyTuesday on Bluesky to get inspiration. One of my favorite Tidy Tuesday creators is Nicola Rennie, check out her respository to see some pretty cool visualizations!"
  },
  {
    "objectID": "rtricks/index.html",
    "href": "rtricks/index.html",
    "title": "A small collection of various R tricks",
    "section": "",
    "text": "Below is a collection of R code & tricks I have found helpful to have on hand. These are the smaller, more obscure pieces of code I forget and then repeatedly search online for. I hope to continue to add to this list overtime!\n\nData Wrangling\n\n1. coalesce()\nSource: post from appliedepi@bsky.social\nReturns the first non-missing value from a set of columns based on the order that you specify. This is great for preventing a long series of case_when() calls.\n\n\n\n2. Text\n\na. Removing accents\nIn a situation where we are working with strings with accents (e.g. cities, names, etc), we likely want all strings to be formatted in the same. The following shows an example where some cities have accents and some do not. We can remove all accents using stringi::stri_trans_general(), as shown below.\n\nhead(cities) %&gt;% gt() # gt() just makes table look nice :) \n\n\n\n\n\n\n\nnames\n\n\n\n\nBogotá\n\n\nBogota\n\n\nQuébec\n\n\nQuebec\n\n\nÎle de la Cité\n\n\nIle de la Cite\n\n\n\n\n\n\n\n\ncities %&gt;%\n  mutate(names = stringi::stri_trans_general(names, \"Latin-ASCII\")) %&gt;% \n  count(names) %&gt;% gt()\n\n\n\n\n\n\n\nnames\nn\n\n\n\n\nBogota\n2\n\n\nIle de la Cite\n2\n\n\nQuebec\n2\n\n\n\n\n\n\n\n\n\nb. unnest_tokens()\nThe unnest_tokens() is helpful for analyzing word counts of text, specifically that might be split up across many lines. I first used this function to analyze historical markers data. The example below is taken from the unnest_tokens() help page.\n\nlibrary(janeaustenr) # for this example\nlibrary(tidytext) # for unnest_tokens()\n\nnovel &lt;- tibble(txt = prideprejudice)\nhead(novel, 15) %&gt;%\n  gt()\n\n\n\n\n\n\n\ntxt\n\n\n\n\nPRIDE AND PREJUDICE\n\n\n\n\n\nBy Jane Austen\n\n\n\n\n\n\n\n\n\n\n\nChapter 1\n\n\n\n\n\n\n\n\nIt is a truth universally acknowledged, that a single man in possession\n\n\nof a good fortune, must be in want of a wife.\n\n\n\n\n\nHowever little known the feelings or views of such a man may be on his\n\n\nfirst entering a neighbourhood, this truth is so well fixed in the minds\n\n\nof the surrounding families, that he is considered the rightful property\n\n\n\n\n\n\n\n\nnovel %&gt;%\n  tidytext::unnest_tokens(output = word, input = txt) %&gt;%\n  head(20) %&gt;%\n  gt()\n\n\n\n\n\n\n\nword\n\n\n\n\npride\n\n\nand\n\n\nprejudice\n\n\nby\n\n\njane\n\n\nausten\n\n\nchapter\n\n\n1\n\n\nit\n\n\nis\n\n\na\n\n\ntruth\n\n\nuniversally\n\n\nacknowledged\n\n\nthat\n\n\na\n\n\nsingle\n\n\nman\n\n\nin\n\n\npossession\n\n\n\n\n\n\n\nWe could then go on to count the number of times each word is used!\n\n\nc. Removing punctuation\nLike accents, we might also want to remove punctuation. We can do this with the [:punct:] regular expression!\n\ndata %&gt;% gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy :)\n\n\nexcited!!\n\n\n**excited**\n\n\nsad :(\n\n\nangry,\n\n\nupset?\n\n\n#mad\n\n\n\n\n\n\ndata %&gt;%\n  mutate(feelings = str_replace_all(feelings, \"[:punct:]\", \"\")) %&gt;% \n  gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy\n\n\nexcited\n\n\nexcited\n\n\nsad\n\n\nangry\n\n\nupset\n\n\nmad\n\n\n\n\n\n\n\n\n\nd. Separate list of words in a column!\nMany times I have ran into there being a list of words separated by a comma in a column of dataset. For example, asking participants to list the most memorable characteristics of a chocolate that they tasted (analyzed in this tidy tuesday).\nThis is some code to count how many time each word is used.\n\nhead(chocolate) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\n\n\n\n\n2454\nrich cocoa, fatty, bready\n\n\n2458\ncocoa, vegetal, savory\n\n\n2454\ncocoa, blackberry, full body\n\n\n2542\nchewy, off, rubbery\n\n\n2546\nfatty, earthy, moss, nutty,chalky\n\n\n2546\nmildly bitter, basic cocoa, fatty\n\n\n\n\n\n\nlist_of_adjectives &lt;- chocolate %&gt;%\n  mutate(id = row_number(), # gives each reviewer/chocolate combination a unique id\n         most_memorable_characteristics = strsplit(as.character(most_memorable_characteristics), \",\")) %&gt;% # split on the comma to create a list of characteristics for each individual\n  unnest(most_memorable_characteristics) # unlists the list, one in each column\n\nhead(list_of_adjectives) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\nid\n\n\n\n\n2454\nrich cocoa\n1\n\n\n2454\nfatty\n1\n\n\n2454\nbready\n1\n\n\n2458\ncocoa\n2\n\n\n2458\nvegetal\n2\n\n\n2458\nsavory\n2\n\n\n\n\n\n\n\n\n# can then count the number of each adjective, or separate into separate columns:\nlist_of_adjectives %&gt;%\n  group_by(id) %&gt;%\n  mutate(adjnum = paste0(\"adj\", row_number(id))) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = c(reviewer, id), names_from = adjnum, values_from = most_memorable_characteristics) %&gt;%\n  head() %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nid\nadj1\nadj2\nadj3\nadj4\nadj5\n\n\n\n\n2454\n1\nrich cocoa\nfatty\nbready\nNA\nNA\n\n\n2458\n2\ncocoa\nvegetal\nsavory\nNA\nNA\n\n\n2454\n3\ncocoa\nblackberry\nfull body\nNA\nNA\n\n\n2542\n4\nchewy\noff\nrubbery\nNA\nNA\n\n\n2546\n5\nfatty\nearthy\nmoss\nnutty\nchalky\n\n\n2546\n6\nmildly bitter\nbasic cocoa\nfatty\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n3. Make things faster\n\na. across()\nacross() lets us apply one function to many columns at once, such as below getting the average of each respective penguin measurement column.\n\nlibrary(palmerpenguins)\n\npenguins %&gt;% \n  summarise(across(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g), ~mean(.x, na.rm=T))) %&gt;%\n  gt()\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n43.92193\n17.15117\n200.9152\n4201.754\n\n\n\n\n\n\n\n\n\nb. mutate if\nUse mutate_if() to apply a particular function under certain conditions.\n\npenguins %&gt;% \n  mutate_if(is.double, as.integer) %&gt;%  # IF is numeric, make AS integer\n  head(3) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39\n18\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39\n17\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40\n18\n195\n3250\nfemale\n2007\n\n\n\n\n\n\n\n\n\nc. data.table\nThe data.table package’s fread() function loads data into R more efficiently than many of the read_X() functions. When loading a large dataset, try replacing the read_X() with fread()!\nExample of loading in tidy tuesday data:\n\nlibrary(data.table)\n\nhistorical_markers &lt;- fread('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-04/historical_markers.csv')\n\n\n\n\n\nData Visualization\n\n1. Title placement\n\na. Move plot title left\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nb. Center title/subtitle\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n2. Color\n\na. Change color of part of title\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", \n       title = \"Penguin &lt;strong&gt;&lt;span style='color:navy'&gt; flipper&lt;/span&gt;&lt;/strong&gt;&lt;/b&gt; vs. &lt;strong&gt;&lt;span style='color:goldenrod3'&gt; bill &lt;/span&gt;&lt;/strong&gt;&lt;/b&gt;length\")+\n  theme_classic()+\n  theme(plot.title = ggtext::element_markdown()) # must have element_markdown() to make this work!!\n\n\n\n\n\n\n\n\n\n\nb. Color palette finder\nThis website is a super fun way to visualize a bunch of different color palettes! You can choose a type (qualitative, diverging, essential), a target color, and the palette length. You can also select for different types of color blindness.\n\n\nc. Change plot background color\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.background = element_rect(fill = \"lightblue3\"), \n        panel.background = element_rect(fill = \"lightblue3\"))\n\n\n\n\n\n\n\n\n\n\n\n3. Annotate plot\n\na. Text\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  annotate(geom=\"text\", x=220, y=3500, label = \"There is a \\nlinear relationship\",\n           color = \"navy\", size=3)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nb. Add an arrow\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  geom_curve(aes(x = 220, xend = 211, y = 3500, yend = 4000),arrow = arrow(length = unit(0.03, \"npc\")), curvature = 0.4, color = \"navy\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n4. Fonts\nI like the showtext package to change fonts. Look here for fonts.\n\nlibrary(showtext)\n\nfont_add_google(\"Shadows Into Light\") # choose fonts to add\nfont_add_google(\"Imprima\")\nfont_add_google(\"Gudea\")\nshowtext_auto() # turns on the automatic use of showtext\n\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(family = \"Imprima\"))\n\n\n\n\n\n\n\n\n\n\n5. Images\n\na. general\n\nlibrary(png)\npenguin_pic &lt;- readPNG(\"images/penguin.png\", native=TRUE)\n\n\nplot &lt;- penguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()\n\nplot +                  \n  patchwork::inset_element(p = penguin_pic,\n                left = 0.87,\n                bottom = 0.5,\n                right = 1,\n                top = 0.7)\n\n\n\n\n\n\n\n\n\n\nb. aes\nYou might not want to do this for a ton of points, but you can replace a typical geom_point() with geom_image(), or think of other fun ways to use this (e.g. putting images at the end of bar chart).\n\npenguins %&gt;%\n  head(10) %&gt;% # select 10 points\n  mutate(img = \"images/penguin.png\") %&gt;%\n  ggplot()+\n  ggimage::geom_image(aes(x = flipper_length_mm, y = body_mass_g, image=img), size=0.06) +\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\n\nOther R tricks\n\n1. Rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑\n\n\n2. Custom Default Theme\nOne of the aspects of my plots that I modify the most is the theme(). During college, I often found myself copying and pasting my custom theme to the end of every plot. To make less work for myself and my code more readable, I learned to permanently load my custom theme when I open R. Special thanks to my undergrad advisor, Brianna Heggeseth, for helping me figure this out :) Here are the steps.\n\nOpen you .Rprofile file by pasting file.edit(file.path(\"~\", \".Rprofile\")) in the console\nModify this file to load your theme when a specific package is loaded (for example, ggplot2). Then, put your desired theme in theme_set(). This could be something as simple as ggplot2::theme_classic(). Below, I put a very simple example of a custom theme.\n\n\nsetHook(packageEvent(\"ggplot2\", \"onLoad\"), \n        function(...) ggplot2::theme_set(ggplot2::theme_classic()+\n                                           ggplot2::theme(plot.title.position = \"plot\",\n                                                 plot.title = ggplot2::element_text(family = \"mono\"))))\n\nA few notes:\n\n\nYou must call each function you use with the appropriate library in order for this to work.\n\nWhen you are done, save the .Rprofile and quit RStudio in order for the changes to be saved.\n\n\nHere is a source I used to help refresh my memory on doing this.\n\n\n3. Highlighting code\nWhile making this blog post, I wanted to be able to highlight the lines of code that corresponded to the topic I was discussing. To do this, I installed the line-highlight extension for Quarto and followed the directions."
  },
  {
    "objectID": "teaching.html#teaching-experience",
    "href": "teaching.html#teaching-experience",
    "title": "Teaching",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nTeaching Assistant\n\n\n36-401: Modern Regression (Fall 2024)\n\n36-236: Probability and Statistical Inference II (Spring 2025)\n\nCarnegie Mellon Sports Analytics Camp (Summer 2025)\n\n\nCo-Guest Lecturer for four sessions"
  },
  {
    "objectID": "teaching/uscots2025.html",
    "href": "teaching/uscots2025.html",
    "title": "My Reflections from USCOTS 2025",
    "section": "",
    "text": "I’m so grateful to have had the opportunity to attend the 2025 United States Conference on Teaching Statistics (USCOTS) in Ames, Iowa! Before jumping into my general reflections, I want to thank Kelly Bodwin and the rest of team behind the POSE: Phase II: Expanding the data.table ecosystem for efficient big data manipulation in R NSF grant for funding my trip, as well as CAUSE for generously waiving the registration fee. I am also want to thank the many people that organized this conference, including Allan Rossman, Kelly McConville, Laura Ziegler, and Matt Beckman. The conference was incredibly well run and I cannot imagine how much work went it.\nThis was my first conference, and it was such a privilege to listen to talks and have discussions with many of the leaders in statistics and data science education. This was very inspiring, and also a bit overwhelming—these individuals have done so much and have so many smart and creative ideas. Both at the conference and typing up these reflections, it was hard to not feel some level of imposter syndrome—unlike the majority of attendees, I have never even taught a class! All of this is just to say, I have lots of to learn, and I feel so lucky to have gotten a chance to jumpstart that learning process by connecting with so many smart people (and am very grateful for how kind and welcoming they all were)!"
  },
  {
    "objectID": "teaching/uscots2025.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "href": "teaching/uscots2025.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "title": "My Reflections from USCOTS 2025",
    "section": "Life as a liberal arts statistician: joys and challenges",
    "text": "Life as a liberal arts statistician: joys and challenges\nIdeas for success in the classroom\n\n\nSurvey the class periodically and discuss the results (both things to change and why particular aspects will remain the same) with the students\n\nLearn from others, both in your department and others\n\n\nSit in on classes\n\n\nTake notes after class on what worked and what didn’t for the future\n\n\nBe active professionally\n\nPre-tenure, make strategic choices. Play the odds to get research that students can help with.\n\nPost-tenure, focus on what nourishes you (e.g. publishing an open source textbook)\n\nCollaborate with non-statistical collegues\n\nAdapt research question (and timeline for undergrads)\n\nServe others\n\nJump into service early, both internally and externally\n\nBe happy and proactive with “yeses”, then you can say “no” to things not as interesting to you\n\nGrowing a program\n\nHelpful to have a customizable concentration/minor\n\nHaving a center for interdisciplinary research is a good way to create community and can lead to research collaborations\n\nAdd modern courses to curriculum (e.g. add a data science course and remove an advanced modeling requirement)\n\nFind projects everywhere! Examples: a bagel shop running out of your favorite flavor, library checkout data\n\nMany aspects are constantly changing\n\nClassroom dynamics and design\n\nTechnology (classroom tech, programming languages)\n\nRising popularity of statistics and data science in recent years\n\nRelationship with math and computer science\n\nData ethics\n\n\nChallenges\n\nSemesters are hectic\n\nSalary\n\nGrading\n\n5% of students\n\nSlow changes in academia\n\nLifelong learning can be exhausting\n\n\nJoys\n\nIt usually doesn’t feel like work\n\nAwesome colleagues\n\nStudents want to make world better\n\nCreating a good learning environment\n\nConstant renewal\n\nCollaborating with experts in different fields\n\nTaking pride in your institution\n\nLifelong learning can be exhilarating\n\n\nShort reflection: The group I was in talked a lot about growing a program and the interaction between math, statistics, and computer science. Some people in the group were in a statistics/mathematics department separate from computer science, and mentioned this being a point of tension. Right now, when data science is so popular, it is ideal to have these departments collaborating and working together on what courses should count for credits to particular majors or minors (as well as just to maximize student learning). Having a combined department at Macalester, this was not something I’d thought about before, but it must make conversations about forming a data science major or minor, for instance, easier. Some other issues mentioned were struggle to find a form of service people find exciting, and issues with institutional financials. I hadn’t previously thought about either of these topics when thinking about a liberal arts career, so it was nice to listen to what others had to say."
  },
  {
    "objectID": "teaching/uscots2025.html#authentic-assessment-with-oral-exams",
    "href": "teaching/uscots2025.html#authentic-assessment-with-oral-exams",
    "title": "My Reflections from USCOTS 2025",
    "section": "Authentic assessment with oral exams",
    "text": "Authentic assessment with oral exams\nWith students now being able to offload parts of “writing to learn” assignments (e.g. data analysis reports) to LLMs, many people at this conference were interested in discussing alternative forms of assessment. One option that has potential in liberal arts classrooms is oral exams. While the potential benefit of oral exams is high, the primary concern seems to be time, both for the professor and students.\nPros\n\nPersonal connection between professor and student.\nProfessors having used oral exams noted they could tell level of the student’s understanding with just a couple of minutes.\n\nGoing off this, the students did not refute their score. The student could tell if they weren’t matching the professor’s expectation for level of understanding.\n\nOpportunity to understand the student’s misunderstandings and redirect them.\n\nFor example, if a student said, “The probability of the null being false is 95%”, you could ask a follow up question like, “Wait, can you explain a little more why that is the case?”.\n\nNo option for student to offload thinking to LLMs.\nGood practice for job interviews.\nOpportunity to meet the student where they are at. For students less comfortable with the material, you can redirect and ask them different questions than a student clearly grasping the concepts.\n\nCons\n\nTime. With 2-3 sections of a class with 20 students, you are trying to schedule 40-60 fifteen minute blocks. That is a lot of time out of the professor’s week, and it is also hard to find time outside of class for students with busy schedules.\n\nCan we do paired oral exams to cut down on that?\n\nHow are students then graded as a pair? What if one partner dominates the conversation? Side note: in an ungrading scenario, this would not be as much of an issue and something the students could reflect on themselves after the fact.\nIdea: Have the pair have a conversation between themselves based on a set of prompts given to them.\n\n\nStudent anxiety. The idea of an oral exam is daunting for many students.\n\nPaired exams could potentially help reduce this anxiety.\nCertain students may understand the concepts well but not be able to verbally communicate on the spot.\n\nAlternatively, you could say that some students have test anxiety, but this is just a common form of assessment so we typically ignore this issue.\n\n\nFairness and equity: can we ensure students are evaluated the same?\n\nOther notes\n\nAs opposed to an exam, how much material can you assess? If, for example, an exam were to cover machine learning methods such as LASSO, clustering, splines, random forests, and GAMs, are you assessing all of these methods in a 15 minute oral exam but sacrificing depth, or vice versa?\nOne professor found no correlation between student oral exam scores and test scores in a semester where they used both as modes of assessment.\n\nI found this really interesting, and don’t think is necessarily a pro or a con, but maybe a reason to utilize multiple forms of assessment within a semester."
  },
  {
    "objectID": "teaching/uscots2025.html#project-design",
    "href": "teaching/uscots2025.html#project-design",
    "title": "My Reflections from USCOTS 2025",
    "section": "Project Design",
    "text": "Project Design\nIn this discussion, we covered some ideas make sure students are getting the most they can out of class projects.\nProject setup\n\nProviding a sample report: should we or should we not?\n\nUseful so that students understand expectations, but they may follow a sample report too closely and not build their own writing style.\nIdea: provide a report with several flaws, and take class time to critique the report and discuss potential changes.\n\nPeer reviews: tell students it is not a time to be “MN nice”. Give honest feedback to help your classmate get a better grade.\nFor group projects, have formal meetings with each project group to discuss next modeling steps, what is going well or what they are struggling with, etc.\n\nPersonal note: I think effectively communicating what I have done and have questions on in an organized matter in a one-on-one meeting was one of the things I struggled with transitioning out of undergrad (both working at Mayo Clinic and starting research in my PhD). Maybe this could help prepare students for that.\n\n\nProject Ideas\n\nClass time activity: story boarding with data. Have students introduce an idea, most important aspects from EDA, conflict/main characters.\n\nStudents don’t necessarily have to even have data, just have them sketch and type of plot that they might expect.\nHave them decide on an ideal storyboard: this is a good way to get them to think about the project workflow and communicating big ideas (perhaps to a nonstatistical audience too).\n\nPodcast (~10 minutes) where you encourage students to write a script (so they think through what they will say) and the verbally communicate their findings with a partner.\n\nHave students send pdf of visualizations that they talk about.\nOne professor found a student that didn’t typically participate in class was great at this."
  },
  {
    "objectID": "teaching/uscots2025.html#positron",
    "href": "teaching/uscots2025.html#positron",
    "title": "My Reflections from USCOTS 2025",
    "section": "Positron",
    "text": "Positron"
  },
  {
    "objectID": "teaching/uscots2025.html#hadley-wickham-keynote",
    "href": "teaching/uscots2025.html#hadley-wickham-keynote",
    "title": "My Reflections from USCOTS 2025",
    "section": "Hadley Wickham Keynote",
    "text": "Hadley Wickham Keynote"
  },
  {
    "objectID": "teaching/uscots2025.html#mine-cr-workshop",
    "href": "teaching/uscots2025.html#mine-cr-workshop",
    "title": "My Reflections from USCOTS 2025",
    "section": "Mine CR Workshop",
    "text": "Mine CR Workshop"
  },
  {
    "objectID": "teaching/uscots2025.html#teaching-poisson-regression-jo-and-nick",
    "href": "teaching/uscots2025.html#teaching-poisson-regression-jo-and-nick",
    "title": "My Reflections from USCOTS 2025",
    "section": "Teaching Poisson Regression, Jo and Nick",
    "text": "Teaching Poisson Regression, Jo and Nick"
  },
  {
    "objectID": "teaching/uscots2025.html#doing-data-science-in-positron",
    "href": "teaching/uscots2025.html#doing-data-science-in-positron",
    "title": "My Reflections from USCOTS 2025",
    "section": "Doing data science in Positron",
    "text": "Doing data science in Positron\nThis workshop by Mine Cetinkaya-Rundel and Hadley Wickham a great first introduction to Positron, which I didn’t know anything about prior to this conference.\nWhat is it?\n\nA next-generation data science IDE that feels like a fusion of RStudio and VS Code.\n\nHappy note: RStudio is not going away!\n\n\nBenefits of Positron\n\nMain benefit: easily combine R and Python in one document\n\nPersonally not currently a Python user, but I know a lot of people that are and I think this could be great for classes or research where groups are using both!\n\nAbility to effortlessly switch between versions of R (e.g. R 4.3.3, 4.5.0)\nCommand palette (Ctrl/Cmd + Shift + P) makes searching for and executing commands easy\nEasy to customize layout and split screen between files.\nMultiple concurrent interpreter sessions, which can be a mix of different R versions, mix of R and Python sessions, or multiple instances of a single R version\n\nAllows you to run different code while something is taking a while to run! This is awesome to me.\n\nCan sort your environment (e.g. by most recent)\nAir: an extension that automatically formats your code nicely\nYou can Preview (Render) your document and it will appear in the viewer pane instead of opening in your internet browser—easy to make side by side changes\nArea for previewing \\(\\LaTeX\\) equations\nEasy to save and share plots\n\nOther differences\n\nNo inline plots\n\nOutputs to “plots” pane, which I think I agree with most people is nice because the inline plots can make your document a bit laggy.\n\nRun button is different/a bit smaller? Personally like the RStudio version better this is not a big deal.\nPositron automatically updates (good, I think?)\n\nThings for me to learn more about\n\nrig\nAir\nSnippets\nPositron Assistant\n\nOverall reflections\nVery cool, also a bit overwhelming—there are just so many features. Hadley and Mine mentioned when you see someone’s RStudio, you can easily become acquainted with their setup, but Positron is completely customizable (which is mostly a pro, but can perhaps be a con in a teaching setting?). At the same time, so many of these features seem useful and Mine and Hadley did an awesome job motivating and explaining the tool. I really like the ability to continue to run smaller code jobs while another piece is running in the background. The mix of versions of R is also great. I am not sure if/when I will be fully doing my coding in Positron over RStudio, but I am excited to get more experience with it."
  },
  {
    "objectID": "teaching/uscots2025.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "href": "teaching/uscots2025.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "title": "My Reflections from USCOTS 2025",
    "section": "A no bullshit guide to programming with LLMs in R",
    "text": "A no bullshit guide to programming with LLMs in R\nThis talk by Hadley Wickham introduced the ellmer R package, which he developed to interact with LLMs in R. As someone that enjoys the challenges and satisfaction of coding and figuring out little tricks, I don’t love the idea of offloading parts of that process to LLMs. But I know this is a part of the current reality that we live in, and can make some dreadful tasks much faster, so I wanted to listen with an open mind.\nOne of Hadley’s first motivating examples had us discuss at our table how we’d extract name and age from text data, such as the following:\n\nprompts &lt;- c(\"I go by Alex. 42 years on planet Earth and counting.\", \n             \"Hey, I'm turning 27 and my name is Jamal\", \n             \"I'm Lei Wei and nineteen years young.\",\n             \"My name is Brett and I'm turning the big 5-0 this year.\")\n\nClearly, this is not a simple regex expression due the mix of age being typed and numeric, names being 1-2 words, and the fact that there is no global pattern in sentence structure. I learned that this is something an LLM could do with essentially the following code:\n\nlibrary(ellmer)\nchat &lt;- chat_anthropic()\nchat$chat(\"Extract the name and age from each sentence I give you\")\n\nchat$chat(prompts[[1]])\n\nAnd this essentially returned the name and age for each prompt. I am not running the code above because you do have to obtain an API Key (which according to the package requires a developer account that you have to sign up and pay for, which poses its own challenges), but nonetheless this definitely caught my attention as a motivating example for LLMs. Writing regex to successfully get name and age from large data like this would be super difficult.\nHadley then went on to talk about ellmer’s ability to interact with external tools defined by the caller. One downfall of LLMs is that they may not know current information, such as the time of day. Defining an R function that knows the time and registering that tool with the LLM can solve that problem and allow you to answer queries like “How long ago exactly did the Chicago Cubs win the World Series?”. See this article for more information.\nI would assume if I were to use these tools for my research, I would want to understand how accurate they are. Hadley mentioned the vitals package which I will have to look at more if/when I work with any of these LLMs in R.\nMore interesting to me right now is the conversation around the pros and cons of using these tools, and also the impact they may have in education. These are the following things I caught from Hadley’s talk:\nPros\n\nLLMs are amazing for quickly generating demos, shiny apps, example data\n\nHadley showed an example of a rough sketch of a histogram, which he fed to Positron Assistant and told it to create a Shiny app using the Palmer Penguins data.\n\nThis worked… which was kind of frightening to me. At the same time it is cool because you get this base Shiny app code working right away (often the most frustrating part) and can go on to make modifications to your preference.\n\n\nGood at translations (e.g. latex \\(\\rightarrow\\) quarto, R code \\(\\rightarrow\\) stan, SQL \\(\\rightarrow\\) dplyr, json \\(\\rightarrow\\) to unit tests)\n\nAll I could think about was how many hours of my life I could saved instead of translating code from SAS to R at Mayo Clinic by hand…🥲\n\nWhile honestly this task was miserable to me, it was a big part of my job, and it definitely makes me scared for me entry level stats/data science positions which are places where people learn lots and gain skills that allow them to learn and move up in the field (or better understand what type of career they want).\n\n\nExplaining and critiquing code\n\nCons/concerns\n\nCost and equity of access: These tools do cost money, even if not that much. According to Hadley, $5 on Claude can get you pretty far and Gemini has a generous free tier.\nEnvironmental concerns: Hadley implied these are worth considering but small and decreasing on the individual level, and that flying to this conference, for example, is a much more detrimental environmental impact.\n\nI am sure that is true, but I just can’t help to think about the overarching impact if millions and millions of people are using these tools on a daily basis. This isn’t going to be a no impact situation.\n\nData privacy: a definite concern on the individual level. Not a problem for most bigger organizations as most data already lives in some cloud, and cloud providers run LLMs.\nReplacing artists: a definite risk at societal level. Hadley mentioned he is trying to supplement, not replace.\nEvil billionaires: we are just giving more money to evil tech people…there weren’t really any ideas for how to get around that.\n\nSo at the end of this I wasn’t really sure what to think. It was slightly encouraging to hear Hadley mention that there are many drawbacks of LLMs and in many ways coding is still a useful tool (e.g. making small changes, thinking critically, programming can still be faster than asking LLMs to do stuff for you), but in many ways, these tools make me feel bleh, and I think a lot more discussion about the place of these tools in statistics research and education is needed."
  },
  {
    "objectID": "teaching/uscots2025.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "href": "teaching/uscots2025.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "title": "My Reflections from USCOTS 2025",
    "section": "Leveraging LLMs for student feedback in introductory data science courses",
    "text": "Leveraging LLMs for student feedback in introductory data science courses\nAfter Hadley’s talk I went right into another LLM talk by Mine Cetinkaya-Rundel, which covered leveraging LLMs for student feedback in STA 199, an introductory data science and statistical thinking course at Duke (no prerequistes). The class had two exams (20% each), but the largest component of the student grade was once weekly lab assignments graded for accuracy (35%).\nAI policy for class\n\nStudents could use AI tools but must explicitly cite them, and the prompt could not be copied and pasted directly from the assignment (the students had to create the prompt themselves).\nStudents were not allowed to copy and paste the AI narrative verbatim to answer questions.\nStudents were welcome to ask AI questions to enhance their learning and understanding.\n\nProject 1\n\nGoal: A chatbot that hopefully generates good, helpful, and correct answers that come from course content and prefers terminology/methods taught in the course.\nTwo motivating reasons for this\n\nStudents don’t read previous questions on online forums that their classmates have asked, even if the instructor asks them to do this before posting.\nChatGPT usually dosen’t generate answers in line with course content (for example, may give a base R response when tidyverse is taught).\n\nTechnical details\n\nUses Retrevial Augmented Generation (RAG) to focus chatbot on course content and give it context. The chatbot gives the student direction to specific pages of interest in the course textbooks.\n\nThis is accomplished through combining semantic similarity and knowledge graph searches.\n\n\nSQL database of student results (completely anonymized to professor)\n\nSome good interactions, some copy-and-paste directly from assignment, some “fix my code” questions.\n\nEvidence that the AI policy was a bit too optimistic\n\nCannot say the majority of answers it gives is better than other LLMs, but no credit card and the fact that it points to course materials are both pluses.\n\nA kind of sad question: Is the chatbot to read from textbook more motivating to the student than the professor telling them they should? Unknown.\n\nProject 2\n\nGoal: A feedback chatbot that hopefully generates good, helpful, and correct feedback based on an instructor designed rubric and suggests terminology/methods taught in the course.\nMotivating reasons\n\nStudents use AI tools as a first step before thinking about how to approach the task.\n\nA chatbot could be like a friend in the classroom that you turn to for helping thinking through a problem.\nBut also, if it gives them code to run and it works, no thinking will happen. See Microsoft study The Impact of Generative AI on Critical Thinking.\n\nMaybe AI can help TAs redistribute their time toward higher value and more enjoyable touch points with students, and away from repetitive and error-prone tasks which often go unread (giving feedback).\nTAs don’t want to provide detailed feedback to answers generated with AI (not that everyone is doing their hw with AI, but some students are)\nIf very detailed rubrics are already being written to ensure grading equivalency across TAs, it is easy to hand these to LLMs.\n\n\nActivity and Thoughts\nWe then went into an activity where we prompted an LLM (ChatGPT) with the question and the student response and asked it to give feedback. The feedback was very verbose, and commented on several things not part of the question (e.g. the year column is a character, it should be numeric). All of this was technically true, but perhaps not the main point of the question. With a rubric and telling it to be to the point, it was better, but still a lot to parse through for a fairly simple question (which was a pivot longer essentially).\nWe had the following thoughts at my table:\n\nShould we be giving feedback on things not part of the question?\n\nOn a similar note, the LLM might comment on small things that do not matter, making it difficult for the student to identify important concepts from small ones\n\nFor a topic I am fairly comfortable with, the more feedback there is, the less likely I am going to be to read it (personally). LLMs just seem way to verbose to me.\n\nCan we ask students to fill out a form about what type of feedback they prefer? Do they want the “compliment sandwich”, or just straight to the point what is wrong? Is an LLM even capable of being straight to the point?\n\nI think we are assuming that goal of the LLM giving feedback that aligns with what is taught in the course is met, otherwise this would be an issue (e.g. the LLM says to use names_transform in pivot longer when in the course teaches students to modify names in a mutate statement).\nI could have misunderstood, but the start of the session, a point was made about using these LLMs for feedback, not grading. If the TAs are still grading, is the amount of time saved by the TAs in not providing feedback worth the faults that come with using an LLM? Does the grading align with the feedback?\n\nWhile I personally wouldn’t be ready to use LLMs for feedback if I was teaching, I am generally interested to learn more about how this evolves. The idea itself kind of blew my mind, and I think it could be quite useful (particularly for low stakes assignments in courses with hundreds of students). Mine mentioned that a few of the next steps were to continue model evaluation (e.g. cost, speed, accuracy) and tradeoffs as new LLMs are released, and to measure learning outcomes for students using the LLM feedback to understand the effectiveness of this approach."
  },
  {
    "objectID": "teaching/uscots2025.html#topics-workshops",
    "href": "teaching/uscots2025.html#topics-workshops",
    "title": "My Reflections from USCOTS 2025",
    "section": "Topics Workshops",
    "text": "Topics Workshops\nTwo other breakout sessions I attended were Jo Hardin and Nick Horton’s Leveraging data technologies to model bigger datasets and Paul Roback and Laura Boehm Vock’s Integrating Poisson regression into the undergraduate curriculum. Both of these were great! Personally I was not familiar with SQL and DuckDB together prior to the workshop, so it was fun to learn something new that could be really helpful when working with large data/databases. In the other workshop, I really liked the way Paul and Laura explained Poisson regression visually and with very minimal math. If I am able to co-instruct/instruct the CMU REU program at some point, I think it would be fun to have a couple sessions on a portion of the materials from both of these workshops."
  },
  {
    "objectID": "blog/uscots2025/index.html",
    "href": "blog/uscots2025/index.html",
    "title": "My Reflections from USCOTS 2025",
    "section": "",
    "text": "I’m so grateful to have had the opportunity to attend the 2025 United States Conference on Teaching Statistics (USCOTS) in Ames, Iowa! Before jumping into my general reflections, I want to thank Kelly Bodwin and the rest of team behind the POSE: Phase II: Expanding the data.table ecosystem for efficient big data manipulation in R NSF grant for funding my trip, as well as CAUSE for generously waiving the registration fee. I also want to thank the many people that organized this conference, including Allan Rossman, Kelly McConville, Laura Ziegler, and Matt Beckman. The conference was incredibly well run and I cannot imagine how much work went it.\nThis was my first conference, and it was such a privilege to listen to talks and have discussions with many of the leaders in statistics and data science education. This was very inspiring, and also a bit overwhelming—these individuals have done so much and have so many smart and creative ideas. Both at the conference and typing up these reflections, it was hard to not feel some level of imposter syndrome—unlike the majority of attendees, I have never even taught a class! All of this is just to say, I have lots of to learn, and I feel so lucky to have gotten a chance to jumpstart that learning process by connecting with so many smart people (and am very grateful for how kind and welcoming they all were)!"
  },
  {
    "objectID": "blog/uscots2025/index.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "href": "blog/uscots2025/index.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "title": "My Reflections from USCOTS 2025",
    "section": "Life as a liberal arts statistician: joys and challenges",
    "text": "Life as a liberal arts statistician: joys and challenges\nIdeas for success in the classroom\n\n\nSurvey the class periodically and discuss the results (both things to change and why particular aspects will remain the same) with the students\n\nLearn from others, both in your department and others\n\n\nSit in on classes\n\n\nTake notes after class on what worked and what didn’t for the future\n\n\nBe active professionally\n\nPre-tenure, make strategic choices. Play the odds to get research that students can help with.\n\nPost-tenure, focus on what nourishes you (e.g. publishing an open source textbook)\n\nCollaborate with non-statistical collegues\n\nAdapt research question (and timeline for undergrads)\n\nServe others\n\nJump into service early, both internally and externally\n\nBe happy and proactive with “yeses”, then you can say “no” to things not as interesting to you\n\nGrowing a program\n\nHelpful to have a customizable concentration/minor\n\nHaving a center for interdisciplinary research is a good way to create community and can lead to research collaborations\n\nAdd modern courses to curriculum (e.g. add a data science course and remove an advanced modeling requirement)\n\nFind projects everywhere! Examples: a bagel shop running out of your favorite flavor, library checkout data\n\nMany aspects are constantly changing\n\nClassroom dynamics and design\n\nTechnology (classroom tech, programming languages)\n\nRising popularity of statistics and data science in recent years\n\nRelationship with math and computer science\n\nData ethics\n\n\nChallenges\n\nSemesters are hectic\n\nSalary\n\nGrading\n\n5% of students\n\nSlow changes in academia\n\nLifelong learning can be exhausting\n\n\nJoys\n\nIt usually doesn’t feel like work\n\nAwesome colleagues\n\nStudents want to make world better\n\nCreating a good learning environment\n\nConstant renewal\n\nCollaborating with experts in different fields\n\nTaking pride in your institution\n\nLifelong learning can be exhilarating\n\n\nShort reflection: The group I was in talked a lot about growing a program and the interaction between math, statistics, and computer science. Some people in the group were in a statistics/mathematics department separate from computer science, and mentioned this being a point of tension. Right now, when data science is so popular, it is ideal to have these departments collaborating and working together on what courses should count for credits to particular majors or minors (as well as just to maximize student learning). Having a combined department at Macalester, this was not something I’d thought about before, but it must make conversations about forming a data science major or minor, for instance, easier. Some other issues mentioned were struggle to find a form of service people find exciting, and issues with institutional financials. I hadn’t previously thought about either of these topics when thinking about a liberal arts career, so it was nice to listen to what others had to say."
  },
  {
    "objectID": "blog/uscots2025/index.html#authentic-assessment-with-oral-exams",
    "href": "blog/uscots2025/index.html#authentic-assessment-with-oral-exams",
    "title": "My Reflections from USCOTS 2025",
    "section": "Authentic assessment with oral exams",
    "text": "Authentic assessment with oral exams\nWith students now being able to offload parts of “writing to learn” assignments (e.g. data analysis reports) to LLMs, many people at this conference were interested in discussing alternative forms of assessment. One option that has potential in liberal arts classrooms is oral exams. While the potential benefit of oral exams is high, the primary concern seems to be time, both for the professor and students.\nPros\n\nPersonal connection between professor and student.\nProfessors having used oral exams noted they could tell level of the student’s understanding with just a couple of minutes.\n\nGoing off this, the students did not refute their score. The student could tell if they weren’t matching the professor’s expectation for level of understanding.\n\nOpportunity to understand the student’s misunderstandings and redirect them.\n\nFor example, if a student said, “The probability of the null being false is 95%”, you could ask a follow up question like, “Wait, can you explain a little more why that is the case?”.\n\nNo option for student to offload thinking to LLMs.\nGood practice for job interviews.\nOpportunity to meet the student where they are at. For students less comfortable with the material, you can redirect and ask them different questions than a student clearly grasping the concepts.\n\nCons\n\nTime. With 2-3 sections of a class with 20 students, you are trying to schedule 40-60 fifteen minute blocks. That is a lot of time out of the professor’s week, and it is also hard to find time outside of class for students with busy schedules.\n\nCan we do paired oral exams to cut down on that?\n\nHow are students then graded as a pair? What if one partner dominates the conversation? Side note: in an ungrading scenario, this would not be as much of an issue and something the students could reflect on themselves after the fact.\nIdea: Have the pair have a conversation between themselves based on a set of prompts given to them.\n\n\nStudent anxiety. The idea of an oral exam is daunting for many students.\n\nPaired exams could potentially help reduce this anxiety.\nCertain students may understand the concepts well but not be able to verbally communicate on the spot.\n\nAlternatively, you could say that some students have test anxiety, but this is just a common form of assessment so we typically ignore this issue.\n\n\nFairness and equity: can we ensure students are evaluated the same?\n\nOther notes\n\nAs opposed to an exam, how much material can you assess? If, for example, an exam were to cover machine learning methods such as LASSO, clustering, splines, random forests, and GAMs, are you assessing all of these methods in a 15 minute oral exam but sacrificing depth, or vice versa?\nOne professor found no correlation between student oral exam scores and test scores in a semester where they used both as modes of assessment.\n\nI found this really interesting, and don’t think is necessarily a pro or a con, but maybe a reason to utilize multiple forms of assessment within a semester."
  },
  {
    "objectID": "blog/uscots2025/index.html#project-design",
    "href": "blog/uscots2025/index.html#project-design",
    "title": "My Reflections from USCOTS 2025",
    "section": "Project Design",
    "text": "Project Design\nIn this discussion, we covered some ideas make sure students are getting the most they can out of class projects.\nProject setup\n\nProviding a sample report: should we or should we not?\n\nUseful so that students understand expectations, but they may follow a sample report too closely and not build their own writing style.\nIdea: provide a report with several flaws, and take class time to critique the report and discuss potential changes.\n\nPeer reviews: tell students it is not a time to be “MN nice”. Give honest feedback to help your classmate get a better grade.\nFor group projects, have formal meetings with each project group to discuss next modeling steps, what is going well or what they are struggling with, etc.\n\nPersonal note: I think effectively communicating what I have done and have questions on in an organized matter in a one-on-one meeting was one of the things I struggled with transitioning out of undergrad (both working at Mayo Clinic and starting research in my PhD). Maybe this could help prepare students for that.\n\n\nProject Ideas\n\nClass time activity: story boarding with data. Have students introduce an idea, most important aspects from EDA, conflict/main characters.\n\nStudents don’t necessarily have to even have data, just have them sketch and type of plot that they might expect.\nHave them decide on an ideal storyboard: this is a good way to get them to think about the project workflow and communicating big ideas (perhaps to a nonstatistical audience too).\n\nPodcast (~10 minutes) where you encourage students to write a script (so they think through what they will say) and the verbally communicate their findings with a partner.\n\nHave students send pdf of visualizations that they talk about.\nOne professor found a student that didn’t typically participate in class was great at this."
  },
  {
    "objectID": "blog/uscots2025/index.html#doing-data-science-in-positron",
    "href": "blog/uscots2025/index.html#doing-data-science-in-positron",
    "title": "My Reflections from USCOTS 2025",
    "section": "Doing data science in Positron",
    "text": "Doing data science in Positron\nThis workshop by Mine Cetinkaya-Rundel and Hadley Wickham a great first introduction to Positron, which I didn’t know anything about prior to this conference.\nWhat is it?\n\nA next-generation data science IDE that feels like a fusion of RStudio and VS Code.\n\nHappy note: RStudio is not going away!\n\n\nBenefits of Positron\n\nMain benefit: easily combine R and Python in one document\n\nPersonally not currently a Python user, but I know a lot of people that are and I think this could be great for classes or research where groups are using both!\n\nAbility to effortlessly switch between versions of R (e.g. R 4.3.3, 4.5.0)\nCommand palette (Ctrl/Cmd + Shift + P) makes searching for and executing commands easy\nEasy to customize layout and split screen between files.\nMultiple concurrent interpreter sessions, which can be a mix of different R versions, mix of R and Python sessions, or multiple instances of a single R version\n\nAllows you to run different code while something is taking a while to run! This is awesome to me.\n\nCan sort your environment (e.g. by most recent)\nAir: an extension that automatically formats your code nicely\nYou can Preview (Render) your document and it will appear in the viewer pane instead of opening in your internet browser—easy to make side by side changes\nArea for previewing \\(\\LaTeX\\) equations\nEasy to save and share plots\n\nOther differences\n\nNo inline plots\n\nOutputs to “plots” pane, which I think I agree with most people is nice because the inline plots can make your document a bit laggy.\n\nRun button is different/a bit smaller? Personally like the RStudio version better this is not a big deal.\nPositron automatically updates (good, I think?)\n\nThings for me to learn more about\n\nrig\nAir\nSnippets\nPositron Assistant\n\nOverall reflections\nVery cool, also a bit overwhelming—there are just so many features. Hadley and Mine mentioned when you see someone’s RStudio, you can easily become acquainted with their setup, but Positron is completely customizable (which is mostly a pro, but can perhaps be a con in a teaching setting?). At the same time, so many of these features seem useful and Mine and Hadley did an awesome job motivating and explaining the tool. I really like the ability to continue to run smaller code jobs while another piece is running in the background. The mix of versions of R is also great. I am not sure if/when I will be fully doing my coding in Positron over RStudio, but I am excited to get more experience with it."
  },
  {
    "objectID": "blog/uscots2025/index.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "href": "blog/uscots2025/index.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "title": "My Reflections from USCOTS 2025",
    "section": "A no bullshit guide to programming with LLMs in R",
    "text": "A no bullshit guide to programming with LLMs in R\nThis talk by Hadley Wickham introduced the ellmer R package, which he developed to interact with LLMs in R. As someone that enjoys the challenges and satisfaction of coding and figuring out little tricks, I don’t love the idea of offloading parts of that process to LLMs. But I know this is a part of the current reality that we live in, and can make some dreadful tasks much faster, so I wanted to listen with an open mind.\nOne of Hadley’s first motivating examples had us discuss at our table how we’d extract name and age from text data, such as the following:\n\nprompts &lt;- c(\"I go by Alex. 42 years on planet Earth and counting.\", \n             \"Hey, I'm turning 27 and my name is Jamal\", \n             \"I'm Lei Wei and nineteen years young.\",\n             \"My name is Brett and I'm turning the big 5-0 this year.\")\n\nClearly, this is not a simple regex expression due the mix of age being typed and numeric, names being 1-2 words, and the fact that there is no global pattern in sentence structure. I learned that this is something an LLM could do with essentially the following code:\n\nlibrary(ellmer)\nchat &lt;- chat_anthropic()\nchat$chat(\"Extract the name and age from each sentence I give you\")\n\nchat$chat(prompts[[1]])\n\nAnd this essentially returned the name and age for each prompt. I am not running the code above because you do have to obtain an API Key (which according to the package requires a developer account that you have to sign up and pay for, which poses its own challenges), but nonetheless this definitely caught my attention as a motivating example for LLMs. Writing regex to successfully get name and age from large data like this would be super difficult.\nHadley then went on to talk about ellmer’s ability to interact with external tools defined by the caller. One downfall of LLMs is that they may not know current information, such as the time of day. Defining an R function that knows the time and registering that tool with the LLM can solve that problem and allow you to answer queries like “How long ago exactly did the Chicago Cubs win the World Series?”. See this article for more information.\nI would assume if I were to use these tools for my research, I would want to understand how accurate they are. Hadley mentioned the vitals package which I will have to look at more if/when I work with any of these LLMs in R.\nMore interesting to me right now is the conversation around the pros and cons of using these tools, and also the impact they may have in education. These are the following things I caught from Hadley’s talk:\nPros\n\nLLMs are amazing for quickly generating demos, shiny apps, example data\n\nHadley showed an example of a rough sketch of a histogram, which he fed to Positron Assistant and told it to create a Shiny app using the Palmer Penguins data.\n\nThis worked… which was kind of frightening to me. At the same time it is cool because you get this base Shiny app code working right away (often the most frustrating part) and can go on to make modifications to your preference.\n\n\nGood at translations (e.g. latex \\(\\rightarrow\\) quarto, R code \\(\\rightarrow\\) stan, SQL \\(\\rightarrow\\) dplyr, json \\(\\rightarrow\\) to unit tests)\n\nAll I could think about was how many hours of my life I could saved instead of translating code from SAS to R at Mayo Clinic by hand…🥲\n\nWhile honestly this task was miserable to me, it was a big part of my job, and it definitely makes me scared for me entry level stats/data science positions which are places where people learn lots and gain skills that allow them to learn and move up in the field (or better understand what type of career they want).\n\n\nExplaining and critiquing code\n\nCons/concerns\n\nCost and equity of access: These tools do cost money, even if not that much. According to Hadley, $5 on Claude can get you pretty far and Gemini has a generous free tier.\nEnvironmental concerns: Hadley implied these are worth considering but small and decreasing on the individual level, and that flying to this conference, for example, is a much more detrimental environmental impact.\n\nI am sure that is true, but I just can’t help to think about the overarching impact if millions and millions of people are using these tools on a daily basis. This isn’t going to be a no impact situation.\n\nData privacy: a definite concern on the individual level. Not a problem for most bigger organizations as most data already lives in some cloud, and cloud providers run LLMs.\nReplacing artists: a definite risk at societal level. Hadley mentioned he is trying to supplement, not replace.\nEvil billionaires: we are just giving more money to evil tech people…there weren’t really any ideas for how to get around that.\n\nSo at the end of this I wasn’t really sure what to think. It was slightly encouraging to hear Hadley mention that there are many drawbacks of LLMs and in many ways coding is still a useful tool (e.g. making small changes, thinking critically, programming can still be faster than asking LLMs to do stuff for you), but in many ways, these tools make me feel bleh, and I think a lot more discussion about the place of these tools in statistics research and education is needed."
  },
  {
    "objectID": "blog/uscots2025/index.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "href": "blog/uscots2025/index.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "title": "My Reflections from USCOTS 2025",
    "section": "Leveraging LLMs for student feedback in introductory data science courses",
    "text": "Leveraging LLMs for student feedback in introductory data science courses\nAfter Hadley’s talk I went right into another LLM talk by Mine Cetinkaya-Rundel, which covered leveraging LLMs for student feedback in STA 199, an introductory data science and statistical thinking course at Duke (no prerequistes). The class had two exams (20% each), but the largest component of the student grade was once weekly lab assignments graded for accuracy (35%).\nAI policy for class\n\nStudents could use AI tools but must explicitly cite them, and the prompt could not be copied and pasted directly from the assignment (the students had to create the prompt themselves).\nStudents were not allowed to copy and paste the AI narrative verbatim to answer questions.\nStudents were welcome to ask AI questions to enhance their learning and understanding.\n\nProject 1\n\nGoal: A chatbot that hopefully generates good, helpful, and correct answers that come from course content and prefers terminology/methods taught in the course.\nTwo motivating reasons for this\n\nStudents don’t read previous questions on online forums that their classmates have asked, even if the instructor asks them to do this before posting.\nChatGPT usually dosen’t generate answers in line with course content (for example, may give a base R response when tidyverse is taught).\n\nTechnical details\n\nUses Retrevial Augmented Generation (RAG) to focus chatbot on course content and give it context. The chatbot gives the student direction to specific pages of interest in the course textbooks.\n\nThis is accomplished through combining semantic similarity and knowledge graph searches.\n\n\nSQL database of student results (completely anonymized to professor)\n\nSome good interactions, some copy-and-paste directly from assignment, some “fix my code” questions.\n\nEvidence that the AI policy was a bit too optimistic\n\nCannot say the majority of answers it gives is better than other LLMs, but no credit card and the fact that it points to course materials are both pluses.\n\nA kind of sad question: Is the chatbot to read from textbook more motivating to the student than the professor telling them they should? Unknown.\n\nProject 2\n\nGoal: A feedback chatbot that hopefully generates good, helpful, and correct feedback based on an instructor designed rubric and suggests terminology/methods taught in the course.\nMotivating reasons\n\nStudents use AI tools as a first step before thinking about how to approach the task.\n\nA chatbot could be like a friend in the classroom that you turn to for helping thinking through a problem.\nBut also, if it gives them code to run and it works, no thinking will happen. See Microsoft study The Impact of Generative AI on Critical Thinking.\n\nMaybe AI can help TAs redistribute their time toward higher value and more enjoyable touch points with students, and away from repetitive and error-prone tasks which often go unread (giving feedback).\nTAs don’t want to provide detailed feedback to answers generated with AI (not that everyone is doing their hw with AI, but some students are)\nIf very detailed rubrics are already being written to ensure grading equivalency across TAs, it is easy to hand these to LLMs.\n\n\nActivity and Thoughts\nWe then went into an activity where we prompted an LLM (ChatGPT) with the question and the student response and asked it to give feedback. The feedback was very verbose, and commented on several things not part of the question (e.g. the year column is a character, it should be numeric). All of this was technically true, but perhaps not the main point of the question. With a rubric and telling it to be to the point, it was better, but still a lot to parse through for a fairly simple question (which was a pivot longer essentially).\nWe had the following thoughts at my table:\n\nShould we be giving feedback on things not part of the question?\n\nOn a similar note, the LLM might comment on small things that do not matter, making it difficult for the student to identify important concepts from small ones\n\nFor a topic I am fairly comfortable with, the more feedback there is, the less likely I am going to be to read it (personally). LLMs just seem way to verbose to me.\n\nCan we ask students to fill out a form about what type of feedback they prefer? Do they want the “compliment sandwich”, or just straight to the point what is wrong? Is an LLM even capable of being straight to the point?\n\nI think we are assuming that goal of the LLM giving feedback that aligns with what is taught in the course is met, otherwise this would be an issue (e.g. the LLM says to use names_transform in pivot longer when in the course teaches students to modify names in a mutate statement).\nI could have misunderstood, but the start of the session, a point was made about using these LLMs for feedback, not grading. If the TAs are still grading, is the amount of time saved by the TAs in not providing feedback worth the faults that come with using an LLM? Does the grading align with the feedback?\n\nWhile I personally wouldn’t be ready to use LLMs for feedback if I was teaching, I am generally interested to learn more about how this evolves. The idea itself kind of blew my mind, and I think it could be quite useful (particularly for low stakes assignments in courses with hundreds of students). Mine mentioned that a few of the next steps were to continue model evaluation (e.g. cost, speed, accuracy) and tradeoffs as new LLMs are released, and to measure learning outcomes for students using the LLM feedback to understand the effectiveness of this approach."
  },
  {
    "objectID": "blog/uscots2025/index.html#topics-workshops",
    "href": "blog/uscots2025/index.html#topics-workshops",
    "title": "My Reflections from USCOTS 2025",
    "section": "Topics Workshops",
    "text": "Topics Workshops\nTwo other breakout sessions I attended were Jo Hardin and Nick Horton’s Leveraging data technologies to model bigger datasets and Paul Roback and Laura Boehm Vock’s Integrating Poisson regression into the undergraduate curriculum. Both of these were great! Personally I was not familiar with SQL and DuckDB together prior to the workshop, so it was fun to learn something new that could be really helpful when working with large data/databases. In the other workshop, I really liked the way Paul and Laura explained Poisson regression visually and with very minimal math. If I am able to co-instruct/instruct the CMU REU program at some point, I think it would be fun to have a couple sessions on a portion of the materials from both of these workshops."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html",
    "href": "teaching/llmwriting/llmwriting.html",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "",
    "text": "Large Language Models (LLMs) have become ubiquitous in academic settings, particularly for writing (Baek, Tate, and Warschauer 2024). Recently, Reinhart et al. (2024) identified systematic differences between LLM and human writing by leveraging Biber feature and lemma usage rates.\nThe overarching goal of our project is to identify whether (and if so, how) students’ statistics writing has systematically shifted toward being more similar to LLM academic writing since LLMs became widely accessible in 2022.\nOur data contains two corpora. The HAP-E Corpus contains 1,227 documents for which ChatGPT-4o (August 2024) was asked to generate the next 500 words (in the same tone and style) when prompted with a piece of academic writing (Brown 2024). The Student Corpus contains 2,353 student reports from three undergraduate statistics courses at Carnegie Mellon University. 36-202: Methods for Statistics & Data Science is typically the second statistics course students take. 36-401: Modern Regression and 36-402: Advanced Methods for Data Analysis are advanced courses students take in their junior or senior year. Students are given a dataset and asked to answer a domain question with a report in the IMRaD format. The average report length is 1,700 words. All reports are anonymized and collected under an IRB."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#comparing-rhetorical-features",
    "href": "teaching/llmwriting/llmwriting.html#comparing-rhetorical-features",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Comparing Rhetorical Features",
    "text": "Comparing Rhetorical Features\nWe start by extracting Biber feature rates (per 1,000 words) for each document in the HAP-E and Student Corpora. We then conduct linear discriminant analysis (LDA) on the ChatGPT and student reports from 2021 using standardized Biber feature rates across all documents. Throughout our analysis, we treat 2021 as a “pre-LLM era”, 2022 as an intermediate year, and 2023-2025 reports as being from a time when LLMs were utilized by and accesible to students. Using the standardized Biber feature rates, we then project 2022-2025 writing onto the first linear discriminant (LD1) space and observe the resulting distribution of LD1 scores. We do this process separately for introductory (36-202) and advanced classes (36-401/402). Due to the advanced classes having fewer reports—particularly none in 2024-2025—the remainder of our analysis focuses on 36-202.\n\n\n\nFigure 1: Distribution of LD1 scores by year for 36-202. The dashed line represents the mean LD1 score for each distribution.\n\n\nKey takeaways from Figure 1 include that 36-202 students are writing more like ChatGPT with each year, on average. We also observe more variability in LD1 scores in more recent years, suggesting that some students rely on ChatGPT for writing portion(s) of their report while other may not use it all.\nIn order to better understand what portions of the report students may use LLMs for, we break the report into five sections. We let the first 20% of sentences represent the introduction of the report, 20-80% represent the middle (EDA/methods) of the report, and the final 20% of sentences represent the conclusion. The results are displayed in Figure 2.\n\n\n\nFigure 2: Distribution of LD1 scores by section of the report, with 2021 reports in orange and 2023-2025 reports in blue.\n\n\nFigure 2 displays that across all sections of the report, students’ writing style is more similiar to ChatGPT’s writing style in 2023-2025 than it was in 2021, aligning with Figure 1. Breaking it down by section, even in 2021 (pre LLMs), the style of writing for the introduction and conclusion was more similar to ChatGPT’s style of writing than the methods section of the report. However, going from 2021 to 2023-2025, we see the distribution of LD1 scores shift toward ChatGPT’s LD1 distribution the most for the introduction and conclusion of the report. This implies these sections of the report are likely where students are using LLMs the most1.\nYou may be wondering what features of writing are more ChatGPT-like and what features are more human-like. Table 2 displays our top ten standardized Biber features that proved to be most helpful for classifying ChatGPT and student writing in our linear discriminant analysis. A negative LD1 coefficient indicates that the feature is more prevalent in ChatGPT writing than 2021 student writing, while a postive feature indicates the opposite. A few features that make sense to me: on average, ChatGPT uses longer words and more attributive adjectives, while students tend use be as a main verb and use first-person pronouns more often. Note that Biber features do not take punctuation into account, which I mention because LLMs like to use em dashes. This would be something additional to look into in the future.\n\n\n\nTable 2: Top 10 Biber features most important for classifying ChatGPT and and student writing in linear discriminant analysis. Some descriptions and examples are taken directly from Reinhart et al. (2024)."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#comparing-lemma-usage",
    "href": "teaching/llmwriting/llmwriting.html#comparing-lemma-usage",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Comparing Lemma Usage",
    "text": "Comparing Lemma Usage\nWe have looked at rhetorical features, but what about the words themselves? After hearing that LLMs at one point used words like delve, tapestry, or commraderie a lot, we were curious to see if our student writing saw an uptick in any of ChatGPT’s favorite verbs. To do so, we identify ChatGPT’s top 50 favorite verbs by doing a simple count of how many times each verb was used in the HAP-E corpus, removing auxiliary verbs. We then find the rate (per 1000 words) at which these verbs are used in the student corpus each year. Figure 3 displays how the frequency of usage of ChatGPT’s top ten favorite verbs have changed in student reports over recent years. Several of these verbs (e.g. provide, remain, ensure, reduce, address, enhance, offer) have seen noticeable increases.\n\n\n\nFigure 3: Frequency of ChatGPT’s top 10 favorite verbs in student writing since 2021.\n\n\nWe go on to perform a keyness analysis to identify which of the 50 verb lemmas have a significant positive frequency change from 2021 to 2023-2025 in the student writing. A keyness analysis is a special kind of chi-square test to compare observed and expected frequencies in text. We find that 75% of these 50 verb lemmas increased frequency in 36-202 student reports from 2021 to 2023-2025, and 33% of these positive changes were statistically significant after adjusting for multiple testing using the Bonferroni correction.\nWe want to make sure that the topic of the report does not play into student word choice (e.g. a prompt “analyze the effect of a reduction in required classes on…” may have the word reduce used very often). Luckily, in 36-202, students were given the same set of datasets and prompts to choose from each year. We additionally conduct a concordance analysis, pulling out the four words prior to and following each of top ten verbs in order to ensure the usage of these verbs is not context dependent. Figure 4 shows a sample of the ways in which provide was used in 36-202 reports in 2025. We see the usage does not appear to be context dependent, and we additionally observe repeated use cases such as “provide evidence-based recommendations”.\n\n\n\nFigure 4: Concordance analysis for provide."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#concluding-thoughts",
    "href": "teaching/llmwriting/llmwriting.html#concluding-thoughts",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nWe see there has been a systematic shift in both the style and vocabulary of students’ statistics reports toward ChatGPT in both lower and upper division courses at Carnegie Mellon in the era of LLMs. The writing style for students’ introductions and conclusions has particularly become more similar to ChatGPT’s writing style, on average.\nWe are still figuring out the next steps for this project, but it might be interesting to compose a ChatGPT corpus where each text is a report generated by the prompt of the assignment. This may be a more fair comparison to the student writing, as opposed to text generated by ChatGPT to meet the style and tone of expert academic writing. If this corpus was used in the linear discrminant analysis, I would expect an even larger shift of the distribution of LD1 scores for 2023-2025 reports toward to the ChatGPT distribution. Additionally, I would be curious to see if ChatGPT writing is “good”. If, using Biber features and linear discriminant analysis, we were to compare both student reports in the LLM and pre-LLM era to expert academic writing, would one perform better than the other? Finally, while it would be difficult to make happen, it would also be interesting to see how these results vary by institution (e.g. liberal arts versus R1 institutions).\nWhile the extensions above may help provide more information, that isn’t going to change what is clear—students are using LLMs for writing. Sara and I are more curious about what the means. We both went to liberal arts colleges where writing (e.g. data analysis projects, reports, portfolios, etc) was at the center of our learning. Personally, writing-to-learn assignments were the place where I would process concepts and immediately figure out my pitfalls when I could not figure out how to explain something. The big question (that I think everyone has) is: do these assignments need modification in an era of LLMs? If so, how do we restructure these assignments to better support student learning?\nAt USCOTS, it was really interesting to hear ideas on this. Some thoughts:\n\nTurn to alternative forms of assessment, such as oral exams. See my USCOTS reflections for a discussion the pros and cons of oral exams.\n\nIn short, I think this could be a good option for advanced capstone classes at liberal arts colleges, but it still may not offer the all of the same benefits as take home data analysis projects (the student has less freedom in topic, cannot produce something they can share to network or look back on to learn from, may have oral exam anxiety).\n\nHave students write a data analysis report during classtime—or more realistically, part of a report. We observed that students are most likely to offload writing the introduction and conclusion to LLMs, so potentially we could provide some already done EDA plots, methods, and results to the student and have them write the introduction or conclusion during classtime on paper.\n\nThere could also be a class discussion where as a group you read some pieces of writing and critique them (kind of like a peer review) so students think more about carefully about their writing style and process. Again, this does not yield all the same benefits as a take home data analysis project, but it is an idea.\n\nTry some AI policy—I liked Nick Horton’s idea where he asked the students in his class to develop an AI policy on the first day of the semester, he refined it, then they discussed it as a group again and students were expected to adhere to it.\n\nNot to be pessimisstic, I personally don’t this would work at a non-liberal arts college. I might be biased though…\n\nFinally and optimistically, maybe some form of pitching the laundry list of reasons why we shouldn’t use LLMs for writing reports could help solve the problem.\n\nThese are all just thoughts. Right now I don’t have experience teaching a class or know what I’d do if I were a professor, but I am excited to keep learning! Thanks so much for reading, and please feel free to reach out to me at efranke@andrew.cmu.edu with feedback, questions, or ideas :)"
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#acknowledgements",
    "href": "teaching/llmwriting/llmwriting.html#acknowledgements",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks so much to Alex Reinhart for his support on this project. We are also grateful to the TeachStat working group at Carnegie Mellon for their valuable feedback and suggestions, and Eric Shau for pre-processing the student reports."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#footnotes",
    "href": "teaching/llmwriting/llmwriting.html#footnotes",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnecdotally, this makes sense. I have had at least four people tell me they are using LLMs for their introductions/conclusions in the last couple of months.↩︎"
  }
]