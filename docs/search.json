[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erin Franke",
    "section": "",
    "text": "Hi, thanks for visiting my website!\nI am a first-year Statistics PhD student at Carnegie Mellon University in Pittsburgh, PA. I graduated from Macalester College in 2023, majoring in Statistics and minoring Computer Science and Economics.\nI am passionate about using statistics as a tool to better understand and solve complex interdisciplinary problems for societal good. I am currently working with Weijing Tang (CMU Stats & DS) and Phoebe Lam (CMU Psychology) to integrate existing studies in order to understand the influence of psychological stress on disease vulnerability, and whether social relationships, at what age, buffer this association.\nAs both a student and teaching assistant, I like thinking about effective ways to learn and teach statistics and data science. Currently, I am enjoying working with my cohort-mate, Sara Colando, on two projects. The first project aims to understand how the trajectory of undergraduate statistics student writing has changed since the introduction of ChatGPT, and the second focuses teaching data manipulation with the data.table R package. I look conducting further pedagogical research and gaining additional teaching experience throughout my PhD.\n\n\n\n\n\nAbout Me\nIn addition to statistics & data science, I also enjoy running, volunteering at my local food pantry in Chicago, hiking, going on walks while listening to podcasts, following women’s pro running & the MLB, and spending time with friends and family. This fall, I will be racing the 2025 Bank of America Chicago Marathon for charity, raising money for Nourishing Hope Food Pantry.\n\n\n\n\n\n\n\n\n\nMy siblings and I on a hike in Washington\n\n\n\n\n\n\n\nMy first marathon!\n\n\n\n\n\n\n\n\n\nRun with Kara Goucher and cohortmates\n\n\n\n\n\n\n\nSchenely Park: My favorite place in Pittsburgh!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Erin Franke",
    "section": "",
    "text": "BA in Statistics, 2023\nMacalester College; Saint Paul, MN\nResume"
  },
  {
    "objectID": "projects/statGen/index.html",
    "href": "projects/statGen/index.html",
    "title": "Statistical Genetics Summary",
    "section": "",
    "text": "In Fall 2022 I took Statistical Genetics, which introduced me to the field and got me really interested in pursuing a career in health/genetics related data science. I had a great time learning how to use both R and PLINK to analyze genomic data - you can check out everything I learned here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Undergrad Projects",
    "section": "",
    "text": "I am very grateful to have had many opportunities to work with real data during my undergrad! I personally find (a) applying methodology to data, and (b) making an effort to explain this work in writing or orally, to be the best way to process what I learn and understand what I don’t know. I worked on the projects below at Macalester. While the majority of these are not polished analyses, I like to keep them on website to refer back to for coding tricks or concept refreshers, and as a reflection of my development as a statistician :)\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGentrification and Crime in the Twin Cities: Insights and Challenges through a Statistical Lens\n\n\nAn independent honors project in statistics completed over my senior at Macalester College.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Oxygen Help? A Causal Analysis\n\n\nAn indepedent project to apply causal inference techniques to understand oxygen use when ascending the Himalayan Mountains.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Genetics Summary\n\n\nA summary of what I learned in Stat 494 Statistical Genetics in Fall 2022.\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounders & Omitted Variable Bias in Linear Regression\n\n\nMy final group project in my Mathematical Statistics class in Spring 2022.\n\n\n\n\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Spatial Analysis of Elevated Blood Lead Levels in the Twin Cities Metropolitan Region\n\n\nResearch using spatial techniques including SAR models, the SF package, and Matern Random Effects Modeling to study where children in the Twin Cities are testing with…\n\n\n\n\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 and Residential Property Crime in Chicago, IL\n\n\nMy econometrics project seeking to understand the impact of the COVID-19 pandemic on residential property crime in Chicago’s 77 community areas.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTommy John surgery and its Relationship to MLB Pitcher Career Trajectory\n\n\nResearch using Bayesian techniques aimed to understand the impacts of Tommy John surgery.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing wOBA with Launch Angle and Exit Velocity\n\n\nSummer research project with the goal to understanding how individual MLB players should swing to maximize their weighted OBP.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUS Universities and the SVD\n\n\nHow we identified a few very unique colleges using the SVD and Forbe’s data set on America’s Top Colleges of 2019.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA survival analysis of draft to debut date for MLB players\n\n\nA survival analysis project completed by Erin Franke and Corey Pieper in March 2021.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy first experience with data analysis for the community\n\n\nWhat I learned during my one month internship working with Common Pantry of Chicago, IL.\n\n\n\n\n\n\nJan 31, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/lead/index.html",
    "href": "projects/lead/index.html",
    "title": "A Spatial Analysis of Elevated Blood Lead Levels in the Twin Cities Metropolitan Region",
    "section": "",
    "text": "In Spring 2022 I got to take Correlated Data (STAT 452) and learned about how to work with time series, longitudinal, and spatially correlated data. I enjoyed combining my passion of mapping with modeling techniques that account for spatial correlation in order to learn more about the critical issue of childhood lead exposure in the Twin Cities, which can cause damage to a child’s brain and nervous system, slowed growth and development, and even comas or death with severe exposure. To learn more about what might cause elevated blood lead levels in children and where this issue is most apparent, check out our writeup on github."
  },
  {
    "objectID": "projects/econometrics/index.html",
    "href": "projects/econometrics/index.html",
    "title": "COVID-19 and Residential Property Crime in Chicago, IL",
    "section": "",
    "text": "In Fall 2021 I took Econometrics (ECON 381) at Macalester College. With the COVID-19 pandemic still looming and consistently and feeling like doing a project related to my hometown, I decided to learn more about COVID-19 and residential property crime on the neighborhood level. Over the course of my analysis (March 2019 - Feb 2021), I found one neighborhood to have a significant increase in residential property crime (Lincoln Square) and 15 neighborhoods to have a significant decrease in residential property crime. Overall, I found significant evidence that property crime decreased for Chicago as a whole with the onset of the pandemic. My statistical methods included fixed and random effects modeling as well as Granger causality. Please check out my paper to learn more!"
  },
  {
    "objectID": "projects/woba/index.html",
    "href": "projects/woba/index.html",
    "title": "Maximizing wOBA with Launch Angle and Exit Velocity",
    "section": "",
    "text": "In summer 2021 I was lucky enough to be part of a 15 person research cohort with a focus on sports analytics through Carnegie Mellon. Not only did I get to learn so many awesome statistical techniques, but I also got to apply them to my favorite topic (baseball) with people who were equally as passionate as me. My research was done in partnership with Sarah Sult (Washington University in St. Louis) and Brooke Coneeny (Swarthmore College) and advised by Adam Brodie of the Houston Astros.\nSome of my main takeaways from this project were how deeply you have to think as a statistician about underlying assumptions. So many times we thought were proud of a model we had developed and ready to apply it, and then realized how it doesn’t take into account something crucial to the game of baseball (like how if a batter changes their swing they will be thrown different pitches). This was also my first time conducting formal team research and I learned how lucky I was to be able to work with people with different strengths than my own. Sarah and Brooke are both computer science majors and were able to use their skills to debug some of our more complicated functions while I focused on data visualization and modeling.\nPlease check out our project repository and most recent presentation to learn more about the complex relationship between launch angle/attack angle, exit velocity, and wOBA!"
  },
  {
    "objectID": "projects/bayes/index.html",
    "href": "projects/bayes/index.html",
    "title": "Tommy John surgery and its Relationship to MLB Pitcher Career Trajectory",
    "section": "",
    "text": "In Fall 2021 I had the opportunity to take Bayesian Statistics (STAT 454) and complete a capstone project on a topic of choice. Hearing so much about the use of Bayesian techniques in sports, I decided to learn more about career trajectory for pitchers in the MLB, with a focus especially on Tommy John surgery. Please check out this website to learn more and feel free to reach out to me with questions and/or suggestions!"
  },
  {
    "objectID": "projects/survival/index.html",
    "href": "projects/survival/index.html",
    "title": "A survival analysis of draft to debut date for MLB players",
    "section": "",
    "text": "In this project we conducted a survival analysis of the time from draft date to debut date for major league baseball players. Our analysis specifically focuses on the variables of fielding position and whether a player was drafted in high school or college, and how these differences change the length of a player’s time between being drafted and making their MLB debut. For the complete analysis, please use the following link to visit our website."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Erin Franke",
    "section": "About Me",
    "text": "About Me\nIn addition to statistics & data science, I also enjoy all things running, hiking, volunteering at my local food pantry in Chicago, following Major League Baseball, and spending time with friends and family.\n\n\n\n\n\n\n\n\n\nMy siblings and I on a hike in Washington\n\n\n\n\n\n\n\nMy first marathon!\n\n\n\n\n\n\n\n\n\nMacalester commencement with cross country teammates\n\n\n\n\n\n\n\nManitou Incline: 2,744 steps of fun!"
  },
  {
    "objectID": "projects/svd/index.html",
    "href": "projects/svd/index.html",
    "title": "US Universities and the SVD",
    "section": "",
    "text": "This past spring, two of my classmates at Macalester College (Vivian Powell and Pippa Gallagher) and I decided to use the SVD to analyze United States colleges. The SVD - Singular Value Decomposition - is a matrix factorization that can be used for data reduction in machine learning, similarity analysis, image compression, and least squares regression among other uses. In this project, we used the SVD and a data set from Forbes 2019 about “America’s Top Colleges” to identify some of America’s most unique colleges and universities, at least in the sense of some of their basic demographics.\n\n\nGeneral Analysis\nUsing a data set with the 523 colleges Forbes selected, we took information on each school’s undergraduate population, student population, net price, average grant aid, total annual cost, alumni salary, acceptance rate, and mean SAT and ACT scores to build an original SVD plot. Giving each variable an equal weighing on a 0 to 1 scale, we got the following plot. \n\nA first step in the analysis of the SVD plot of the entire matrix was to look at the first two left singular vectors in order to understand which variables were most strongly associated with the horizontal axis and vertical axis in the plot of the first two right singular vectors. Below is a table that displays the left singular vectors U1 and U2, and which variable each entry represents:\n\nU1 and U2, which are the two most important left singular vectors of our matrix, essentially tell us which variables are the most influential on a college’s placement in the plot of V1 and V2. By analyzing U1, we can see which colleges are going to be closest to the direction of V1 (the negative horizontal axis in our case) and which will be furthest (the positive horizontal direction). We can see that the largest positive value in U1 is for Average Grant Aid, indicating that colleges with more grant aid will be closer to the direction of V1 and colleges with less grant aid will be farther from that direction. Total Annual Cost is similarly influential, with a slightly lower positive value. The largest negative value in U1 is for Acceptance Rate, which indicates that colleges with the lowest acceptance rates will be more negative on the x-axis, while colleges with high acceptance rates will be more positive on the x-axis. Overall, this tells us that colleges closer to the left side of the plot are likely to have a higher cost, higher grant aid, and a lower acceptance rate. We can expect this to include schools like the Ivies and “prestigious” liberal arts colleges with substantial endowments. On the other hand, colleges closer to the right side of the plot are likely to have a lower cost and less aid, as well as a higher acceptance rate. We could expect to see state schools with lower tuition in this category.\n\nLooking at U2, which tells us which variables are the most influential in a college’s placement on the vertical axis, we see that the largest positive value is for Acceptance Rate. This means that colleges with a higher acceptance rate will tend to have more positive y values, while colleges with lower acceptance rates will tend to have more negative y values. U2 also has two very large negative values, for Student Population and Undergraduate Population. This indicates that schools with a very large number of students will tend to be closer to the negative y-axis, while schools with few students will be closer to the positive y-axis. Overall, colleges near the top of the plot are likely to have a higher acceptance rate and/or smaller student populations, and colleges near the bottom of the plot are likely to be more selective and/or have larger student populations.\n\nClearly, this analysis isn’t perfect, because there will be schools with very low acceptance rates and tiny populations, and according to our analysis these schools won’t have a specific place on the plot. However, the information we gain from analyzing these first two columns of U is mostly to help us understand what the main factors are for grouping colleges in this way, and why two colleges might be plotted as opposites even if they are similar in some ways. Knowing that cost, financial aid, size, and acceptance rate are the most important variables for our SVD analysis will allow us to understand what makes some of these colleges unique.\n\n\n\nOutliers\n\nIn order to apply what we now know about U and V, we can look at the SVD plot and consider our outliers. Above is a plot of the SVD with labels on a few of the major outlier schools that we noticed in our analysis (BYU, University of Central Florida, and Liberty University). The major outlier we saw in almost all plots was BYU. Upon further research, we found that BYU has an incredibly low annual total cost of 18,370 dollars in comparison to almost all other universities. As a result of this, it also has a very low grant aid ($4843 annually). To really understand quite how low these numbers are in comparison to other universities, take a look at the density plots of total annual cost and grant aid below.\n\nThat’s pretty crazy! However, this information actually lines up with our SVD plots. If you remember, when we discussed singular vectors we said that colleges with more grant aid and a higher total annual cost would be much more on the negative side of the x axis and those with lower costs and financial aid would be on the right, positive side. BYU definitely follows this trend, being a huge right horizontal outlier on all of our plots.\nNow let’s see what might be going on at the University of Central Florida. Digging into the data, we found that the University of Central Florida has a large student population of 66,059 students. Looking at the student population density plot, there appear to be hardly any schools of this size.\n\nAdditionally, UCF’s average grant aid is $5757, which we know from the density plot of average grant aid is certainly on the lower end of the spectrum. The low grant aid explains this university’s positive V1 value on the SVD plot. The university’s large student population explains why UCF is such a noticeable vertical outlier.\nFinally, let’s investigate our third major outlier, Liberty University. Based on Liberty’s negative vertical location on the plot we might predict Liberty has a large student population or is a more selective university. From the positive V1 value we guess that Liberty tends to be a cheaper university and/or gives out less financial aid, though not quite to the degree as BYU or even UCF. Looking into the numbers, we find our size prediction confirmed - Liberty has the largest student population at 75,735 students! Furthermore, our cost prediction is correct as well as Liberty’s average grant aid is 10,400 dollars and their annual total cost is $38,364. Wow, our SVD plot seems pretty reliable!\n\n\nSingle Variable Analysis\n\nACT and SAT scores\nAfter completing the original plot, we decided it would be best to start looking at different variables and assessing how they appeared on the plot. For example, we took the ACT variable and divided it into three categories - low average ACT (&lt;24), medium average ACT (24-30), and high average ACT (30+). We then created plots with the universities colored by these groups - one plot without the ACT/SAT variables included in our main matrix (below left) and one with them included (below right). Green indicates universities with high ACT scores, blue indicates medium scores of 24-30, and red indicates low scores of less than 24.\n\nThere are subtle differences between the two plots, but you really have to look closely! This means that the ACT/SAT variable is highly correlated with another variable in the data set that was plotted originally. It makes most logical sense that this would be the acceptance rate variable, which we will analyze next. However, it is important to note that based on the somewhat distinct red, blue, and green groups in these plots, ACT score and potentially acceptance rate are pretty good indicators of what makes colleges similar and different.\n\n\nAcceptance Rate\nNext, we analyzed Acceptance Rate, as it seems to be one of the most influential variables based on our conclusions from U1 and U2. In order to do this, we defined low acceptance rates as &lt; 35%, medium acceptance rates as &gt; 35% and &lt; 70%, and high acceptance rates as &gt; 70%. We then repeated the same process used for SAT/ACT scores by creating two plots, both color-coded by acceptance rate: one where acceptance rate was included in the SVD, and one where it was not included. In the plots below, green represents the low acceptance rates, blue represents medium, and red represents high.\n\nThe first plot, with acceptance rate, looks very similar to the plots color coded for ACT/SAT score above (which makes sense given that colleges with similar acceptance rates will likely accept students with proportionally similar scores). There is a fairly clear distinction on the plot between colleges in different categories. Consistent with our conclusions from the left singular vectors earlier, we see that colleges with the lowest acceptance rates (green) tend to be towards the left and bottom sides of the plot, while colleges with high acceptance rates (red) tend to be along the top and right sides of the plot (and medium rates fall somewhere in between). However, when acceptance rate is removed from the matrix, the clarity of this pattern collapses significantly, indicating that the data of acceptance rate cannot be accurately represented as a linear combination of the other variables in the data set. So we can conclude that as we found earlier, acceptance rate is a fairly strong tool to group similar colleges by.\n\n\nPublic versus Private\nFinally, we chose to analyze the predictability of private vs public institutions. We did not include the private/public variable in our SVD as it held far too much weight due to being a binary variable with values of only 0 and 1. Instead, we will investigate whether other variables could accurately depict a college as private or public. In the figure below, blue represents public universities and red represents private universities. The separation between the two categories is definite with minimal mixing of red and blue near the top of the plot. This demonstrates that relying on other factors - including size, cost and acceptance rate - can fairly accurately encompass the information in the public vs private variable.\n\nThere are several visible outliers on this plot, including Liberty University and Brigham Young University-Idaho. While these universities are both private, they are plotted far on the positive side of the x-axis, far from any other private colleges. As we discussed earlier, this is due to their unique qualities. This graph demonstrates that private universities are more likely to have a higher tuition cost (falling on the negative x-axis), which is a common and generally accurate stereotype of private universities. However, Liberty and BYU have uncharacteristically low tuition and therefore could not be easily recognized as a private institution without the private/public factor included. If we were to include the private/public variable we can see a significantly clearer separation of the two categories, where the outliers are even easier to spot, located between the two groups of universities.\n\nIf this graph were not color coded a viewer might assume that Liberty University and Brigham Young University are public rather than private. This emphasizes the uniqueness of the two colleges and their unpredictability. While determining if a college is more likely to be private or public appears to be easy given other variables, there will always be outliers that don’t fall into the stereotypes of alike colleges.\n\n\n\nConclusion\nSVD analysis is not a perfect tool, but this paper has demonstrated that it carries great value in the ability to reduce a very large data set to something plottable in two dimensions that is visually digestible to the average reader. By using only the first two right singular vectors of the data, we can extract a much simpler representation of the vast majority of the information contained in the data set, and use it to understand colleges in a way that comparing schools by a single variable at a time simply would not achieve. Overall, the SVD is a good way to draw general and overarching conclusions rather than specific and pointed ones. The SVD takes advantage of having the ability to pull from a large data set that otherwise would take far too much storage, time, and machine power to analyze."
  },
  {
    "objectID": "projects/commonpantry/index.html",
    "href": "projects/commonpantry/index.html",
    "title": "My first experience with data analysis for the community",
    "section": "",
    "text": "Brief reflection\nDuring January of 2021 I was fortunate enough to have the opportunity to complete a one credit internship analyzing data with Common Pantry, my local food pantry in Chicago, IL. This internship was very rewarding as not only did I become much more comfortable in R and learn quicker and easier ways to analyze data, but in the process I was able to help out to my local community with information that could play a part of feeding more families in the coming months.\nOne of my main takeaways from this internship was the importance of good data. As a small nonprofit, a lot of Common Pantry’s data entry is done by hand which makes sense. Throughout my analysis, I encountered problems such as mismatched or missing units within the data, inconsistencies with data being recorded as characters or numeric entries, and simply missing information among other issues. This gave me a lot of practice cleaning the data, and learned new techniques to do so including using functions such as str_replace_all(), str_extract(), as.numeric(), and several others. I also got good practice with joining data sets horizontally and vertically, renaming columns, exporting and importing data from a variety of sources, and more.\n\n\nResults\nWhen COVID-19 hit in March 2020 and demand at Common Pantry spiked to about 2.11x their normal levels, Common Pantry could understandably no longer collect in depth information on clients such as their names, addresses, family size, and more. However, one piece of information they were able to collect was client zip code. I used this to create a basic density plot of the distribution of clients in Chicago. The Common Pantry service area, outlined in red, was removed once COVID-19 put many out of work.\n\nThe map above shows an important point, which is that a large portion of Common Pantry’s clients during COVID are coming from out of their service area and perhaps this service area needs to be enlarged for the long term (if Common Pantry can get adequate funding to do so). Four months after the conclusion of this project, one super exciting thing to note is that Common Pantry has recently announced the purchase of their own new and larger building only three blocks from the current pantry that will allow them to serve a greater amount of clients!\n\n\n\nCommon Pantry’s new location\n\n\nOutside of the zip code analysis, as a result of the limited 2020 data a large portion of my work was done with the 2019-2020 data on the donor appeal instead of information on the clients coming to the pantry. However, I was able to create the following plot which demonstrates the sheer number of clients visiting the pantry each month. Notice the spike in numbers as COVID begins and people are laid off of work in March and April.\n\nThankfully, while much of the community was in need of increased support for the majority of 2020, other members were able to step up and support Common Pantry to a greater level than before. In 2019, Common Pantry received 1,429 total donations, with 55.7% of them being financial. Total donations more than doubled in 2020 at 3,415, with 74.8% being financial. While the average donation amount fell in 2020, it was not by much, falling from 304.23 dollars in 2019 to $275.96 in 2020. For a visual summary of how donations changed throughout 2020 itself, see below.\n\n\n\nGrowth in number of donations throughout 2020\n\n\n\n\n\nA comparison of the value of monetary donations in 2019 versus 2020,by month\n\n\nAnother interesting thing I found when investigating the data was that despite monetary donations skyrocketing when COVID-19 hit, food donations stayed relatively constant. This makes sense - in the virtual environment, schools and businesses that may have been doing food drives in 2019 transitioned over to monetary fundraisers. This comparison of food versus monetary donations overtime can be seen below.\n\nThe remainder of the work I did with Common Pantry is more private, as I identified their top 75 donors of 2019-2020 to better inform them who to potentially thank and/or target for additional funds in the future. I also used R to filter for frequent donors and was able to develop a list of people giving a certain amount monthly. Additionally, I identified some of Common Pantry’s most successful appeals and campaigns for raising money (unsurprisingly, the COVID-19 appeal and the I am Your Neighbor fundraisers were incredibly successful).\n\n\nConclusion\nOverall, I am so happy I got to have this experience and give back in a new way to a group that I have been volunteering with since middle school. It is my hope that as I continue to learn more data science and R that I can help Common Pantry and other nonprofits in ways that I haven’t even imagined yet."
  },
  {
    "objectID": "projects/causal/index.html",
    "href": "projects/causal/index.html",
    "title": "Does Oxygen Help? A Causal Analysis",
    "section": "",
    "text": "In Spring 2023 I had the opportunity to take Causal Inference (STAT 451) and complete a capstone project on a topic of choice. As a hiker, I was interested to learn about the value of oxygen when ascending the Himalayan Mountains. Please check out this website to learn more about causal inference and this project."
  },
  {
    "objectID": "projects/honors/index.html",
    "href": "projects/honors/index.html",
    "title": "Gentrification and Crime in the Twin Cities: Insights and Challenges through a Statistical Lens",
    "section": "",
    "text": "Over the course of my senior year, I completed an independent research project in correspondence with my advisor, Victor Addona. Given that gentrification is highly contested phenomenon in today’s society and the neighborhood just north of Macalester has seen rapid redevelopment of the past several years, I chose to study the relationship between gentrification and crime in the Twin Cities. Using mapping, Poisson generalized linear models, and spatial modeling techniques, we did not find gentrification to yield meaningful decreases in violent crime or theft. To learn more, please download the completed paper here."
  },
  {
    "objectID": "projects/mathstat/index.html",
    "href": "projects/mathstat/index.html",
    "title": "Confounders & Omitted Variable Bias in Linear Regression",
    "section": "",
    "text": "In Spring 2022, I took Mathematical Statistics (STAT 455) where I deepened my understanding of theoretical statistics. Two of my classmates, Vivian Powell and Cheikh Fall, and I completed a final project on a topic of our choice which we taught to the remainder of our class. Given the importance of linear regression, we chose to dive deeper in a key assumption of ordinary least squares, exogeneity, and the consequences of omitted variable bias if this assumption does not hold. Please check out our our paper to learn more!"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Erin Franke",
    "section": "",
    "text": "Hi, my name is Erin (she/her). Thanks for visiting my page! \nI am a Statistics PhD student at Carnegie Mellon University. Previously, I graduated from Macalester College in May 2023 (Statistics major, Computer Science and Economics minors) and spent time at Mayo Clinic in a Statistical Programmer role. I am passionate about working with data to solve problems for the greater benefit of society, and have enjoyed applying statistical methodology to a variety of fields, including healthcare, sports, and urban studies/society. Please check out my Projects tab to learn more about my skills and see examples of my work."
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Tidy Tuesday!",
    "section": "",
    "text": "I first started doing Tidy Tuesday in my Intro to Data Science class in Summer 2021. While I often don’t have time to participate, it is my favorite way to grow my data visualization skills! This page includes all my Tidy Tuesdays from most recent to oldest. Click on the image to see the corresponding code."
  },
  {
    "objectID": "tidytuesday.html#section",
    "href": "tidytuesday.html#section",
    "title": "Tidy Tuesday!",
    "section": "2023",
    "text": "2023"
  },
  {
    "objectID": "tidytuesday.html#section-1",
    "href": "tidytuesday.html#section-1",
    "title": "Tidy Tuesday!",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "tidytuesday.html#section-2",
    "href": "tidytuesday.html#section-2",
    "title": "Tidy Tuesday!",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Thanks for visiting my blog!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nA Brief Introduction to Differential Privacy\n\n\n\nData Science\n\n\nData Ethics\n\n\n\nWhat does privacy mean in data analysis? Why is it important, and what does Differential Privacy offer?\n\n\n\nErin Franke\n\n\nMay 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA small collection of various R tricks\n\n\n\nR\n\n\n\nA selection of pieces of code I find to be useful or fun :)\n\n\n\nErin Franke\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/rtricks/index.html",
    "href": "blog/rtricks/index.html",
    "title": "A small collection of various R tricks",
    "section": "",
    "text": "Below is a collection of R code & tricks I have found helpful to have on hand. These are the smaller, more obscure pieces of code I forget and then repeatedly search online for. I hope to continue to add to this list overtime!\n\nData Wrangling\n\n1. coalesce()\nSource: post from appliedepi@bsky.social\nReturns the first non-missing value from a set of columns based on the order that you specify. This is great for preventing a long series of case_when() calls.\n\n\n\n2. Text\n\na. Removing accents\nIn a situation where we are working with strings with accents (e.g. cities, names, etc), we likely want all strings to be formatted in the same. The following shows an example where some cities have accents and some do not. We can remove all accents using stringi::stri_trans_general(), as shown below.\n\nhead(cities) %&gt;% gt() # gt() just makes table look nice :) \n\n\n\n\n\n\n\nnames\n\n\n\n\nBogotá\n\n\nBogota\n\n\nQuébec\n\n\nQuebec\n\n\nÎle de la Cité\n\n\nIle de la Cite\n\n\n\n\n\n\n\n\ncities %&gt;%\n  mutate(names = stringi::stri_trans_general(names, \"Latin-ASCII\")) %&gt;% \n  count(names) %&gt;% gt()\n\n\n\n\n\n\n\nnames\nn\n\n\n\n\nBogota\n2\n\n\nIle de la Cite\n2\n\n\nQuebec\n2\n\n\n\n\n\n\n\n\n\nb. unnest_tokens()\nThe unnest_tokens() is helpful for analyzing word counts of text, specifically that might be split up across many lines. I first used this function to analyze historical markers data. The example below is taken from the unnest_tokens() help page.\n\nlibrary(janeaustenr) # for this example\nlibrary(tidytext) # for unnest_tokens()\n\nnovel &lt;- tibble(txt = prideprejudice)\nhead(novel, 15) %&gt;%\n  gt()\n\n\n\n\n\n\n\ntxt\n\n\n\n\nPRIDE AND PREJUDICE\n\n\n\n\n\nBy Jane Austen\n\n\n\n\n\n\n\n\n\n\n\nChapter 1\n\n\n\n\n\n\n\n\nIt is a truth universally acknowledged, that a single man in possession\n\n\nof a good fortune, must be in want of a wife.\n\n\n\n\n\nHowever little known the feelings or views of such a man may be on his\n\n\nfirst entering a neighbourhood, this truth is so well fixed in the minds\n\n\nof the surrounding families, that he is considered the rightful property\n\n\n\n\n\n\n\n\nnovel %&gt;%\n  tidytext::unnest_tokens(output = word, input = txt) %&gt;%\n  head(20) %&gt;%\n  gt()\n\n\n\n\n\n\n\nword\n\n\n\n\npride\n\n\nand\n\n\nprejudice\n\n\nby\n\n\njane\n\n\nausten\n\n\nchapter\n\n\n1\n\n\nit\n\n\nis\n\n\na\n\n\ntruth\n\n\nuniversally\n\n\nacknowledged\n\n\nthat\n\n\na\n\n\nsingle\n\n\nman\n\n\nin\n\n\npossession\n\n\n\n\n\n\n\nWe could then go on to count the number of times each word is used!\n\n\nc. Removing punctuation\nLike accents, we might also want to remove punctuation. We can do this with the [:punct:] regular expression!\n\ndata %&gt;% gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy :)\n\n\nexcited!!\n\n\n**excited**\n\n\nsad :(\n\n\nangry,\n\n\nupset?\n\n\n#mad\n\n\n\n\n\n\ndata %&gt;%\n  mutate(feelings = str_replace_all(feelings, \"[:punct:]\", \"\")) %&gt;% \n  gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy\n\n\nexcited\n\n\nexcited\n\n\nsad\n\n\nangry\n\n\nupset\n\n\nmad\n\n\n\n\n\n\n\n\n\nd. Separate list of words in a column!\nMany times I have ran into there being a list of words separated by a comma in a column of dataset. For example, asking participants to list the most memorable characteristics of a chocolate that they tasted (analyzed in this tidy tuesday).\nThis is some code to count how many time each word is used.\n\nhead(chocolate) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\n\n\n\n\n2454\nrich cocoa, fatty, bready\n\n\n2458\ncocoa, vegetal, savory\n\n\n2454\ncocoa, blackberry, full body\n\n\n2542\nchewy, off, rubbery\n\n\n2546\nfatty, earthy, moss, nutty,chalky\n\n\n2546\nmildly bitter, basic cocoa, fatty\n\n\n\n\n\n\nlist_of_adjectives &lt;- chocolate %&gt;%\n  mutate(id = row_number(), # gives each reviewer/chocolate combination a unique id\n         most_memorable_characteristics = strsplit(as.character(most_memorable_characteristics), \",\")) %&gt;% # split on the comma to create a list of characteristics for each individual\n  unnest(most_memorable_characteristics) # unlists the list, one in each column\n\nhead(list_of_adjectives) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\nid\n\n\n\n\n2454\nrich cocoa\n1\n\n\n2454\nfatty\n1\n\n\n2454\nbready\n1\n\n\n2458\ncocoa\n2\n\n\n2458\nvegetal\n2\n\n\n2458\nsavory\n2\n\n\n\n\n\n\n\n\n# can then count the number of each adjective, or separate into separate columns:\nlist_of_adjectives %&gt;%\n  group_by(id) %&gt;%\n  mutate(adjnum = paste0(\"adj\", row_number(id))) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = c(reviewer, id), names_from = adjnum, values_from = most_memorable_characteristics) %&gt;%\n  head() %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nid\nadj1\nadj2\nadj3\nadj4\nadj5\n\n\n\n\n2454\n1\nrich cocoa\nfatty\nbready\nNA\nNA\n\n\n2458\n2\ncocoa\nvegetal\nsavory\nNA\nNA\n\n\n2454\n3\ncocoa\nblackberry\nfull body\nNA\nNA\n\n\n2542\n4\nchewy\noff\nrubbery\nNA\nNA\n\n\n2546\n5\nfatty\nearthy\nmoss\nnutty\nchalky\n\n\n2546\n6\nmildly bitter\nbasic cocoa\nfatty\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n3. Make things faster\n\na. across()\nacross() lets us apply one function to many columns at once, such as below getting the average of each respective penguin measurement column.\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\npenguins %&gt;% \n  summarise(across(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g), ~mean(.x, na.rm=T))) %&gt;%\n  gt()\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n43.92193\n17.15117\n200.9152\n4201.754\n\n\n\n\n\n\n\n\n\nb. mutate if\nUse mutate_if() to apply a particular function under certain conditions.\n\npenguins %&gt;% \n  mutate_if(is.double, as.integer) %&gt;%  # IF is numeric, make AS integer\n  head(3) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39\n18\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39\n17\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40\n18\n195\n3250\nfemale\n2007\n\n\n\n\n\n\n\n\n\nc. data.table\nThe data.table package’s fread() function loads data into R more efficiently than many of the read_X() functions. When loading a large dataset, try replacing the read_X() with fread()!\nExample of loading in tidy tuesday data:\n\nlibrary(data.table)\n\nhistorical_markers &lt;- fread('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-04/historical_markers.csv')\n\n\n\n\n\nData Visualization\n\n1. Title placement\n\na. Move plot title left\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nb. Center title/subtitle\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n2. Color\n\na. Change color of part of title\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", \n       title = \"Penguin &lt;strong&gt;&lt;span style='color:navy'&gt; flipper&lt;/span&gt;&lt;/strong&gt;&lt;/b&gt; vs. &lt;strong&gt;&lt;span style='color:goldenrod3'&gt; bill &lt;/span&gt;&lt;/strong&gt;&lt;/b&gt;length\")+\n  theme_classic()+\n  theme(plot.title = ggtext::element_markdown()) # must have element_markdown() to make this work!!\n\n\n\n\n\n\n\n\n\n\nb. Color palette finder\nThis website is a super fun way to visualize a bunch of different color palettes! You can choose a type (qualitative, diverging, essential), a target color, and the palette length. You can also select for different types of color blindness.\n\n\nc. Change plot background color\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.background = element_rect(fill = \"lightblue3\"), \n        panel.background = element_rect(fill = \"lightblue3\"))\n\n\n\n\n\n\n\n\n\n\n\n3. Annotate plot\n\na. Text\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  annotate(geom=\"text\", x=220, y=3500, label = \"There is a \\nlinear relationship\",\n           color = \"navy\", size=3)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nb. Add an arrow\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  geom_curve(aes(x = 220, xend = 211, y = 3500, yend = 4000),arrow = arrow(length = unit(0.03, \"npc\")), curvature = 0.4, color = \"navy\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n4. Fonts\nI like the showtext package to change fonts. Look here for fonts.\n\nlibrary(showtext)\n\nfont_add_google(\"Shadows Into Light\") # choose fonts to add\nfont_add_google(\"Imprima\")\nfont_add_google(\"Gudea\")\nshowtext_auto() # turns on the automatic use of showtext\n\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(family = \"Imprima\"))\n\n\n\n\n\n\n\n\n\n\n5. Images\n\na. general\n\nlibrary(png)\npenguin_pic &lt;- readPNG(\"images/penguin.png\", native=TRUE)\n\n\nplot &lt;- penguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()\n\nplot +                  \n  patchwork::inset_element(p = penguin_pic,\n                left = 0.87,\n                bottom = 0.5,\n                right = 1,\n                top = 0.7)\n\n\n\n\n\n\n\n\n\n\nb. aes\nYou might not want to do this for a ton of points, but you can replace a typical geom_point() with geom_image(), or think of other fun ways to use this (e.g. putting images at the end of bar chart).\n\npenguins %&gt;%\n  head(10) %&gt;% # select 10 points\n  mutate(img = \"images/penguin.png\") %&gt;%\n  ggplot()+\n  ggimage::geom_image(aes(x = flipper_length_mm, y = body_mass_g, image=img), size=0.06) +\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\n\nOther R tricks\n\n1. Rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑\n\n\n2. Custom Default Theme\nDuring college (and still now) I liked to modify the theme() of a plot a lot. This ended up with a lot of repeated code, so my advisor, Brianna Heggeseth, taught me how to setup a custom default theme. Thanks, Brianna! :) Here are the steps.\n\nOpen you .Rprofile file by pasting file.edit(file.path(\"~\", \".Rprofile\")) in the console\nModify this file to load your theme when a specific package is loaded (for example, ggplot2). Then, put your desired theme in theme_set(). This could be something as simple as ggplot2::theme_classic(). Below, I put a very simple example of a custom theme.\n\n\nsetHook(packageEvent(\"ggplot2\", \"onLoad\"), \n        function(...) ggplot2::theme_set(ggplot2::theme_classic()+\n                                           ggplot2::theme(plot.title.position = \"plot\",\n                                                 plot.title = ggplot2::element_text(family = \"mono\"))))\n\nA few notes:\n- You must call each function you use with the appropriate library in order for this to work.\n- When you are done, save the .Rprofile and quit RStudio in order for the changes to be saved.\n\nHere is a source I used to help refresh my memory on doing this.\n\n\n3. Highlighting code\nWhile making this blog post, I wanted to be able to highlight the lines of code that corresponded to the topic I was discussing. To do this, I installed the line-highlight extension for Quarto and followed the directions.\n\n\n\nThanks!\nThanks for reading my first blog post! If you have any R tricks you know of or feedback on this post, please email me at efranke@andrew.cmu.edu; I’d love to hear :)"
  },
  {
    "objectID": "blog/rtricks/index.html#data-wrangling",
    "href": "blog/rtricks/index.html#data-wrangling",
    "title": "My favorite R Tricks",
    "section": "Data Wrangling",
    "text": "Data Wrangling"
  },
  {
    "objectID": "blog/rtricks/index.html#data-visualization",
    "href": "blog/rtricks/index.html#data-visualization",
    "title": "My favorite R Tricks",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nOther R tricks\n\nSet up rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑"
  },
  {
    "objectID": "blog/rtricks/index.html#other-r-tricks",
    "href": "blog/rtricks/index.html#other-r-tricks",
    "title": "My favorite R Tricks",
    "section": "Other R tricks",
    "text": "Other R tricks\n\nSet up rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑"
  },
  {
    "objectID": "blog/differentialprivacy/index.html",
    "href": "blog/differentialprivacy/index.html",
    "title": "A Brief Introduction to Differential Privacy",
    "section": "",
    "text": "In this blog post, I discuss Differential Privacy, a mathematical definition of privacy that has grown in popularity over the last decade. This post is meant to give the reader an understanding of what privacy is, why historical techniques fail, and what Differential Privacy offers. I recommend checking out the references at the bottom of this page if you are interested in learning more!"
  },
  {
    "objectID": "blog/differentialprivacy/index.html#properties-of-differential-privacy",
    "href": "blog/differentialprivacy/index.html#properties-of-differential-privacy",
    "title": "A Brief Introduction to Differential Privacy",
    "section": "Properties of Differential Privacy",
    "text": "Properties of Differential Privacy\nWe next discuss mathematically how Differential Privacy satisfies the desired components of privacy—composition, post-processing, and group privacy. These properties hold for any mechanism that satisfies Equation 2.\n\nComposition\nComposition guarantees that Differential Privacy remains when combining multiple differentially private mechanisms. It aggregates the privacy guarantees of individual mechanisms to analyze the overall privacy loss of an algorithm, a process known as privacy accounting. Dwork and Roth (2014) found the following:\nTheorem 3.1: Let \\(\\mathcal{M}_i: \\mathcal{D} \\to \\mathcal{R}_i\\) be an \\(\\epsilon_i\\)-differentially private mechanism for \\(i \\in \\{1, 2\\}\\). Then, their composition, defined as \\(\\mathcal{M}(D) = (\\mathcal{M}_1(D), \\mathcal{M}_2(D))\\), is \\((\\epsilon_1 + \\epsilon_2)\\)-differentially private.\nProof: For any \\((R_1, R_2) \\subseteq \\mathcal{R}_1 \\times \\mathcal{R}_2\\) and any two neighboring datasets \\(D \\sim D'\\),\n\\[\n\\begin{aligned}\n\\frac{\\text{Pr}[\\mathcal{M}(D) \\in (R_1, R_2)]}{\\text{Pr}[\\mathcal{M}(D') \\in (R_1, R_2)]}\n&= \\frac{\\text{Pr}[\\mathcal{M}_1(D) \\in R_1] \\, \\text{Pr}[\\mathcal{M}_2(D) \\in R_2]}{\\text{Pr}[\\mathcal{M}_1(D') \\in R_1] \\, \\text{Pr}[\\mathcal{M}_2(D') \\in R_2]} \\\\\n&= \\left( \\frac{\\text{Pr}[\\mathcal{M}_1(D) \\in R_1]}{\\text{Pr}[\\mathcal{M}_1(D') \\in R_1]} \\right)\n   \\left( \\frac{\\text{Pr}[\\mathcal{M}_2(D) \\in R_2]}{\\text{Pr}[\\mathcal{M}_2(D') \\in R_2]} \\right) \\\\\n   &\\leq \\exp(\\epsilon_1)\\exp(\\epsilon_2) \\\\\n   &= \\exp(\\epsilon_1 + \\epsilon_2)\n\\end{aligned}\n\\tag{3}\\]\nIn Equation 3, we assume independence between \\(\\mathcal{M}_1\\) and \\(\\mathcal{M}_2\\). This assumption is usually incorrect, but in practice it is a good enough approximation (Desfontaines 2019). We then use the assumption \\(M_i\\) is \\(\\epsilon_i\\) differentially private, meaning \\(\\frac{\\text{Pr}[\\mathcal{M}_i(D) \\in R_i]}{\\text{Pr}[\\mathcal{M}_i(D') \\in R_i]} \\leq \\exp(\\epsilon_i)\\). Theorem 3.1 can be generalized to \\(k\\) differentially private mechanisms. If \\(\\mathcal{M}_i: \\mathcal{D} \\to \\mathcal{R}_i\\) is an \\(\\epsilon_i\\)-differentially private mechanism for \\(i = 1, \\dots, k\\), then composition \\(\\mathcal{M}(D) = (\\mathcal{M}_1(D), \\dots, \\mathcal{M}_k(D))\\) is \\((\\sum_{i=1}^k \\epsilon_i)\\)-differentially private.\n\n\nPost-processing immunity\nPost-processing immunity ensures that a data analyst cannot compute a function of the output of a private algorithm \\(\\mathcal{M}\\) and make it less differentially private (Dwork and Roth 2014).\nTheorem 3.2: Let \\(\\mathcal{M}: \\mathcal{D} \\to \\mathcal{R}\\) be an \\(\\epsilon\\)-differentially private mechanism and \\(g: \\mathcal{R} \\to \\mathcal{R}'\\) be a data-independent mapping. The mechanism \\(g \\space\\circ \\mathcal{M}\\) is \\(\\epsilon\\)-differentially private.\nProof: The proof can be done for a deterministic function \\(g: \\mathcal{R} \\to \\mathcal{R}'\\). Any randomized mapping can be decomposed into a convex combination of deterministic functions. The result follows because a convex combination of differentially private mechanisms is differentially private. Fix any pair of datasets \\(D\\) and \\(D'\\) with \\(\\Vert D-D'\\Vert_1 \\leq 1\\), and fix any event \\(S \\subseteq \\mathcal{R}'\\). Let \\(T = \\{r \\in \\mathcal{R}: g(r) \\in S\\}\\). Then:\n\\[\\begin{aligned}\n    \\text{Pr}[g(\\mathcal{M}(D)) \\in S] &= \\text{Pr}[\\mathcal{M}(D) \\in T] \\\\\n    &\\leq \\exp(\\epsilon)\\text{Pr}[\\mathcal{M}(D') \\in T] + \\delta \\\\\n    &= \\exp(\\epsilon) \\text{Pr}[g(\\mathcal{M}(D')) \\in S]+\\delta\n\\end{aligned}\\]\nwhich is what we wanted.\n\n\nGroup Privacy\nGroup privacy takes a privacy guarantee from the individual level to the group level. For example, group privacy addresses privacy in surveys that include multiple family members.\nTheorem 3.3: Let \\(\\mathcal{M}: \\mathcal{D} \\to \\mathcal{R}\\) be an \\(\\epsilon\\)-differentially private mechanism and let datasets \\(D\\) and \\(D'\\) differ in \\(k\\) entries. Then for all \\(S \\subseteq \\mathcal{R}\\):\n\\[\\text{Pr}[\\mathcal{M}(D) \\in S] \\leq \\exp(k\\epsilon) \\text{Pr}[\\mathcal{M}(D') \\in S].\\]\nProof: Assume \\(\\mathcal{M}\\) satisfies \\((\\epsilon, 0)\\)-Differential Privacy. If \\(D\\) and \\(D'\\) are two datasets differing by \\(k\\) rows, we can construct \\(D=D^{(0)}, D^{(1)}, D^{(2)}, \\dots, D' = D^{(k)}\\) where \\(D^{(i)} \\sim D^{(i+1)}\\) for \\(i=0, \\dots, k-1\\). These are intermediate datasets obtained when going from \\(D\\) to \\(D'\\) by changing one entry at a time successively. Then, by the Differential Privacy guarantee of \\(\\mathcal{M}\\), for any \\(R \\subseteq \\mathcal{R}\\) and \\(i \\in [k-1]\\),\n\\[\\begin{aligned}\n\\text{Pr}[\\mathcal{M}(D) \\in R] &= \\text{Pr}[\\mathcal{M}(D^{(0)}) \\in R] \\\\\n&\\leq \\exp(\\epsilon) \\text{Pr}[\\mathcal{M}(D^{(1)}) \\in R] \\\\\n&\\leq \\exp(2\\epsilon) \\text{Pr}[\\mathcal{M}(D^{(2)}) \\in R] \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n&\\leq \\exp(k\\epsilon) \\text{Pr}[\\mathcal{M}(D^{(k)}) \\in R] \\\\\n&= \\exp(k\\epsilon) \\text{Pr}[\\mathcal{M}(D') \\in R]. \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "blog/differentialprivacy/index.html#the-laplace-and-exponential-mechanism",
    "href": "blog/differentialprivacy/index.html#the-laplace-and-exponential-mechanism",
    "title": "A Brief Introduction to Differential Privacy",
    "section": "The Laplace and Exponential Mechanism",
    "text": "The Laplace and Exponential Mechanism\nHaving now introduced Differential Privacy and proved its guarantees, we discuss how noise is added to the data so that these guarantees hold. The Laplace mechanism is a differentially private mechanism based on the Laplace distribution for answering numeric queries (Dwork et al. 2006). It functions by computing the output of query \\(f\\) and then adding random noise drawn from the Laplace distribution independently to each of the \\(d\\) dimensions of the query response. The Laplace distribution has mean 0, scale \\(b\\), and probability density function \\(\\text{Lap}(x|b) = \\frac{1}{2b}e^{-\\frac{|x|}{b}}\\). Scale \\(b\\) is calculated by the global sensitivity \\(\\Delta_p f\\) divided by \\(\\epsilon\\) (Fioretto, Hentenryck, and Ziani 2024). The Laplace mechanism achieves \\((\\epsilon,0)\\)-Differential Privacy by bounding the ratio of output probabilities between \\(D\\) and \\(D'\\) by \\(\\exp(\\epsilon)\\).\nThe exponential mechanism (McSherry and Talwar 2007) is capable of performing selection privately while also preserving the quality of the selection made. It is intended to be used in situations where we wish to choose the “best” response, but adding noise directly to the computed quantity can completely ruin its value. For example, at an auction, the goal is to maximize revenue. If we were to add a small amount of positive noise to the optimal price in order to protect the privacy of the bid, it could dramatically reduce the resulting revenue. The exponential mechanism takes a set of objects \\(\\mathcal{H}\\), a dataset \\(D \\in \\mathcal{D}\\), and a utility function \\(s: \\mathcal{D} \\times \\mathcal{H} \\to \\mathbb{R}\\) and outputs \\(h \\in \\mathcal{H}\\) with probability proportional to \\(\\exp\\left(\\frac{\\epsilon s(D, h)}{2 \\Delta s}\\right)\\), where the global sensitivity of the utility function is \\(\\Delta s \\equiv \\underset{h \\in \\mathcal{H}}{\\max} \\underset{D \\sim D'}{\\max}|s(D, h) - s(D', h)|\\). The exponential mechanism achieves \\((\\epsilon, 0)\\)-Differential Privacy.\n\nApplication of the Laplace Mechanism\nSuppose we return to Equation 1 where we define the global sensitivity of a mechanism computing the average age in a dataset. We now show how the Laplace mechanism can be applied in practice, using a dataset of 1,000 individuals of ages 0 to 100 years. We first compute the global sensitivity,\n\\[\\Delta f = \\frac{\\max \\text{age} - \\min \\text{age}}{n} = \\frac{100}{1,000} = 0.1.\\]\nWe then select privacy parameter \\(\\epsilon=0.5\\) and add noise drawn from the Laplace distribution with the scale parameter \\(\\frac{\\Delta f}{\\epsilon}\\).\n\\[\\text{noise} \\sim \\text{Lap}\\left(\\frac{\\Delta f}{\\epsilon}\\right) = \\left(\\frac{0.1}{0.5}\\right) = \\text{Lap(0.2)}\\]\nThe differentially private query then reports \\(f\\)(data) + noise. For instance, if the average was truly 43.8 years, and we drew 0.13563 using R’s rlaplace(n = 1, location = 0, scale = 0.2), we would report the average age as 43.936 years.\n\n\nApplication of the Exponential Mechanism\nThe following situation is adapted from (Dwork and Roth 2014). A retailer is selling bedframes and there are three potential buyers, \\(A\\), \\(B\\), and \\(C\\), each of whom have a maximum price they are willing to pay (their valuation). The buyers keep their valuations to themselves to avoid disclosing private information about their finances. The retailer wants to determine a sale price to maximize their total revenue without revealing the valuations of the buyers in the process. Suppose the valuations of the buyers \\(A\\), \\(B\\), and \\(C\\) are $400, $700, $1,000, respectively. The following prices options would give the corresponding revenues:\n\nPrice of $400: Revenue = $400 \\(\\times\\) 3 buyers = $1,200\nPrice of $700: Revenue = $700 \\(\\times\\) 2 buyers = $1,400\nPrice of $1,000: Revenue = $1,000 \\(\\times\\) 1 buyers = $1,000\n\nTo maximize revenue, the seller should set the price at $700. If the seller added random noise to the valuations to preserve privacy, however, \\(B\\)’s valuation could become $701 (while remaining $700 in reality), leading only \\(C\\) to purchase the bed at the price of $701 and the seller to make $701 instead of $\\(1,400\\). To account for this situation, the exponential mechanism is useful. The seller can define utility function \\(s(D,h)\\) that calculates the total revenue generated at a price \\(h\\), given the buyers’ valuations in dataset \\(D\\). The exponential mechanism then selects a price \\(h\\) with probability proportional to \\(\\exp\\left(\\frac{\\epsilon s(D, h)}{2 \\Delta s}\\right)\\). As \\(\\epsilon \\to 0\\), all prices become equally likely independently of the buyers’ valuations \\(D\\), leading to perfect privacy. As \\(\\epsilon\\) increases, more importance is given to the utility function \\(s(D, h)\\), providing higher utility to the seller but less privacy to the buyer."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Currently under construction, coming July 2025 🚧.\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]