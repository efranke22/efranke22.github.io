[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erin Franke",
    "section": "",
    "text": "I am a second-year Statistics PhD student at Carnegie Mellon University in Pittsburgh, PA. I graduated from Macalester College in 2023, majoring in Statistics and minoring Computer Science and Economics.\nI recently completed a year-long project in collaboration with Weijing Tang (CMU Stats & DS) and Phoebe Lam (CMU Psychology) that integrated existing studies to understand the influence of perceived stress and negative emotion on disease vulnerability. I am part of the CMU Delphi group and am looking forward to starting research advised by Will Townes beginning in the spring semester. More information about past and current projects can be found on my Research page.\nAt CMU, I have been fortunate to both TA and become involved in pedagogical research. Visit my Teaching page to learn more about some recent projects.\n\n\n\n\n\nAbout Me\nIn addition to statistics & data science, I also enjoy all things running, hiking, volunteering with Common Pantry, entering the CAUSE caption contest, and spending time with friends and family.\n\n\n\n\n\n\n\n\n\nMy siblings and I on a hike in Washington\n\n\n\n\n\n\n\nMy first marathon!\n\n\n\n\n\n\n\n\n\nPirates game with cohortmates\n\n\n\n\n\n\n\nHiking at Peyto Lake in Banff, Summer 2025"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Erin Franke",
    "section": "",
    "text": "BA in Statistics, 2023\nMacalester College; Saint Paul, MN\nResume"
  },
  {
    "objectID": "projects/statGen/index.html",
    "href": "projects/statGen/index.html",
    "title": "Statistical Genetics Summary",
    "section": "",
    "text": "In Fall 2022 I took Statistical Genetics, which introduced me to the field and got me really interested in pursuing a career in health/genetics related data science. I had a great time learning how to use both R and PLINK to analyze genomic data - you can check out everything I learned here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Undergrad Projects",
    "section": "",
    "text": "I’m very grateful to have had many opportunities to work with real data during my time at Macalester. I personally find (a) applying methodology to data, and (b) making an effort to explain this work in writing or orally, to be the best way to process what I learn and understand what I don’t know. I worked on these projects during my undergrad and keep them on my website for the role they played in developing my passion for statistics.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGentrification and Crime in the Twin Cities: Insights and Challenges through a Statistical Lens\n\n\nAn independent honors project in statistics completed over my senior year at Macalester College.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Oxygen Help? A Causal Analysis\n\n\nAn indepedent project to apply causal inference techniques to understand oxygen use when ascending the Himalayan Mountains.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Genetics Summary\n\n\nA summary of what I learned in Stat 494 Statistical Genetics in Fall 2022.\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounders & Omitted Variable Bias in Linear Regression\n\n\nMy final group project in my Mathematical Statistics class in Spring 2022.\n\n\n\n\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Spatial Analysis of Elevated Blood Lead Levels in the Twin Cities Metropolitan Region\n\n\nResearch using spatial techniques including SAR models, the SF package, and Matern Random Effects Modeling to study where children in the Twin Cities are testing with…\n\n\n\n\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 and Residential Property Crime in Chicago, IL\n\n\nMy econometrics project seeking to understand the impact of the COVID-19 pandemic on residential property crime in Chicago’s 77 community areas.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTommy John surgery and its Relationship to MLB Pitcher Career Trajectory\n\n\nResearch using Bayesian techniques aimed to understand the impacts of Tommy John surgery.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing wOBA with Launch Angle and Exit Velocity\n\n\nSummer research project with the goal to understanding how individual MLB players should swing to maximize their weighted OBP.\n\n\n\n\n\n\nDec 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUS Universities and the SVD\n\n\nHow we identified a few very unique colleges using the SVD and Forbe’s data set on America’s Top Colleges of 2019.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA survival analysis of draft to debut date for MLB players\n\n\nA survival analysis project completed by Erin Franke and Corey Pieper in March 2021.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy first experience with data analysis for the community\n\n\nWhat I learned during my one month internship working with Common Pantry of Chicago, IL.\n\n\n\n\n\n\nJan 31, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/lead/index.html",
    "href": "projects/lead/index.html",
    "title": "A Spatial Analysis of Elevated Blood Lead Levels in the Twin Cities Metropolitan Region",
    "section": "",
    "text": "In Spring 2022 I got to take Correlated Data (STAT 452) and learned about how to work with time series, longitudinal, and spatially correlated data. I enjoyed combining my passion of mapping with modeling techniques that account for spatial correlation in order to learn more about the critical issue of childhood lead exposure in the Twin Cities, which can cause damage to a child’s brain and nervous system, slowed growth and development, and even comas or death with severe exposure. To learn more about what might cause elevated blood lead levels in children and where this issue is most apparent, check out our writeup on github."
  },
  {
    "objectID": "projects/econometrics/index.html",
    "href": "projects/econometrics/index.html",
    "title": "COVID-19 and Residential Property Crime in Chicago, IL",
    "section": "",
    "text": "In Fall 2021 I took Econometrics (ECON 381) at Macalester College. With the COVID-19 pandemic still looming and consistently and feeling like doing a project related to my hometown, I decided to learn more about COVID-19 and residential property crime on the neighborhood level. Over the course of my analysis (March 2019 - Feb 2021), I found one neighborhood to have a significant increase in residential property crime (Lincoln Square) and 15 neighborhoods to have a significant decrease in residential property crime. Overall, I found significant evidence that property crime decreased for Chicago as a whole with the onset of the pandemic. My statistical methods included fixed and random effects modeling as well as Granger causality. Please check out my paper to learn more!"
  },
  {
    "objectID": "projects/woba/index.html",
    "href": "projects/woba/index.html",
    "title": "Maximizing wOBA with Launch Angle and Exit Velocity",
    "section": "",
    "text": "In summer 2021 I was lucky enough to be part of a 15 person research cohort with a focus on sports analytics through Carnegie Mellon. Not only did I get to learn so many awesome statistical techniques, but I also got to apply them to my favorite topic (baseball) with people who were equally as passionate as me. My research was done in partnership with Sarah Sult (Washington University in St. Louis) and Brooke Coneeny (Swarthmore College) and advised by Adam Brodie of the Houston Astros.\nSome of my main takeaways from this project were how deeply you have to think as a statistician about underlying assumptions. So many times we thought were proud of a model we had developed and ready to apply it, and then realized how it doesn’t take into account something crucial to the game of baseball (like how if a batter changes their swing they will be thrown different pitches). This was also my first time conducting formal team research and I learned how lucky I was to be able to work with people with different strengths than my own. Sarah and Brooke are both computer science majors and were able to use their skills to debug some of our more complicated functions while I focused on data visualization and modeling.\nPlease check out our project repository and most recent presentation to learn more about the complex relationship between launch angle/attack angle, exit velocity, and wOBA!"
  },
  {
    "objectID": "projects/bayes/index.html",
    "href": "projects/bayes/index.html",
    "title": "Tommy John surgery and its Relationship to MLB Pitcher Career Trajectory",
    "section": "",
    "text": "In Fall 2021 I had the opportunity to take Bayesian Statistics (STAT 454) and complete a capstone project on a topic of choice. Hearing so much about the use of Bayesian techniques in sports, I decided to learn more about career trajectory for pitchers in the MLB, with a focus especially on Tommy John surgery. Please check out this website to learn more and feel free to reach out to me with questions and/or suggestions!"
  },
  {
    "objectID": "projects/survival/index.html",
    "href": "projects/survival/index.html",
    "title": "A survival analysis of draft to debut date for MLB players",
    "section": "",
    "text": "In this project we conducted a survival analysis of the time from draft date to debut date for major league baseball players. Our analysis specifically focuses on the variables of fielding position and whether a player was drafted in high school or college, and how these differences change the length of a player’s time between being drafted and making their MLB debut. For the complete analysis, please use the following link to visit our website."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Erin Franke",
    "section": "About Me",
    "text": "About Me\nIn addition to statistics & data science, I also enjoy all things running, hiking, volunteering at my local food pantry in Chicago, following Major League Baseball, and spending time with friends and family.\n\n\n\n\n\n\n\n\n\nMy siblings and I on a hike in Washington\n\n\n\n\n\n\n\nMy first marathon!\n\n\n\n\n\n\n\n\n\nMacalester commencement with cross country teammates\n\n\n\n\n\n\n\nManitou Incline: 2,744 steps of fun!"
  },
  {
    "objectID": "projects/svd/index.html",
    "href": "projects/svd/index.html",
    "title": "US Universities and the SVD",
    "section": "",
    "text": "This past spring, two of my classmates at Macalester College (Vivian Powell and Pippa Gallagher) and I decided to use the SVD to analyze United States colleges. The SVD - Singular Value Decomposition - is a matrix factorization that can be used for data reduction in machine learning, similarity analysis, image compression, and least squares regression among other uses. In this project, we used the SVD and a data set from Forbes 2019 about “America’s Top Colleges” to identify some of America’s most unique colleges and universities, at least in the sense of some of their basic demographics.\n\n\nGeneral Analysis\nUsing a data set with the 523 colleges Forbes selected, we took information on each school’s undergraduate population, student population, net price, average grant aid, total annual cost, alumni salary, acceptance rate, and mean SAT and ACT scores to build an original SVD plot. Giving each variable an equal weighing on a 0 to 1 scale, we got the following plot. \n\nA first step in the analysis of the SVD plot of the entire matrix was to look at the first two left singular vectors in order to understand which variables were most strongly associated with the horizontal axis and vertical axis in the plot of the first two right singular vectors. Below is a table that displays the left singular vectors U1 and U2, and which variable each entry represents:\n\nU1 and U2, which are the two most important left singular vectors of our matrix, essentially tell us which variables are the most influential on a college’s placement in the plot of V1 and V2. By analyzing U1, we can see which colleges are going to be closest to the direction of V1 (the negative horizontal axis in our case) and which will be furthest (the positive horizontal direction). We can see that the largest positive value in U1 is for Average Grant Aid, indicating that colleges with more grant aid will be closer to the direction of V1 and colleges with less grant aid will be farther from that direction. Total Annual Cost is similarly influential, with a slightly lower positive value. The largest negative value in U1 is for Acceptance Rate, which indicates that colleges with the lowest acceptance rates will be more negative on the x-axis, while colleges with high acceptance rates will be more positive on the x-axis. Overall, this tells us that colleges closer to the left side of the plot are likely to have a higher cost, higher grant aid, and a lower acceptance rate. We can expect this to include schools like the Ivies and “prestigious” liberal arts colleges with substantial endowments. On the other hand, colleges closer to the right side of the plot are likely to have a lower cost and less aid, as well as a higher acceptance rate. We could expect to see state schools with lower tuition in this category.\n\nLooking at U2, which tells us which variables are the most influential in a college’s placement on the vertical axis, we see that the largest positive value is for Acceptance Rate. This means that colleges with a higher acceptance rate will tend to have more positive y values, while colleges with lower acceptance rates will tend to have more negative y values. U2 also has two very large negative values, for Student Population and Undergraduate Population. This indicates that schools with a very large number of students will tend to be closer to the negative y-axis, while schools with few students will be closer to the positive y-axis. Overall, colleges near the top of the plot are likely to have a higher acceptance rate and/or smaller student populations, and colleges near the bottom of the plot are likely to be more selective and/or have larger student populations.\n\nClearly, this analysis isn’t perfect, because there will be schools with very low acceptance rates and tiny populations, and according to our analysis these schools won’t have a specific place on the plot. However, the information we gain from analyzing these first two columns of U is mostly to help us understand what the main factors are for grouping colleges in this way, and why two colleges might be plotted as opposites even if they are similar in some ways. Knowing that cost, financial aid, size, and acceptance rate are the most important variables for our SVD analysis will allow us to understand what makes some of these colleges unique.\n\n\n\nOutliers\n\nIn order to apply what we now know about U and V, we can look at the SVD plot and consider our outliers. Above is a plot of the SVD with labels on a few of the major outlier schools that we noticed in our analysis (BYU, University of Central Florida, and Liberty University). The major outlier we saw in almost all plots was BYU. Upon further research, we found that BYU has an incredibly low annual total cost of 18,370 dollars in comparison to almost all other universities. As a result of this, it also has a very low grant aid ($4843 annually). To really understand quite how low these numbers are in comparison to other universities, take a look at the density plots of total annual cost and grant aid below.\n\nThat’s pretty crazy! However, this information actually lines up with our SVD plots. If you remember, when we discussed singular vectors we said that colleges with more grant aid and a higher total annual cost would be much more on the negative side of the x axis and those with lower costs and financial aid would be on the right, positive side. BYU definitely follows this trend, being a huge right horizontal outlier on all of our plots.\nNow let’s see what might be going on at the University of Central Florida. Digging into the data, we found that the University of Central Florida has a large student population of 66,059 students. Looking at the student population density plot, there appear to be hardly any schools of this size.\n\nAdditionally, UCF’s average grant aid is $5757, which we know from the density plot of average grant aid is certainly on the lower end of the spectrum. The low grant aid explains this university’s positive V1 value on the SVD plot. The university’s large student population explains why UCF is such a noticeable vertical outlier.\nFinally, let’s investigate our third major outlier, Liberty University. Based on Liberty’s negative vertical location on the plot we might predict Liberty has a large student population or is a more selective university. From the positive V1 value we guess that Liberty tends to be a cheaper university and/or gives out less financial aid, though not quite to the degree as BYU or even UCF. Looking into the numbers, we find our size prediction confirmed - Liberty has the largest student population at 75,735 students! Furthermore, our cost prediction is correct as well as Liberty’s average grant aid is 10,400 dollars and their annual total cost is $38,364. Wow, our SVD plot seems pretty reliable!\n\n\nSingle Variable Analysis\n\nACT and SAT scores\nAfter completing the original plot, we decided it would be best to start looking at different variables and assessing how they appeared on the plot. For example, we took the ACT variable and divided it into three categories - low average ACT (&lt;24), medium average ACT (24-30), and high average ACT (30+). We then created plots with the universities colored by these groups - one plot without the ACT/SAT variables included in our main matrix (below left) and one with them included (below right). Green indicates universities with high ACT scores, blue indicates medium scores of 24-30, and red indicates low scores of less than 24.\n\nThere are subtle differences between the two plots, but you really have to look closely! This means that the ACT/SAT variable is highly correlated with another variable in the data set that was plotted originally. It makes most logical sense that this would be the acceptance rate variable, which we will analyze next. However, it is important to note that based on the somewhat distinct red, blue, and green groups in these plots, ACT score and potentially acceptance rate are pretty good indicators of what makes colleges similar and different.\n\n\nAcceptance Rate\nNext, we analyzed Acceptance Rate, as it seems to be one of the most influential variables based on our conclusions from U1 and U2. In order to do this, we defined low acceptance rates as &lt; 35%, medium acceptance rates as &gt; 35% and &lt; 70%, and high acceptance rates as &gt; 70%. We then repeated the same process used for SAT/ACT scores by creating two plots, both color-coded by acceptance rate: one where acceptance rate was included in the SVD, and one where it was not included. In the plots below, green represents the low acceptance rates, blue represents medium, and red represents high.\n\nThe first plot, with acceptance rate, looks very similar to the plots color coded for ACT/SAT score above (which makes sense given that colleges with similar acceptance rates will likely accept students with proportionally similar scores). There is a fairly clear distinction on the plot between colleges in different categories. Consistent with our conclusions from the left singular vectors earlier, we see that colleges with the lowest acceptance rates (green) tend to be towards the left and bottom sides of the plot, while colleges with high acceptance rates (red) tend to be along the top and right sides of the plot (and medium rates fall somewhere in between). However, when acceptance rate is removed from the matrix, the clarity of this pattern collapses significantly, indicating that the data of acceptance rate cannot be accurately represented as a linear combination of the other variables in the data set. So we can conclude that as we found earlier, acceptance rate is a fairly strong tool to group similar colleges by.\n\n\nPublic versus Private\nFinally, we chose to analyze the predictability of private vs public institutions. We did not include the private/public variable in our SVD as it held far too much weight due to being a binary variable with values of only 0 and 1. Instead, we will investigate whether other variables could accurately depict a college as private or public. In the figure below, blue represents public universities and red represents private universities. The separation between the two categories is definite with minimal mixing of red and blue near the top of the plot. This demonstrates that relying on other factors - including size, cost and acceptance rate - can fairly accurately encompass the information in the public vs private variable.\n\nThere are several visible outliers on this plot, including Liberty University and Brigham Young University-Idaho. While these universities are both private, they are plotted far on the positive side of the x-axis, far from any other private colleges. As we discussed earlier, this is due to their unique qualities. This graph demonstrates that private universities are more likely to have a higher tuition cost (falling on the negative x-axis), which is a common and generally accurate stereotype of private universities. However, Liberty and BYU have uncharacteristically low tuition and therefore could not be easily recognized as a private institution without the private/public factor included. If we were to include the private/public variable we can see a significantly clearer separation of the two categories, where the outliers are even easier to spot, located between the two groups of universities.\n\nIf this graph were not color coded a viewer might assume that Liberty University and Brigham Young University are public rather than private. This emphasizes the uniqueness of the two colleges and their unpredictability. While determining if a college is more likely to be private or public appears to be easy given other variables, there will always be outliers that don’t fall into the stereotypes of alike colleges.\n\n\n\nConclusion\nSVD analysis is not a perfect tool, but this paper has demonstrated that it carries great value in the ability to reduce a very large data set to something plottable in two dimensions that is visually digestible to the average reader. By using only the first two right singular vectors of the data, we can extract a much simpler representation of the vast majority of the information contained in the data set, and use it to understand colleges in a way that comparing schools by a single variable at a time simply would not achieve. Overall, the SVD is a good way to draw general and overarching conclusions rather than specific and pointed ones. The SVD takes advantage of having the ability to pull from a large data set that otherwise would take far too much storage, time, and machine power to analyze."
  },
  {
    "objectID": "projects/commonpantry/index.html",
    "href": "projects/commonpantry/index.html",
    "title": "My first experience with data analysis for the community",
    "section": "",
    "text": "Brief reflection\nDuring January of 2021 I was fortunate enough to have the opportunity to complete a one credit internship analyzing data with Common Pantry, my local food pantry in Chicago, IL. This internship was very rewarding as not only did I become much more comfortable in R and learn quicker and easier ways to analyze data, but in the process I was able to help out to my local community with information that could play a part of feeding more families in the coming months.\nOne of my main takeaways from this internship was the importance of good data. As a small nonprofit, a lot of Common Pantry’s data entry is done by hand which makes sense. Throughout my analysis, I encountered problems such as mismatched or missing units within the data, inconsistencies with data being recorded as characters or numeric entries, and simply missing information among other issues. This gave me a lot of practice cleaning the data, and learned new techniques to do so including using functions such as str_replace_all(), str_extract(), as.numeric(), and several others. I also got good practice with joining data sets horizontally and vertically, renaming columns, exporting and importing data from a variety of sources, and more.\n\n\nResults\nWhen COVID-19 hit in March 2020 and demand at Common Pantry spiked to about 2.11x their normal levels, Common Pantry could understandably no longer collect in depth information on clients such as their names, addresses, family size, and more. However, one piece of information they were able to collect was client zip code. I used this to create a basic density plot of the distribution of clients in Chicago. The Common Pantry service area, outlined in red, was removed once COVID-19 put many out of work.\n\nThe map above shows an important point, which is that a large portion of Common Pantry’s clients during COVID are coming from out of their service area and perhaps this service area needs to be enlarged for the long term (if Common Pantry can get adequate funding to do so). Four months after the conclusion of this project, one super exciting thing to note is that Common Pantry has recently announced the purchase of their own new and larger building only three blocks from the current pantry that will allow them to serve a greater amount of clients!\n\n\n\nCommon Pantry’s new location\n\n\nOutside of the zip code analysis, as a result of the limited 2020 data a large portion of my work was done with the 2019-2020 data on the donor appeal instead of information on the clients coming to the pantry. However, I was able to create the following plot which demonstrates the sheer number of clients visiting the pantry each month. Notice the spike in numbers as COVID begins and people are laid off of work in March and April.\n\nThankfully, while much of the community was in need of increased support for the majority of 2020, other members were able to step up and support Common Pantry to a greater level than before. In 2019, Common Pantry received 1,429 total donations, with 55.7% of them being financial. Total donations more than doubled in 2020 at 3,415, with 74.8% being financial. While the average donation amount fell in 2020, it was not by much, falling from 304.23 dollars in 2019 to $275.96 in 2020. For a visual summary of how donations changed throughout 2020 itself, see below.\n\n\n\nGrowth in number of donations throughout 2020\n\n\n\n\n\nA comparison of the value of monetary donations in 2019 versus 2020,by month\n\n\nAnother interesting thing I found when investigating the data was that despite monetary donations skyrocketing when COVID-19 hit, food donations stayed relatively constant. This makes sense - in the virtual environment, schools and businesses that may have been doing food drives in 2019 transitioned over to monetary fundraisers. This comparison of food versus monetary donations overtime can be seen below.\n\nThe remainder of the work I did with Common Pantry is more private, as I identified their top 75 donors of 2019-2020 to better inform them who to potentially thank and/or target for additional funds in the future. I also used R to filter for frequent donors and was able to develop a list of people giving a certain amount monthly. Additionally, I identified some of Common Pantry’s most successful appeals and campaigns for raising money (unsurprisingly, the COVID-19 appeal and the I am Your Neighbor fundraisers were incredibly successful).\n\n\nConclusion\nOverall, I am so happy I got to have this experience and give back in a new way to a group that I have been volunteering with since middle school. It is my hope that as I continue to learn more data science and R that I can help Common Pantry and other nonprofits in ways that I haven’t even imagined yet."
  },
  {
    "objectID": "projects/causal/index.html",
    "href": "projects/causal/index.html",
    "title": "Does Oxygen Help? A Causal Analysis",
    "section": "",
    "text": "In Spring 2023 I had the opportunity to take Causal Inference (STAT 451) and complete a capstone project on a topic of choice. As a hiker, I was interested to learn about the value of oxygen when ascending the Himalayan Mountains. Please check out this website to learn more about causal inference and this project."
  },
  {
    "objectID": "projects/honors/index.html",
    "href": "projects/honors/index.html",
    "title": "Gentrification and Crime in the Twin Cities: Insights and Challenges through a Statistical Lens",
    "section": "",
    "text": "Over the course of my senior year, I completed an independent research project in correspondence with my advisor, Victor Addona. Given that gentrification is highly contested phenomenon in today’s society and the neighborhood just north of Macalester has seen rapid redevelopment of the past several years, I chose to study the relationship between gentrification and crime in the Twin Cities. Using mapping, Poisson generalized linear models, and spatial modeling techniques, we did not find gentrification to yield meaningful decreases in violent crime or theft. To learn more, please download the completed paper here."
  },
  {
    "objectID": "projects/mathstat/index.html",
    "href": "projects/mathstat/index.html",
    "title": "Confounders & Omitted Variable Bias in Linear Regression",
    "section": "",
    "text": "In Spring 2022, I took Mathematical Statistics (STAT 455) where I deepened my understanding of theoretical statistics. Two of my classmates, Vivian Powell and Cheikh Fall, and I completed a final project on a topic of our choice which we taught to the remainder of our class. Given the importance of linear regression, we chose to dive deeper in a key assumption of ordinary least squares, exogeneity, and the consequences of omitted variable bias if this assumption does not hold. Please check out our our paper to learn more!"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Erin Franke",
    "section": "",
    "text": "Hi, my name is Erin (she/her). Thanks for visiting my page! \nI am a Statistics PhD student at Carnegie Mellon University. Previously, I graduated from Macalester College in May 2023 (Statistics major, Computer Science and Economics minors) and spent time at Mayo Clinic in a Statistical Programmer role. I am passionate about working with data to solve problems for the greater benefit of society, and have enjoyed applying statistical methodology to a variety of fields, including healthcare, sports, and urban studies/society. Please check out my Projects tab to learn more about my skills and see examples of my work."
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Tidy Tuesday!",
    "section": "",
    "text": "Tidy Tuesday is a weekly data project from the Data Science Learning Community. The data is shared every Monday here and on these social media platforms. If you are new to working with data, it is a great way to get practice! There are clear instructions for how to load each dataset into R, Python, and Julia. Tidy Tuesday also has a supportive online presence and it is encouraged to share what you create on social media—just follow these guidelines.\nI first started doing Tidy Tuesday when I took my first data science class in 2021. Challenging myself to find creative ways to visualize data and tell a story remains one of my favorite aspects of statistics. This page includes all my Tidy Tuesdays from most recent to oldest. Feel free to click on the image to see the corresponding code. While I often don’t have time to participate, I enjoy following #TidyTuesday on Bluesky to get inspiration. One of my favorite Tidy Tuesday creators is Nicola Rennie, check out her respository to see some pretty cool visualizations!"
  },
  {
    "objectID": "tidytuesday.html#section",
    "href": "tidytuesday.html#section",
    "title": "Tidy Tuesday!",
    "section": "2023",
    "text": "2023"
  },
  {
    "objectID": "tidytuesday.html#section-1",
    "href": "tidytuesday.html#section-1",
    "title": "Tidy Tuesday!",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "tidytuesday.html#section-2",
    "href": "tidytuesday.html#section-2",
    "title": "Tidy Tuesday!",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nRegression Recipes\n\n\n \n\n\n\n\nSole searching: How super shoes have changed marathoning\n\n\nAug 15, 2025\n\n\n\n\nMy Reflections from USCOTS 2025\n\n\nJul 21, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/rtricks/index.html",
    "href": "blog/rtricks/index.html",
    "title": "A small collection of various R tricks",
    "section": "",
    "text": "Below is a collection of R code & tricks I have found helpful to have on hand. These are the smaller, more obscure pieces of code I forget and then repeatedly search online for. I hope to continue to add to this list overtime!\n\nData Wrangling\n\n1. coalesce()\nSource: post from appliedepi@bsky.social\nReturns the first non-missing value from a set of columns based on the order that you specify. This is great for preventing a long series of case_when() calls.\n\n\n\n2. Text\n\na. Removing accents\nIn a situation where we are working with strings with accents (e.g. cities, names, etc), we likely want all strings to be formatted in the same. The following shows an example where some cities have accents and some do not. We can remove all accents using stringi::stri_trans_general(), as shown below.\n\nhead(cities) %&gt;% gt() # gt() just makes table look nice :) \n\n\n\n\n\n\n\nnames\n\n\n\n\nBogotá\n\n\nBogota\n\n\nQuébec\n\n\nQuebec\n\n\nÎle de la Cité\n\n\nIle de la Cite\n\n\n\n\n\n\n\n\ncities %&gt;%\n  mutate(names = stringi::stri_trans_general(names, \"Latin-ASCII\")) %&gt;% \n  count(names) %&gt;% gt()\n\n\n\n\n\n\n\nnames\nn\n\n\n\n\nBogota\n2\n\n\nIle de la Cite\n2\n\n\nQuebec\n2\n\n\n\n\n\n\n\n\n\nb. unnest_tokens()\nThe unnest_tokens() is helpful for analyzing word counts of text, specifically that might be split up across many lines. I first used this function to analyze historical markers data. The example below is taken from the unnest_tokens() help page.\n\nlibrary(janeaustenr) # for this example\nlibrary(tidytext) # for unnest_tokens()\n\nnovel &lt;- tibble(txt = prideprejudice)\nhead(novel, 15) %&gt;%\n  gt()\n\n\n\n\n\n\n\ntxt\n\n\n\n\nPRIDE AND PREJUDICE\n\n\n\n\n\nBy Jane Austen\n\n\n\n\n\n\n\n\n\n\n\nChapter 1\n\n\n\n\n\n\n\n\nIt is a truth universally acknowledged, that a single man in possession\n\n\nof a good fortune, must be in want of a wife.\n\n\n\n\n\nHowever little known the feelings or views of such a man may be on his\n\n\nfirst entering a neighbourhood, this truth is so well fixed in the minds\n\n\nof the surrounding families, that he is considered the rightful property\n\n\n\n\n\n\n\n\nnovel %&gt;%\n  tidytext::unnest_tokens(output = word, input = txt) %&gt;%\n  head(20) %&gt;%\n  gt()\n\n\n\n\n\n\n\nword\n\n\n\n\npride\n\n\nand\n\n\nprejudice\n\n\nby\n\n\njane\n\n\nausten\n\n\nchapter\n\n\n1\n\n\nit\n\n\nis\n\n\na\n\n\ntruth\n\n\nuniversally\n\n\nacknowledged\n\n\nthat\n\n\na\n\n\nsingle\n\n\nman\n\n\nin\n\n\npossession\n\n\n\n\n\n\n\nWe could then go on to count the number of times each word is used!\n\n\nc. Removing punctuation\nLike accents, we might also want to remove punctuation. We can do this with the [:punct:] regular expression!\n\ndata %&gt;% gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy :)\n\n\nexcited!!\n\n\n**excited**\n\n\nsad :(\n\n\nangry,\n\n\nupset?\n\n\n#mad\n\n\n\n\n\n\ndata %&gt;%\n  mutate(feelings = str_replace_all(feelings, \"[:punct:]\", \"\")) %&gt;% \n  gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy\n\n\nexcited\n\n\nexcited\n\n\nsad\n\n\nangry\n\n\nupset\n\n\nmad\n\n\n\n\n\n\n\n\n\nd. Separate list of words in a column!\nMany times I have ran into there being a list of words separated by a comma in a column of dataset. For example, asking participants to list the most memorable characteristics of a chocolate that they tasted (analyzed in this tidy tuesday).\nThis is some code to count how many time each word is used.\n\nhead(chocolate) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\n\n\n\n\n2454\nrich cocoa, fatty, bready\n\n\n2458\ncocoa, vegetal, savory\n\n\n2454\ncocoa, blackberry, full body\n\n\n2542\nchewy, off, rubbery\n\n\n2546\nfatty, earthy, moss, nutty,chalky\n\n\n2546\nmildly bitter, basic cocoa, fatty\n\n\n\n\n\n\nlist_of_adjectives &lt;- chocolate %&gt;%\n  mutate(id = row_number(), # gives each reviewer/chocolate combination a unique id\n         most_memorable_characteristics = strsplit(as.character(most_memorable_characteristics), \",\")) %&gt;% # split on the comma to create a list of characteristics for each individual\n  unnest(most_memorable_characteristics) # unlists the list, one in each column\n\nhead(list_of_adjectives) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\nid\n\n\n\n\n2454\nrich cocoa\n1\n\n\n2454\nfatty\n1\n\n\n2454\nbready\n1\n\n\n2458\ncocoa\n2\n\n\n2458\nvegetal\n2\n\n\n2458\nsavory\n2\n\n\n\n\n\n\n\n\n# can then count the number of each adjective, or separate into separate columns:\nlist_of_adjectives %&gt;%\n  group_by(id) %&gt;%\n  mutate(adjnum = paste0(\"adj\", row_number(id))) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = c(reviewer, id), names_from = adjnum, values_from = most_memorable_characteristics) %&gt;%\n  head() %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nid\nadj1\nadj2\nadj3\nadj4\nadj5\n\n\n\n\n2454\n1\nrich cocoa\nfatty\nbready\nNA\nNA\n\n\n2458\n2\ncocoa\nvegetal\nsavory\nNA\nNA\n\n\n2454\n3\ncocoa\nblackberry\nfull body\nNA\nNA\n\n\n2542\n4\nchewy\noff\nrubbery\nNA\nNA\n\n\n2546\n5\nfatty\nearthy\nmoss\nnutty\nchalky\n\n\n2546\n6\nmildly bitter\nbasic cocoa\nfatty\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n3. Make things faster\n\na. across()\nacross() lets us apply one function to many columns at once, such as below getting the average of each respective penguin measurement column.\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\npenguins %&gt;% \n  summarise(across(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g), ~mean(.x, na.rm=T))) %&gt;%\n  gt()\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n43.92193\n17.15117\n200.9152\n4201.754\n\n\n\n\n\n\n\n\n\nb. mutate if\nUse mutate_if() to apply a particular function under certain conditions.\n\npenguins %&gt;% \n  mutate_if(is.double, as.integer) %&gt;%  # IF is numeric, make AS integer\n  head(3) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39\n18\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39\n17\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40\n18\n195\n3250\nfemale\n2007\n\n\n\n\n\n\n\n\n\nc. data.table\nThe data.table package’s fread() function loads data into R more efficiently than many of the read_X() functions. When loading a large dataset, try replacing the read_X() with fread()!\nExample of loading in tidy tuesday data:\n\nlibrary(data.table)\n\nhistorical_markers &lt;- fread('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-04/historical_markers.csv')\n\n\n\n\n\nData Visualization\n\n1. Title placement\n\na. Move plot title left\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nb. Center title/subtitle\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n2. Color\n\na. Change color of part of title\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", \n       title = \"Penguin &lt;strong&gt;&lt;span style='color:navy'&gt; flipper&lt;/span&gt;&lt;/strong&gt;&lt;/b&gt; vs. &lt;strong&gt;&lt;span style='color:goldenrod3'&gt; bill &lt;/span&gt;&lt;/strong&gt;&lt;/b&gt;length\")+\n  theme_classic()+\n  theme(plot.title = ggtext::element_markdown()) # must have element_markdown() to make this work!!\n\n\n\n\n\n\n\n\n\n\nb. Color palette finder\nThis website is a super fun way to visualize a bunch of different color palettes! You can choose a type (qualitative, diverging, essential), a target color, and the palette length. You can also select for different types of color blindness.\n\n\nc. Change plot background color\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.background = element_rect(fill = \"lightblue3\"), \n        panel.background = element_rect(fill = \"lightblue3\"))\n\n\n\n\n\n\n\n\n\n\n\n3. Annotate plot\n\na. Text\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  annotate(geom=\"text\", x=220, y=3500, label = \"There is a \\nlinear relationship\",\n           color = \"navy\", size=3)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nb. Add an arrow\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  geom_curve(aes(x = 220, xend = 211, y = 3500, yend = 4000),arrow = arrow(length = unit(0.03, \"npc\")), curvature = 0.4, color = \"navy\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n4. Fonts\nI like the showtext package to change fonts. Look here for fonts.\n\nlibrary(showtext)\n\nfont_add_google(\"Shadows Into Light\") # choose fonts to add\nfont_add_google(\"Imprima\")\nfont_add_google(\"Gudea\")\nshowtext_auto() # turns on the automatic use of showtext\n\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(family = \"Imprima\"))\n\n\n\n\n\n\n\n\n\n\n5. Images\n\na. general\n\nlibrary(png)\npenguin_pic &lt;- readPNG(\"images/penguin.png\", native=TRUE)\n\n\nplot &lt;- penguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()\n\nplot +                  \n  patchwork::inset_element(p = penguin_pic,\n                left = 0.87,\n                bottom = 0.5,\n                right = 1,\n                top = 0.7)\n\n\n\n\n\n\n\n\n\n\nb. aes\nYou might not want to do this for a ton of points, but you can replace a typical geom_point() with geom_image(), or think of other fun ways to use this (e.g. putting images at the end of bar chart).\n\npenguins %&gt;%\n  head(10) %&gt;% # select 10 points\n  mutate(img = \"images/penguin.png\") %&gt;%\n  ggplot()+\n  ggimage::geom_image(aes(x = flipper_length_mm, y = body_mass_g, image=img), size=0.06) +\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\n\nOther R tricks\n\n1. Rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑\n\n\n2. Custom Default Theme\nDuring college (and still now) I liked to modify the theme() of a plot a lot. This ended up with a lot of repeated code, so my advisor, Brianna Heggeseth, taught me how to setup a custom default theme. Thanks, Brianna! :) Here are the steps.\n\nOpen you .Rprofile file by pasting file.edit(file.path(\"~\", \".Rprofile\")) in the console\nModify this file to load your theme when a specific package is loaded (for example, ggplot2). Then, put your desired theme in theme_set(). This could be something as simple as ggplot2::theme_classic(). Below, I put a very simple example of a custom theme.\n\n\nsetHook(packageEvent(\"ggplot2\", \"onLoad\"), \n        function(...) ggplot2::theme_set(ggplot2::theme_classic()+\n                                           ggplot2::theme(plot.title.position = \"plot\",\n                                                 plot.title = ggplot2::element_text(family = \"mono\"))))\n\nA few notes:\n- You must call each function you use with the appropriate library in order for this to work.\n- When you are done, save the .Rprofile and quit RStudio in order for the changes to be saved.\n\nHere is a source I used to help refresh my memory on doing this.\n\n\n3. Highlighting code\nWhile making this blog post, I wanted to be able to highlight the lines of code that corresponded to the topic I was discussing. To do this, I installed the line-highlight extension for Quarto and followed the directions.\n\n\n\nThanks!\nThanks for reading my first blog post! If you have any R tricks you know of or feedback on this post, please email me at efranke@andrew.cmu.edu; I’d love to hear :)"
  },
  {
    "objectID": "blog/rtricks/index.html#data-wrangling",
    "href": "blog/rtricks/index.html#data-wrangling",
    "title": "My favorite R Tricks",
    "section": "Data Wrangling",
    "text": "Data Wrangling"
  },
  {
    "objectID": "blog/rtricks/index.html#data-visualization",
    "href": "blog/rtricks/index.html#data-visualization",
    "title": "My favorite R Tricks",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nOther R tricks\n\nSet up rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑"
  },
  {
    "objectID": "blog/rtricks/index.html#other-r-tricks",
    "href": "blog/rtricks/index.html#other-r-tricks",
    "title": "My favorite R Tricks",
    "section": "Other R tricks",
    "text": "Other R tricks\n\nSet up rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Having the opportunity to collaborate with experts in a wide variety of fields is my favorite part of being a statistician. As I develop as a researcher, my goal is to contribute work that helps understand and solve complex interdisciplinary problems.\n\nCurrent Projects\n\nIn the Spring 2026 semester, I am looking forward to starting research advised by Will Townes as part of the CMU Delphi group.\nStress and Cold Susceptibility: Comparing Z-Score and Latent Measurement Models\n\n\nI recently completed this year-long project in collaboration with Weijing Tang (CMU Stats & DS) and Phoebe Lam (CMU Psychology). We integrated existing studies to understand the influence of perceived stress and negative emotion on disease vulnerability using moderated nonlinear factor analysis (Bauer and Hussong, 2009).\nI am currently working on a spinoff paper, A Comparison of Moderated Nonlinear Factor Analysis and Standardization Approaches for Psychological Research Using Integrated Data, that extends our prior work. This paper will take further steps to understand the differences between moderated nonlinear factor analysis and z-scoring approaches through simulations and derivations.\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "My undergraduate experience at Macalester College sparked my interest in teaching and helped motivate my decision to pursue a PhD. I’ve continued to find my experiences teaching and mentoring at Carnegie Mellon especially rewarding, and plan on pursuing a career in academia following my PhD."
  },
  {
    "objectID": "tidytuesday.html#what-is-tidy-tuesday",
    "href": "tidytuesday.html#what-is-tidy-tuesday",
    "title": "Tidy Tuesday!",
    "section": "",
    "text": "Tidy Tuesday is a weekly data project from the Data Science Learning Community. The data is shared every Monday here and on these social media platforms. If you are new to working with data, it is a great way to get practice! There are clear instructions for how to load each dataset to R, Python, and Julia. Tidy Tuesday also has a supportive online presence and it is encouraged to share what you create on social media—just follow these guidelines.\nI first started doing Tidy Tuesday when I took my first data science class in 2021. Challenging myself to find creative ways to visualize data and tell a story remains one of my favorite aspects of statistics. This page includes all my Tidy Tuesdays from most recent to oldest. Feel free to click on the image to see the corresponding code. I have also compiled a blog post with some obscure pieces of data wrangling and visualization code that have come in handy for many of my Tidy Tuesdays. While I often don’t have time to participate, I enjoy following #TidyTuesday on Bluesky to get inspiration. One of my favorite Tidy Tuesday creators is Nicola Rennie, check out her respository to see some pretty cool visualizations!"
  },
  {
    "objectID": "rtricks/index.html",
    "href": "rtricks/index.html",
    "title": "A small collection of various R tricks",
    "section": "",
    "text": "Below is a collection of R code & tricks I have found helpful to have on hand. These are the smaller, more obscure pieces of code I forget and then repeatedly search online for. I hope to continue to add to this list overtime!\n\nData Wrangling\n\n1. coalesce()\nSource: post from appliedepi@bsky.social\nReturns the first non-missing value from a set of columns based on the order that you specify. This is great for preventing a long series of case_when() calls.\n\n\n\n2. Text\n\na. Removing accents\nIn a situation where we are working with strings with accents (e.g. cities, names, etc), we likely want all strings to be formatted in the same. The following shows an example where some cities have accents and some do not. We can remove all accents using stringi::stri_trans_general(), as shown below.\n\nhead(cities) %&gt;% gt() # gt() just makes table look nice :) \n\n\n\n\n\n\n\nnames\n\n\n\n\nBogotá\n\n\nBogota\n\n\nQuébec\n\n\nQuebec\n\n\nÎle de la Cité\n\n\nIle de la Cite\n\n\n\n\n\n\n\n\ncities %&gt;%\n  mutate(names = stringi::stri_trans_general(names, \"Latin-ASCII\")) %&gt;% \n  count(names) %&gt;% gt()\n\n\n\n\n\n\n\nnames\nn\n\n\n\n\nBogota\n2\n\n\nIle de la Cite\n2\n\n\nQuebec\n2\n\n\n\n\n\n\n\n\n\nb. unnest_tokens()\nThe unnest_tokens() is helpful for analyzing word counts of text, specifically that might be split up across many lines. I first used this function to analyze historical markers data. The example below is taken from the unnest_tokens() help page.\n\nlibrary(janeaustenr) # for this example\nlibrary(tidytext) # for unnest_tokens()\n\nnovel &lt;- tibble(txt = prideprejudice)\nhead(novel, 15) %&gt;%\n  gt()\n\n\n\n\n\n\n\ntxt\n\n\n\n\nPRIDE AND PREJUDICE\n\n\n\n\n\nBy Jane Austen\n\n\n\n\n\n\n\n\n\n\n\nChapter 1\n\n\n\n\n\n\n\n\nIt is a truth universally acknowledged, that a single man in possession\n\n\nof a good fortune, must be in want of a wife.\n\n\n\n\n\nHowever little known the feelings or views of such a man may be on his\n\n\nfirst entering a neighbourhood, this truth is so well fixed in the minds\n\n\nof the surrounding families, that he is considered the rightful property\n\n\n\n\n\n\n\n\nnovel %&gt;%\n  tidytext::unnest_tokens(output = word, input = txt) %&gt;%\n  head(20) %&gt;%\n  gt()\n\n\n\n\n\n\n\nword\n\n\n\n\npride\n\n\nand\n\n\nprejudice\n\n\nby\n\n\njane\n\n\nausten\n\n\nchapter\n\n\n1\n\n\nit\n\n\nis\n\n\na\n\n\ntruth\n\n\nuniversally\n\n\nacknowledged\n\n\nthat\n\n\na\n\n\nsingle\n\n\nman\n\n\nin\n\n\npossession\n\n\n\n\n\n\n\nWe could then go on to count the number of times each word is used!\n\n\nc. Removing punctuation\nLike accents, we might also want to remove punctuation. We can do this with the [:punct:] regular expression!\n\ndata %&gt;% gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy :)\n\n\nexcited!!\n\n\n**excited**\n\n\nsad :(\n\n\nangry,\n\n\nupset?\n\n\n#mad\n\n\n\n\n\n\ndata %&gt;%\n  mutate(feelings = str_replace_all(feelings, \"[:punct:]\", \"\")) %&gt;% \n  gt()\n\n\n\n\n\n\n\nfeelings\n\n\n\n\nhappy\n\n\nexcited\n\n\nexcited\n\n\nsad\n\n\nangry\n\n\nupset\n\n\nmad\n\n\n\n\n\n\n\n\n\nd. Separate list of words in a column!\nMany times I have ran into there being a list of words separated by a comma in a column of dataset. For example, asking participants to list the most memorable characteristics of a chocolate that they tasted (analyzed in this tidy tuesday).\nThis is some code to count how many time each word is used.\n\nhead(chocolate) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\n\n\n\n\n2454\nrich cocoa, fatty, bready\n\n\n2458\ncocoa, vegetal, savory\n\n\n2454\ncocoa, blackberry, full body\n\n\n2542\nchewy, off, rubbery\n\n\n2546\nfatty, earthy, moss, nutty,chalky\n\n\n2546\nmildly bitter, basic cocoa, fatty\n\n\n\n\n\n\nlist_of_adjectives &lt;- chocolate %&gt;%\n  mutate(id = row_number(), # gives each reviewer/chocolate combination a unique id\n         most_memorable_characteristics = strsplit(as.character(most_memorable_characteristics), \",\")) %&gt;% # split on the comma to create a list of characteristics for each individual\n  unnest(most_memorable_characteristics) # unlists the list, one in each column\n\nhead(list_of_adjectives) %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nmost_memorable_characteristics\nid\n\n\n\n\n2454\nrich cocoa\n1\n\n\n2454\nfatty\n1\n\n\n2454\nbready\n1\n\n\n2458\ncocoa\n2\n\n\n2458\nvegetal\n2\n\n\n2458\nsavory\n2\n\n\n\n\n\n\n\n\n# can then count the number of each adjective, or separate into separate columns:\nlist_of_adjectives %&gt;%\n  group_by(id) %&gt;%\n  mutate(adjnum = paste0(\"adj\", row_number(id))) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = c(reviewer, id), names_from = adjnum, values_from = most_memorable_characteristics) %&gt;%\n  head() %&gt;% gt()\n\n\n\n\n\n\n\nreviewer\nid\nadj1\nadj2\nadj3\nadj4\nadj5\n\n\n\n\n2454\n1\nrich cocoa\nfatty\nbready\nNA\nNA\n\n\n2458\n2\ncocoa\nvegetal\nsavory\nNA\nNA\n\n\n2454\n3\ncocoa\nblackberry\nfull body\nNA\nNA\n\n\n2542\n4\nchewy\noff\nrubbery\nNA\nNA\n\n\n2546\n5\nfatty\nearthy\nmoss\nnutty\nchalky\n\n\n2546\n6\nmildly bitter\nbasic cocoa\nfatty\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n3. Make things faster\n\na. across()\nacross() lets us apply one function to many columns at once, such as below getting the average of each respective penguin measurement column.\n\nlibrary(palmerpenguins)\n\npenguins %&gt;% \n  summarise(across(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g), ~mean(.x, na.rm=T))) %&gt;%\n  gt()\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n43.92193\n17.15117\n200.9152\n4201.754\n\n\n\n\n\n\n\n\n\nb. mutate if\nUse mutate_if() to apply a particular function under certain conditions.\n\npenguins %&gt;% \n  mutate_if(is.double, as.integer) %&gt;%  # IF is numeric, make AS integer\n  head(3) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39\n18\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39\n17\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40\n18\n195\n3250\nfemale\n2007\n\n\n\n\n\n\n\n\n\nc. data.table\nThe data.table package’s fread() function loads data into R more efficiently than many of the read_X() functions. When loading a large dataset, try replacing the read_X() with fread()!\nExample of loading in tidy tuesday data:\n\nlibrary(data.table)\n\nhistorical_markers &lt;- fread('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-04/historical_markers.csv')\n\n\n\n\n\nData Visualization\n\n1. Title placement\n\na. Move plot title left\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nb. Center title/subtitle\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n2. Color\n\na. Change color of part of title\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", \n       title = \"Penguin &lt;strong&gt;&lt;span style='color:navy'&gt; flipper&lt;/span&gt;&lt;/strong&gt;&lt;/b&gt; vs. &lt;strong&gt;&lt;span style='color:goldenrod3'&gt; bill &lt;/span&gt;&lt;/strong&gt;&lt;/b&gt;length\")+\n  theme_classic()+\n  theme(plot.title = ggtext::element_markdown()) # must have element_markdown() to make this work!!\n\n\n\n\n\n\n\n\n\n\nb. Color palette finder\nThis website is a super fun way to visualize a bunch of different color palettes! You can choose a type (qualitative, diverging, essential), a target color, and the palette length. You can also select for different types of color blindness.\n\n\nc. Change plot background color\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.background = element_rect(fill = \"lightblue3\"), \n        panel.background = element_rect(fill = \"lightblue3\"))\n\n\n\n\n\n\n\n\n\n\n\n3. Annotate plot\n\na. Text\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  annotate(geom=\"text\", x=220, y=3500, label = \"There is a \\nlinear relationship\",\n           color = \"navy\", size=3)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nb. Add an arrow\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  geom_curve(aes(x = 220, xend = 211, y = 3500, yend = 4000),arrow = arrow(length = unit(0.03, \"npc\")), curvature = 0.4, color = \"navy\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n4. Fonts\nI like the showtext package to change fonts. Look here for fonts.\n\nlibrary(showtext)\n\nfont_add_google(\"Shadows Into Light\") # choose fonts to add\nfont_add_google(\"Imprima\")\nfont_add_google(\"Gudea\")\nshowtext_auto() # turns on the automatic use of showtext\n\n\npenguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()+\n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(family = \"Imprima\"))\n\n\n\n\n\n\n\n\n\n\n5. Images\n\na. general\n\nlibrary(png)\npenguin_pic &lt;- readPNG(\"images/penguin.png\", native=TRUE)\n\n\nplot &lt;- penguins %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g))+\n  geom_point()+\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic()\n\nplot +                  \n  patchwork::inset_element(p = penguin_pic,\n                left = 0.87,\n                bottom = 0.5,\n                right = 1,\n                top = 0.7)\n\n\n\n\n\n\n\n\n\n\nb. aes\nYou might not want to do this for a ton of points, but you can replace a typical geom_point() with geom_image(), or think of other fun ways to use this (e.g. putting images at the end of bar chart).\n\npenguins %&gt;%\n  head(10) %&gt;% # select 10 points\n  mutate(img = \"images/penguin.png\") %&gt;%\n  ggplot()+\n  ggimage::geom_image(aes(x = flipper_length_mm, y = body_mass_g, image=img), size=0.06) +\n  labs(x=\"Flipper length (mm)\", y=\"Body mass (g)\", title = \"Penguin flipper vs. bill length\")+\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\n\nOther R tricks\n\n1. Rainbow parentheses\nThis is super useful for making sure your parentheses are in the right place with a long sequence code!\nTo turn on rainbow parentheses, go to Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Use Rainbow Parentheses ☑\n\n\n2. Custom Default Theme\nOne of the aspects of my plots that I modify the most is the theme(). During college, I often found myself copying and pasting my custom theme to the end of every plot. To make less work for myself and my code more readable, I learned to permanently load my custom theme when I open R. Special thanks to my undergrad advisor, Brianna Heggeseth, for helping me figure this out :) Here are the steps.\n\nOpen you .Rprofile file by pasting file.edit(file.path(\"~\", \".Rprofile\")) in the console\nModify this file to load your theme when a specific package is loaded (for example, ggplot2). Then, put your desired theme in theme_set(). This could be something as simple as ggplot2::theme_classic(). Below, I put a very simple example of a custom theme.\n\n\nsetHook(packageEvent(\"ggplot2\", \"onLoad\"), \n        function(...) ggplot2::theme_set(ggplot2::theme_classic()+\n                                           ggplot2::theme(plot.title.position = \"plot\",\n                                                 plot.title = ggplot2::element_text(family = \"mono\"))))\n\nA few notes:\n\n\nYou must call each function you use with the appropriate library in order for this to work.\n\nWhen you are done, save the .Rprofile and quit RStudio in order for the changes to be saved.\n\n\nHere is a source I used to help refresh my memory on doing this.\n\n\n3. Highlighting code\nWhile making this blog post, I wanted to be able to highlight the lines of code that corresponded to the topic I was discussing. To do this, I installed the line-highlight extension for Quarto and followed the directions."
  },
  {
    "objectID": "teaching.html#teaching-experience",
    "href": "teaching.html#teaching-experience",
    "title": "Teaching",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nTeaching Assistant\n\n\n36-707: Regression Analysis & 36-617: Applied Linear Models (Fall 2025)\n\nCarnegie Mellon Sports Analytics Camp (Summer 2025)\n\n\nCo-Guest Lecturer for four sessions\n\n36-236: Probability and Statistical Inference II (Spring 2025)\n\n36-401: Modern Regression (Fall 2024)"
  },
  {
    "objectID": "teaching/uscots2025.html",
    "href": "teaching/uscots2025.html",
    "title": "My Reflections from USCOTS 2025",
    "section": "",
    "text": "I’m so grateful to have had the opportunity to attend the 2025 United States Conference on Teaching Statistics (USCOTS) in Ames, Iowa! Before jumping into my general reflections, I want to thank Kelly Bodwin and the rest of team behind the POSE: Phase II: Expanding the data.table ecosystem for efficient big data manipulation in R NSF grant for funding my trip, as well as CAUSE for generously waiving the registration fee. I am also want to thank the many people that organized this conference, including Allan Rossman, Kelly McConville, Laura Ziegler, and Matt Beckman. The conference was incredibly well run and I cannot imagine how much work went it.\nThis was my first conference, and it was such a privilege to listen to talks and have discussions with many of the leaders in statistics and data science education. This was very inspiring, and also a bit overwhelming—these individuals have done so much and have so many smart and creative ideas. Both at the conference and typing up these reflections, it was hard to not feel some level of imposter syndrome—unlike the majority of attendees, I have never even taught a class! All of this is just to say, I have lots of to learn, and I feel so lucky to have gotten a chance to jumpstart that learning process by connecting with so many smart people (and am very grateful for how kind and welcoming they all were)!"
  },
  {
    "objectID": "teaching/uscots2025.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "href": "teaching/uscots2025.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "title": "My Reflections from USCOTS 2025",
    "section": "Life as a liberal arts statistician: joys and challenges",
    "text": "Life as a liberal arts statistician: joys and challenges\nIdeas for success in the classroom\n\n\nSurvey the class periodically and discuss the results (both things to change and why particular aspects will remain the same) with the students\n\nLearn from others, both in your department and others\n\n\nSit in on classes\n\n\nTake notes after class on what worked and what didn’t for the future\n\n\nBe active professionally\n\nPre-tenure, make strategic choices. Play the odds to get research that students can help with.\n\nPost-tenure, focus on what nourishes you (e.g. publishing an open source textbook)\n\nCollaborate with non-statistical collegues\n\nAdapt research question (and timeline for undergrads)\n\nServe others\n\nJump into service early, both internally and externally\n\nBe happy and proactive with “yeses”, then you can say “no” to things not as interesting to you\n\nGrowing a program\n\nHelpful to have a customizable concentration/minor\n\nHaving a center for interdisciplinary research is a good way to create community and can lead to research collaborations\n\nAdd modern courses to curriculum (e.g. add a data science course and remove an advanced modeling requirement)\n\nFind projects everywhere! Examples: a bagel shop running out of your favorite flavor, library checkout data\n\nMany aspects are constantly changing\n\nClassroom dynamics and design\n\nTechnology (classroom tech, programming languages)\n\nRising popularity of statistics and data science in recent years\n\nRelationship with math and computer science\n\nData ethics\n\n\nChallenges\n\nSemesters are hectic\n\nSalary\n\nGrading\n\n5% of students\n\nSlow changes in academia\n\nLifelong learning can be exhausting\n\n\nJoys\n\nIt usually doesn’t feel like work\n\nAwesome colleagues\n\nStudents want to make world better\n\nCreating a good learning environment\n\nConstant renewal\n\nCollaborating with experts in different fields\n\nTaking pride in your institution\n\nLifelong learning can be exhilarating\n\n\nShort reflection: The group I was in talked a lot about growing a program and the interaction between math, statistics, and computer science. Some people in the group were in a statistics/mathematics department separate from computer science, and mentioned this being a point of tension. Right now, when data science is so popular, it is ideal to have these departments collaborating and working together on what courses should count for credits to particular majors or minors (as well as just to maximize student learning). Having a combined department at Macalester, this was not something I’d thought about before, but it must make conversations about forming a data science major or minor, for instance, easier. Some other issues mentioned were struggle to find a form of service people find exciting, and issues with institutional financials. I hadn’t previously thought about either of these topics when thinking about a liberal arts career, so it was nice to listen to what others had to say."
  },
  {
    "objectID": "teaching/uscots2025.html#authentic-assessment-with-oral-exams",
    "href": "teaching/uscots2025.html#authentic-assessment-with-oral-exams",
    "title": "My Reflections from USCOTS 2025",
    "section": "Authentic assessment with oral exams",
    "text": "Authentic assessment with oral exams\nWith students now being able to offload parts of “writing to learn” assignments (e.g. data analysis reports) to LLMs, many people at this conference were interested in discussing alternative forms of assessment. One option that has potential in liberal arts classrooms is oral exams. While the potential benefit of oral exams is high, the primary concern seems to be time, both for the professor and students.\nPros\n\nPersonal connection between professor and student.\nProfessors having used oral exams noted they could tell level of the student’s understanding with just a couple of minutes.\n\nGoing off this, the students did not refute their score. The student could tell if they weren’t matching the professor’s expectation for level of understanding.\n\nOpportunity to understand the student’s misunderstandings and redirect them.\n\nFor example, if a student said, “The probability of the null being false is 95%”, you could ask a follow up question like, “Wait, can you explain a little more why that is the case?”.\n\nNo option for student to offload thinking to LLMs.\nGood practice for job interviews.\nOpportunity to meet the student where they are at. For students less comfortable with the material, you can redirect and ask them different questions than a student clearly grasping the concepts.\n\nCons\n\nTime. With 2-3 sections of a class with 20 students, you are trying to schedule 40-60 fifteen minute blocks. That is a lot of time out of the professor’s week, and it is also hard to find time outside of class for students with busy schedules.\n\nCan we do paired oral exams to cut down on that?\n\nHow are students then graded as a pair? What if one partner dominates the conversation? Side note: in an ungrading scenario, this would not be as much of an issue and something the students could reflect on themselves after the fact.\nIdea: Have the pair have a conversation between themselves based on a set of prompts given to them.\n\n\nStudent anxiety. The idea of an oral exam is daunting for many students.\n\nPaired exams could potentially help reduce this anxiety.\nCertain students may understand the concepts well but not be able to verbally communicate on the spot.\n\nAlternatively, you could say that some students have test anxiety, but this is just a common form of assessment so we typically ignore this issue.\n\n\nFairness and equity: can we ensure students are evaluated the same?\n\nOther notes\n\nAs opposed to an exam, how much material can you assess? If, for example, an exam were to cover machine learning methods such as LASSO, clustering, splines, random forests, and GAMs, are you assessing all of these methods in a 15 minute oral exam but sacrificing depth, or vice versa?\nOne professor found no correlation between student oral exam scores and test scores in a semester where they used both as modes of assessment.\n\nI found this really interesting, and don’t think is necessarily a pro or a con, but maybe a reason to utilize multiple forms of assessment within a semester."
  },
  {
    "objectID": "teaching/uscots2025.html#project-design",
    "href": "teaching/uscots2025.html#project-design",
    "title": "My Reflections from USCOTS 2025",
    "section": "Project Design",
    "text": "Project Design\nIn this discussion, we covered some ideas make sure students are getting the most they can out of class projects.\nProject setup\n\nProviding a sample report: should we or should we not?\n\nUseful so that students understand expectations, but they may follow a sample report too closely and not build their own writing style.\nIdea: provide a report with several flaws, and take class time to critique the report and discuss potential changes.\n\nPeer reviews: tell students it is not a time to be “MN nice”. Give honest feedback to help your classmate get a better grade.\nFor group projects, have formal meetings with each project group to discuss next modeling steps, what is going well or what they are struggling with, etc.\n\nPersonal note: I think effectively communicating what I have done and have questions on in an organized matter in a one-on-one meeting was one of the things I struggled with transitioning out of undergrad (both working at Mayo Clinic and starting research in my PhD). Maybe this could help prepare students for that.\n\n\nProject Ideas\n\nClass time activity: story boarding with data. Have students introduce an idea, most important aspects from EDA, conflict/main characters.\n\nStudents don’t necessarily have to even have data, just have them sketch and type of plot that they might expect.\nHave them decide on an ideal storyboard: this is a good way to get them to think about the project workflow and communicating big ideas (perhaps to a nonstatistical audience too).\n\nPodcast (~10 minutes) where you encourage students to write a script (so they think through what they will say) and the verbally communicate their findings with a partner.\n\nHave students send pdf of visualizations that they talk about.\nOne professor found a student that didn’t typically participate in class was great at this."
  },
  {
    "objectID": "teaching/uscots2025.html#positron",
    "href": "teaching/uscots2025.html#positron",
    "title": "My Reflections from USCOTS 2025",
    "section": "Positron",
    "text": "Positron"
  },
  {
    "objectID": "teaching/uscots2025.html#hadley-wickham-keynote",
    "href": "teaching/uscots2025.html#hadley-wickham-keynote",
    "title": "My Reflections from USCOTS 2025",
    "section": "Hadley Wickham Keynote",
    "text": "Hadley Wickham Keynote"
  },
  {
    "objectID": "teaching/uscots2025.html#mine-cr-workshop",
    "href": "teaching/uscots2025.html#mine-cr-workshop",
    "title": "My Reflections from USCOTS 2025",
    "section": "Mine CR Workshop",
    "text": "Mine CR Workshop"
  },
  {
    "objectID": "teaching/uscots2025.html#teaching-poisson-regression-jo-and-nick",
    "href": "teaching/uscots2025.html#teaching-poisson-regression-jo-and-nick",
    "title": "My Reflections from USCOTS 2025",
    "section": "Teaching Poisson Regression, Jo and Nick",
    "text": "Teaching Poisson Regression, Jo and Nick"
  },
  {
    "objectID": "teaching/uscots2025.html#doing-data-science-in-positron",
    "href": "teaching/uscots2025.html#doing-data-science-in-positron",
    "title": "My Reflections from USCOTS 2025",
    "section": "Doing data science in Positron",
    "text": "Doing data science in Positron\nThis workshop by Mine Cetinkaya-Rundel and Hadley Wickham a great first introduction to Positron, which I didn’t know anything about prior to this conference.\nWhat is it?\n\nA next-generation data science IDE that feels like a fusion of RStudio and VS Code.\n\nHappy note: RStudio is not going away!\n\n\nBenefits of Positron\n\nMain benefit: easily combine R and Python in one document\n\nPersonally not currently a Python user, but I know a lot of people that are and I think this could be great for classes or research where groups are using both!\n\nAbility to effortlessly switch between versions of R (e.g. R 4.3.3, 4.5.0)\nCommand palette (Ctrl/Cmd + Shift + P) makes searching for and executing commands easy\nEasy to customize layout and split screen between files.\nMultiple concurrent interpreter sessions, which can be a mix of different R versions, mix of R and Python sessions, or multiple instances of a single R version\n\nAllows you to run different code while something is taking a while to run! This is awesome to me.\n\nCan sort your environment (e.g. by most recent)\nAir: an extension that automatically formats your code nicely\nYou can Preview (Render) your document and it will appear in the viewer pane instead of opening in your internet browser—easy to make side by side changes\nArea for previewing \\(\\LaTeX\\) equations\nEasy to save and share plots\n\nOther differences\n\nNo inline plots\n\nOutputs to “plots” pane, which I think I agree with most people is nice because the inline plots can make your document a bit laggy.\n\nRun button is different/a bit smaller? Personally like the RStudio version better this is not a big deal.\nPositron automatically updates (good, I think?)\n\nThings for me to learn more about\n\nrig\nAir\nSnippets\nPositron Assistant\n\nOverall reflections\nVery cool, also a bit overwhelming—there are just so many features. Hadley and Mine mentioned when you see someone’s RStudio, you can easily become acquainted with their setup, but Positron is completely customizable (which is mostly a pro, but can perhaps be a con in a teaching setting?). At the same time, so many of these features seem useful and Mine and Hadley did an awesome job motivating and explaining the tool. I really like the ability to continue to run smaller code jobs while another piece is running in the background. The mix of versions of R is also great. I am not sure if/when I will be fully doing my coding in Positron over RStudio, but I am excited to get more experience with it."
  },
  {
    "objectID": "teaching/uscots2025.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "href": "teaching/uscots2025.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "title": "My Reflections from USCOTS 2025",
    "section": "A no bullshit guide to programming with LLMs in R",
    "text": "A no bullshit guide to programming with LLMs in R\nThis talk by Hadley Wickham introduced the ellmer R package, which he developed to interact with LLMs in R. As someone that enjoys the challenges and satisfaction of coding and figuring out little tricks, I don’t love the idea of offloading parts of that process to LLMs. But I know this is a part of the current reality that we live in, and can make some dreadful tasks much faster, so I wanted to listen with an open mind.\nOne of Hadley’s first motivating examples had us discuss at our table how we’d extract name and age from text data, such as the following:\n\nprompts &lt;- c(\"I go by Alex. 42 years on planet Earth and counting.\", \n             \"Hey, I'm turning 27 and my name is Jamal\", \n             \"I'm Lei Wei and nineteen years young.\",\n             \"My name is Brett and I'm turning the big 5-0 this year.\")\n\nClearly, this is not a simple regex expression due the mix of age being typed and numeric, names being 1-2 words, and the fact that there is no global pattern in sentence structure. I learned that this is something an LLM could do with essentially the following code:\n\nlibrary(ellmer)\nchat &lt;- chat_anthropic()\nchat$chat(\"Extract the name and age from each sentence I give you\")\n\nchat$chat(prompts[[1]])\n\nAnd this essentially returned the name and age for each prompt. I am not running the code above because you do have to obtain an API Key (which according to the package requires a developer account that you have to sign up and pay for, which poses its own challenges), but nonetheless this definitely caught my attention as a motivating example for LLMs. Writing regex to successfully get name and age from large data like this would be super difficult.\nHadley then went on to talk about ellmer’s ability to interact with external tools defined by the caller. One downfall of LLMs is that they may not know current information, such as the time of day. Defining an R function that knows the time and registering that tool with the LLM can solve that problem and allow you to answer queries like “How long ago exactly did the Chicago Cubs win the World Series?”. See this article for more information.\nI would assume if I were to use these tools for my research, I would want to understand how accurate they are. Hadley mentioned the vitals package which I will have to look at more if/when I work with any of these LLMs in R.\nMore interesting to me right now is the conversation around the pros and cons of using these tools, and also the impact they may have in education. These are the following things I caught from Hadley’s talk:\nPros\n\nLLMs are amazing for quickly generating demos, shiny apps, example data\n\nHadley showed an example of a rough sketch of a histogram, which he fed to Positron Assistant and told it to create a Shiny app using the Palmer Penguins data.\n\nThis worked… which was kind of frightening to me. At the same time it is cool because you get this base Shiny app code working right away (often the most frustrating part) and can go on to make modifications to your preference.\n\n\nGood at translations (e.g. latex \\(\\rightarrow\\) quarto, R code \\(\\rightarrow\\) stan, SQL \\(\\rightarrow\\) dplyr, json \\(\\rightarrow\\) to unit tests)\n\nAll I could think about was how many hours of my life I could saved instead of translating code from SAS to R at Mayo Clinic by hand…🥲\n\nWhile honestly this task was miserable to me, it was a big part of my job, and it definitely makes me scared for me entry level stats/data science positions which are places where people learn lots and gain skills that allow them to learn and move up in the field (or better understand what type of career they want).\n\n\nExplaining and critiquing code\n\nCons/concerns\n\nCost and equity of access: These tools do cost money, even if not that much. According to Hadley, $5 on Claude can get you pretty far and Gemini has a generous free tier.\nEnvironmental concerns: Hadley implied these are worth considering but small and decreasing on the individual level, and that flying to this conference, for example, is a much more detrimental environmental impact.\n\nI am sure that is true, but I just can’t help to think about the overarching impact if millions and millions of people are using these tools on a daily basis. This isn’t going to be a no impact situation.\n\nData privacy: a definite concern on the individual level. Not a problem for most bigger organizations as most data already lives in some cloud, and cloud providers run LLMs.\nReplacing artists: a definite risk at societal level. Hadley mentioned he is trying to supplement, not replace.\nEvil billionaires: we are just giving more money to evil tech people…there weren’t really any ideas for how to get around that.\n\nSo at the end of this I wasn’t really sure what to think. It was slightly encouraging to hear Hadley mention that there are many drawbacks of LLMs and in many ways coding is still a useful tool (e.g. making small changes, thinking critically, programming can still be faster than asking LLMs to do stuff for you), but in many ways, these tools make me feel bleh, and I think a lot more discussion about the place of these tools in statistics research and education is needed."
  },
  {
    "objectID": "teaching/uscots2025.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "href": "teaching/uscots2025.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "title": "My Reflections from USCOTS 2025",
    "section": "Leveraging LLMs for student feedback in introductory data science courses",
    "text": "Leveraging LLMs for student feedback in introductory data science courses\nAfter Hadley’s talk I went right into another LLM talk by Mine Cetinkaya-Rundel, which covered leveraging LLMs for student feedback in STA 199, an introductory data science and statistical thinking course at Duke (no prerequistes). The class had two exams (20% each), but the largest component of the student grade was once weekly lab assignments graded for accuracy (35%).\nAI policy for class\n\nStudents could use AI tools but must explicitly cite them, and the prompt could not be copied and pasted directly from the assignment (the students had to create the prompt themselves).\nStudents were not allowed to copy and paste the AI narrative verbatim to answer questions.\nStudents were welcome to ask AI questions to enhance their learning and understanding.\n\nProject 1\n\nGoal: A chatbot that hopefully generates good, helpful, and correct answers that come from course content and prefers terminology/methods taught in the course.\nTwo motivating reasons for this\n\nStudents don’t read previous questions on online forums that their classmates have asked, even if the instructor asks them to do this before posting.\nChatGPT usually dosen’t generate answers in line with course content (for example, may give a base R response when tidyverse is taught).\n\nTechnical details\n\nUses Retrevial Augmented Generation (RAG) to focus chatbot on course content and give it context. The chatbot gives the student direction to specific pages of interest in the course textbooks.\n\nThis is accomplished through combining semantic similarity and knowledge graph searches.\n\n\nSQL database of student results (completely anonymized to professor)\n\nSome good interactions, some copy-and-paste directly from assignment, some “fix my code” questions.\n\nEvidence that the AI policy was a bit too optimistic\n\nCannot say the majority of answers it gives is better than other LLMs, but no credit card and the fact that it points to course materials are both pluses.\n\nA kind of sad question: Is the chatbot to read from textbook more motivating to the student than the professor telling them they should? Unknown.\n\nProject 2\n\nGoal: A feedback chatbot that hopefully generates good, helpful, and correct feedback based on an instructor designed rubric and suggests terminology/methods taught in the course.\nMotivating reasons\n\nStudents use AI tools as a first step before thinking about how to approach the task.\n\nA chatbot could be like a friend in the classroom that you turn to for helping thinking through a problem.\nBut also, if it gives them code to run and it works, no thinking will happen. See Microsoft study The Impact of Generative AI on Critical Thinking.\n\nMaybe AI can help TAs redistribute their time toward higher value and more enjoyable touch points with students, and away from repetitive and error-prone tasks which often go unread (giving feedback).\nTAs don’t want to provide detailed feedback to answers generated with AI (not that everyone is doing their hw with AI, but some students are)\nIf very detailed rubrics are already being written to ensure grading equivalency across TAs, it is easy to hand these to LLMs.\n\n\nActivity and Thoughts\nWe then went into an activity where we prompted an LLM (ChatGPT) with the question and the student response and asked it to give feedback. The feedback was very verbose, and commented on several things not part of the question (e.g. the year column is a character, it should be numeric). All of this was technically true, but perhaps not the main point of the question. With a rubric and telling it to be to the point, it was better, but still a lot to parse through for a fairly simple question (which was a pivot longer essentially).\nWe had the following thoughts at my table:\n\nShould we be giving feedback on things not part of the question?\n\nOn a similar note, the LLM might comment on small things that do not matter, making it difficult for the student to identify important concepts from small ones\n\nFor a topic I am fairly comfortable with, the more feedback there is, the less likely I am going to be to read it (personally). LLMs just seem way to verbose to me.\n\nCan we ask students to fill out a form about what type of feedback they prefer? Do they want the “compliment sandwich”, or just straight to the point what is wrong? Is an LLM even capable of being straight to the point?\n\nI think we are assuming that goal of the LLM giving feedback that aligns with what is taught in the course is met, otherwise this would be an issue (e.g. the LLM says to use names_transform in pivot longer when in the course teaches students to modify names in a mutate statement).\nI could have misunderstood, but the start of the session, a point was made about using these LLMs for feedback, not grading. If the TAs are still grading, is the amount of time saved by the TAs in not providing feedback worth the faults that come with using an LLM? Does the grading align with the feedback?\n\nWhile I personally wouldn’t be ready to use LLMs for feedback if I was teaching, I am generally interested to learn more about how this evolves. The idea itself kind of blew my mind, and I think it could be quite useful (particularly for low stakes assignments in courses with hundreds of students). Mine mentioned that a few of the next steps were to continue model evaluation (e.g. cost, speed, accuracy) and tradeoffs as new LLMs are released, and to measure learning outcomes for students using the LLM feedback to understand the effectiveness of this approach."
  },
  {
    "objectID": "teaching/uscots2025.html#topics-workshops",
    "href": "teaching/uscots2025.html#topics-workshops",
    "title": "My Reflections from USCOTS 2025",
    "section": "Topics Workshops",
    "text": "Topics Workshops\nTwo other breakout sessions I attended were Jo Hardin and Nick Horton’s Leveraging data technologies to model bigger datasets and Paul Roback and Laura Boehm Vock’s Integrating Poisson regression into the undergraduate curriculum. Both of these were great! Personally I was not familiar with SQL and DuckDB together prior to the workshop, so it was fun to learn something new that could be really helpful when working with large data/databases. In the other workshop, I really liked the way Paul and Laura explained Poisson regression visually and with very minimal math. If I am able to co-instruct/instruct the CMU REU program at some point, I think it would be fun to have a couple sessions on a portion of the materials from both of these workshops."
  },
  {
    "objectID": "blog/uscots2025/index.html",
    "href": "blog/uscots2025/index.html",
    "title": "My Reflections from USCOTS 2025",
    "section": "",
    "text": "I’m so grateful to have had the opportunity to attend the 2025 United States Conference on Teaching Statistics (USCOTS) in Ames, Iowa! Before jumping into my general reflections, I want to thank Kelly Bodwin and the rest of team behind the POSE: Phase II: Expanding the data.table ecosystem for efficient big data manipulation in R NSF grant for funding my trip, as well as CAUSE for generously waiving the registration fee. I also want to thank the many people that organized this conference, including Allan Rossman, Kelly McConville, Laura Ziegler, and Matt Beckman. The conference was incredibly well run and I cannot imagine how much work went it.\nThis was my first conference, and it was such a privilege to listen to talks and have discussions with many of the leaders in statistics and data science education. This was very inspiring, and also a bit overwhelming—these individuals have done so much and have so many smart and creative ideas. Both at the conference and typing up these reflections, it was hard to not feel some level of imposter syndrome—unlike the majority of attendees, I have never even taught a class! All of this is just to say, I have lots of to learn, and I feel so lucky to have gotten a chance to jumpstart that learning process by connecting with so many smart people (and am very grateful for how kind and welcoming they all were)!"
  },
  {
    "objectID": "blog/uscots2025/index.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "href": "blog/uscots2025/index.html#life-as-a-liberal-arts-statistician-joys-and-challenges",
    "title": "My Reflections from USCOTS 2025",
    "section": "Life as a liberal arts statistician: joys and challenges",
    "text": "Life as a liberal arts statistician: joys and challenges\nIdeas for success in the classroom\n\n\nSurvey the class periodically and discuss the results (both things to change and why particular aspects will remain the same) with the students\n\nLearn from others, both in your department and others\n\n\nSit in on classes\n\n\nTake notes after class on what worked and what didn’t for the future\n\n\nBe active professionally\n\nPre-tenure, make strategic choices. Play the odds to get research that students can help with.\n\nPost-tenure, focus on what nourishes you (e.g. publishing an open source textbook)\n\nCollaborate with non-statistical collegues\n\nAdapt research question (and timeline for undergrads)\n\nServe others\n\nJump into service early, both internally and externally\n\nBe happy and proactive with “yeses”, then you can say “no” to things not as interesting to you\n\nGrowing a program\n\nHelpful to have a customizable concentration/minor\n\nHaving a center for interdisciplinary research is a good way to create community and can lead to research collaborations\n\nAdd modern courses to curriculum (e.g. add a data science course and remove an advanced modeling requirement)\n\nFind projects everywhere! Examples: a bagel shop running out of your favorite flavor, library checkout data\n\nMany aspects are constantly changing\n\nClassroom dynamics and design\n\nTechnology (classroom tech, programming languages)\n\nRising popularity of statistics and data science in recent years\n\nRelationship with math and computer science\n\nData ethics\n\n\nChallenges\n\nSemesters are hectic\n\nSalary\n\nGrading\n\n5% of students\n\nSlow changes in academia\n\nLifelong learning can be exhausting\n\n\nJoys\n\nIt usually doesn’t feel like work\n\nAwesome colleagues\n\nStudents want to make world better\n\nCreating a good learning environment\n\nConstant renewal\n\nCollaborating with experts in different fields\n\nTaking pride in your institution\n\nLifelong learning can be exhilarating\n\n\nShort reflection: The group I was in talked a lot about growing a program and the interaction between math, statistics, and computer science. Some people in the group were in a statistics/mathematics department separate from computer science, and mentioned this being a point of tension. Right now, when data science is so popular, it is ideal to have these departments collaborating and working together on what courses should count for credits to particular majors or minors (as well as just to maximize student learning). Having a combined department at Macalester, this was not something I’d thought about before, but it must make conversations about forming a data science major or minor, for instance, easier. Some other issues mentioned were struggle to find a form of service people find exciting, and issues with institutional financials. I hadn’t previously thought about either of these topics when thinking about a liberal arts career, so it was nice to listen to what others had to say."
  },
  {
    "objectID": "blog/uscots2025/index.html#authentic-assessment-with-oral-exams",
    "href": "blog/uscots2025/index.html#authentic-assessment-with-oral-exams",
    "title": "My Reflections from USCOTS 2025",
    "section": "Authentic assessment with oral exams",
    "text": "Authentic assessment with oral exams\nWith students now being able to offload parts of “writing to learn” assignments (e.g. data analysis reports) to LLMs, many people at this conference were interested in discussing alternative forms of assessment. One option that has potential in liberal arts classrooms is oral exams. While the potential benefit of oral exams is high, the primary concern seems to be time, both for the professor and students.\nPros\n\nPersonal connection between professor and student.\nProfessors having used oral exams noted they could tell level of the student’s understanding with just a couple of minutes.\n\nGoing off this, the students did not refute their score. The student could tell if they weren’t matching the professor’s expectation for level of understanding.\n\nOpportunity to understand the student’s misunderstandings and redirect them.\n\nFor example, if a student said, “The probability of the null being false is 95%”, you could ask a follow up question like, “Wait, can you explain a little more why that is the case?”.\n\nNo option for student to offload thinking to LLMs.\nGood practice for job interviews.\nOpportunity to meet the student where they are at. For students less comfortable with the material, you can redirect and ask them different questions than a student clearly grasping the concepts.\n\nCons\n\nTime. With 2-3 sections of a class with 20 students, you are trying to schedule 40-60 fifteen minute blocks. That is a lot of time out of the professor’s week, and it is also hard to find time outside of class for students with busy schedules.\n\nCan we do paired oral exams to cut down on that?\n\nHow are students then graded as a pair? What if one partner dominates the conversation? Side note: in an ungrading scenario, this would not be as much of an issue and something the students could reflect on themselves after the fact.\nIdea: Have the pair have a conversation between themselves based on a set of prompts given to them.\n\n\nStudent anxiety. The idea of an oral exam is daunting for many students.\n\nPaired exams could potentially help reduce this anxiety.\nCertain students may understand the concepts well but not be able to verbally communicate on the spot.\n\nAlternatively, you could say that some students have test anxiety, but this is just a common form of assessment so we typically ignore this issue.\n\n\nFairness and equity: can we ensure students are evaluated the same?\n\nOther notes\n\nAs opposed to an exam, how much material can you assess? If, for example, an exam were to cover machine learning methods such as LASSO, clustering, splines, random forests, and GAMs, are you assessing all of these methods in a 15 minute oral exam but sacrificing depth, or vice versa?\nOne professor found no correlation between student oral exam scores and test scores in a semester where they used both as modes of assessment.\n\nI found this really interesting, and don’t think is necessarily a pro or a con, but maybe a reason to utilize multiple forms of assessment within a semester."
  },
  {
    "objectID": "blog/uscots2025/index.html#project-design",
    "href": "blog/uscots2025/index.html#project-design",
    "title": "My Reflections from USCOTS 2025",
    "section": "Project Design",
    "text": "Project Design\nIn this discussion, we covered some ideas make sure students are getting the most they can out of class projects.\nProject setup\n\nProviding a sample report: should we or should we not?\n\nUseful so that students understand expectations, but they may follow a sample report too closely and not build their own writing style.\nIdea: provide a report with several flaws, and take class time to critique the report and discuss potential changes.\n\nPeer reviews: tell students it is not a time to be “MN nice”. Give honest feedback to help your classmate get a better grade.\nFor group projects, have formal meetings with each project group to discuss next modeling steps, what is going well or what they are struggling with, etc.\n\nPersonal note: I think effectively communicating what I have done and have questions on in an organized matter in a one-on-one meeting was one of the things I struggled with transitioning out of undergrad (both working at Mayo Clinic and starting research in my PhD). Maybe this could help prepare students for that.\n\n\nProject Ideas\n\nClass time activity: story boarding with data. Have students introduce an idea, most important aspects from EDA, conflict/main characters.\n\nStudents don’t necessarily have to even have data, just have them sketch and type of plot that they might expect.\nHave them decide on an ideal storyboard: this is a good way to get them to think about the project workflow and communicating big ideas (perhaps to a nonstatistical audience too).\n\nPodcast (~10 minutes) where you encourage students to write a script (so they think through what they will say) and the verbally communicate their findings with a partner.\n\nHave students send pdf of visualizations that they talk about.\nOne professor found a student that didn’t typically participate in class was great at this."
  },
  {
    "objectID": "blog/uscots2025/index.html#doing-data-science-in-positron",
    "href": "blog/uscots2025/index.html#doing-data-science-in-positron",
    "title": "My Reflections from USCOTS 2025",
    "section": "Doing data science in Positron",
    "text": "Doing data science in Positron\nThis workshop by Mine Cetinkaya-Rundel and Hadley Wickham a great first introduction to Positron, which I didn’t know anything about prior to this conference.\nWhat is it?\n\nA next-generation data science IDE that feels like a fusion of RStudio and VS Code.\n\nHappy note: RStudio is not going away!\n\n\nBenefits of Positron\n\nMain benefit: easily combine R and Python in one document\n\nPersonally not currently a Python user, but I know a lot of people that are and I think this could be great for classes or research where groups are using both!\n\nAbility to effortlessly switch between versions of R (e.g. R 4.3.3, 4.5.0)\nCommand palette (Ctrl/Cmd + Shift + P) makes searching for and executing commands easy\nEasy to customize layout and split screen between files.\nMultiple concurrent interpreter sessions, which can be a mix of different R versions, mix of R and Python sessions, or multiple instances of a single R version\n\nAllows you to run different code while something is taking a while to run! This is awesome to me.\n\nCan sort your environment (e.g. by most recent)\nAir: an extension that automatically formats your code nicely\nYou can Preview (Render) your document and it will appear in the viewer pane instead of opening in your internet browser—easy to make side by side changes\nArea for previewing \\(\\LaTeX\\) equations\nEasy to save and share plots\n\nOther differences\n\nNo inline plots\n\nOutputs to “plots” pane, which I think I agree with most people is nice because the inline plots can make your document a bit laggy.\n\nRun button is different/a bit smaller? Personally like the RStudio version better this is not a big deal.\nPositron automatically updates (good, I think?)\n\nThings for me to learn more about\n\nrig\nAir\nSnippets\nPositron Assistant\n\nOverall reflections\nVery cool, also a bit overwhelming—there are just so many features. Hadley and Mine mentioned when you see someone’s RStudio, you can easily become acquainted with their setup, but Positron is completely customizable (which is mostly a pro, but can perhaps be a con in a teaching setting?). At the same time, so many of these features seem useful and Mine and Hadley did an awesome job motivating and explaining the tool. I really like the ability to continue to run smaller code jobs while another piece is running in the background. The mix of versions of R is also great. I am not sure if/when I will be fully doing my coding in Positron over RStudio, but I am excited to get more experience with it."
  },
  {
    "objectID": "blog/uscots2025/index.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "href": "blog/uscots2025/index.html#a-no-bullshit-guide-to-programming-with-llms-in-r",
    "title": "My Reflections from USCOTS 2025",
    "section": "A no bullshit guide to programming with LLMs in R",
    "text": "A no bullshit guide to programming with LLMs in R\nThis talk by Hadley Wickham introduced the ellmer R package, which he developed to interact with LLMs in R. As someone that enjoys the challenges and satisfaction of coding and figuring out little tricks, I don’t love the idea of offloading parts of that process to LLMs. But I know this is a part of the current reality that we live in, and can make some dreadful tasks much faster, so I wanted to listen with an open mind.\nOne of Hadley’s first motivating examples had us discuss at our table how we’d extract name and age from text data, such as the following:\n\nprompts &lt;- c(\"I go by Alex. 42 years on planet Earth and counting.\", \n             \"Hey, I'm turning 27 and my name is Jamal\", \n             \"I'm Lei Wei and nineteen years young.\",\n             \"My name is Brett and I'm turning the big 5-0 this year.\")\n\nClearly, this is not a simple regex expression due the mix of age being typed and numeric, names being 1-2 words, and the fact that there is no global pattern in sentence structure. I learned that this is something an LLM could do with essentially the following code:\n\nlibrary(ellmer)\nchat &lt;- chat_anthropic()\nchat$chat(\"Extract the name and age from each sentence I give you\")\n\nchat$chat(prompts[[1]])\n\nAnd this essentially returned the name and age for each prompt. I am not running the code above because you do have to obtain an API Key (which according to the package requires a developer account that you have to sign up and pay for, which poses its own challenges), but nonetheless this definitely caught my attention as a motivating example for LLMs. Writing regex to successfully get name and age from large data like this would be super difficult.\nHadley then went on to talk about ellmer’s ability to interact with external tools defined by the caller. One downfall of LLMs is that they may not know current information, such as the time of day. Defining an R function that knows the time and registering that tool with the LLM can solve that problem and allow you to answer queries like “How long ago exactly did the Chicago Cubs win the World Series?”. See this article for more information.\nI would assume if I were to use these tools for my research, I would want to understand how accurate they are. Hadley mentioned the vitals package which I will have to look at more if/when I work with any of these LLMs in R.\nMore interesting to me right now is the conversation around the pros and cons of using these tools, and also the impact they may have in education. These are the following things I caught from Hadley’s talk:\nPros\n\nLLMs are amazing for quickly generating demos, shiny apps, example data\n\nHadley showed an example of a rough sketch of a histogram, which he fed to Positron Assistant and told it to create a Shiny app using the Palmer Penguins data.\n\nThis worked… which was kind of frightening to me. At the same time it is cool because you get this base Shiny app code working right away (often the most frustrating part) and can go on to make modifications to your preference.\n\n\nGood at translations (e.g. latex \\(\\rightarrow\\) quarto, R code \\(\\rightarrow\\) stan, SQL \\(\\rightarrow\\) dplyr, json \\(\\rightarrow\\) to unit tests)\n\nAll I could think about was how many hours of my life I could saved instead of translating code from SAS to R at Mayo Clinic by hand…🥲\n\nWhile honestly this task was miserable to me, it was a big part of my job, and it definitely makes me scared for me entry level stats/data science positions which are places where people learn lots and gain skills that allow them to learn and move up in the field (or better understand what type of career they want).\n\n\nExplaining and critiquing code\n\nCons/concerns\n\nCost and equity of access: These tools do cost money, even if not that much. According to Hadley, $5 on Claude can get you pretty far and Gemini has a generous free tier.\nEnvironmental concerns: Hadley implied these are worth considering but small and decreasing on the individual level, and that flying to this conference, for example, is a much more detrimental environmental impact.\n\nI am sure that is true, but I just can’t help to think about the overarching impact if millions and millions of people are using these tools on a daily basis. This isn’t going to be a no impact situation.\n\nData privacy: a definite concern on the individual level. Not a problem for most bigger organizations as most data already lives in some cloud, and cloud providers run LLMs.\nReplacing artists: a definite risk at societal level. Hadley mentioned he is trying to supplement, not replace.\nEvil billionaires: we are just giving more money to evil tech people…there weren’t really any ideas for how to get around that.\n\nSo at the end of this I wasn’t really sure what to think. It was slightly encouraging to hear Hadley mention that there are many drawbacks of LLMs and in many ways coding is still a useful tool (e.g. making small changes, thinking critically, programming can still be faster than asking LLMs to do stuff for you), but in many ways, these tools make me feel bleh, and I think a lot more discussion about the place of these tools in statistics research and education is needed."
  },
  {
    "objectID": "blog/uscots2025/index.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "href": "blog/uscots2025/index.html#leveraging-llms-for-student-feedback-in-introductory-data-science-courses",
    "title": "My Reflections from USCOTS 2025",
    "section": "Leveraging LLMs for student feedback in introductory data science courses",
    "text": "Leveraging LLMs for student feedback in introductory data science courses\nAfter Hadley’s talk I went right into another LLM talk by Mine Cetinkaya-Rundel, which covered leveraging LLMs for student feedback in STA 199, an introductory data science and statistical thinking course at Duke (no prerequistes). The class had two exams (20% each), but the largest component of the student grade was once weekly lab assignments graded for accuracy (35%).\nAI policy for class\n\nStudents could use AI tools but must explicitly cite them, and the prompt could not be copied and pasted directly from the assignment (the students had to create the prompt themselves).\nStudents were not allowed to copy and paste the AI narrative verbatim to answer questions.\nStudents were welcome to ask AI questions to enhance their learning and understanding.\n\nProject 1\n\nGoal: A chatbot that hopefully generates good, helpful, and correct answers that come from course content and prefers terminology/methods taught in the course.\nTwo motivating reasons for this\n\nStudents don’t read previous questions on online forums that their classmates have asked, even if the instructor asks them to do this before posting.\nChatGPT usually dosen’t generate answers in line with course content (for example, may give a base R response when tidyverse is taught).\n\nTechnical details\n\nUses Retrevial Augmented Generation (RAG) to focus chatbot on course content and give it context. The chatbot gives the student direction to specific pages of interest in the course textbooks.\n\nThis is accomplished through combining semantic similarity and knowledge graph searches.\n\n\nSQL database of student results (completely anonymized to professor)\n\nSome good interactions, some copy-and-paste directly from assignment, some “fix my code” questions.\n\nEvidence that the AI policy was a bit too optimistic\n\nCannot say the majority of answers it gives is better than other LLMs, but no credit card and the fact that it points to course materials are both pluses.\n\nA kind of sad question: Is the chatbot to read from textbook more motivating to the student than the professor telling them they should? Unknown.\n\nProject 2\n\nGoal: A feedback chatbot that hopefully generates good, helpful, and correct feedback based on an instructor designed rubric and suggests terminology/methods taught in the course.\nMotivating reasons\n\nStudents use AI tools as a first step before thinking about how to approach the task.\n\nA chatbot could be like a friend in the classroom that you turn to for helping thinking through a problem.\nBut also, if it gives them code to run and it works, no thinking will happen. See Microsoft study The Impact of Generative AI on Critical Thinking.\n\nMaybe AI can help TAs redistribute their time toward higher value and more enjoyable touch points with students, and away from repetitive and error-prone tasks which often go unread (giving feedback).\nTAs don’t want to provide detailed feedback to answers generated with AI (not that everyone is doing their hw with AI, but some students are)\nIf very detailed rubrics are already being written to ensure grading equivalency across TAs, it is easy to hand these to LLMs.\n\n\nActivity and Thoughts\nWe then went into an activity where we prompted an LLM (ChatGPT) with the question and the student response and asked it to give feedback. The feedback was very verbose, and commented on several things not part of the question (e.g. the year column is a character, it should be numeric). All of this was technically true, but perhaps not the main point of the question. With a rubric and telling it to be to the point, it was better, but still a lot to parse through for a fairly simple question (which was a pivot longer essentially).\nWe had the following thoughts at my table:\n\nShould we be giving feedback on things not part of the question?\n\nOn a similar note, the LLM might comment on small things that do not matter, making it difficult for the student to identify important concepts from small ones\n\nFor a topic I am fairly comfortable with, the more feedback there is, the less likely I am going to be to read it (personally). LLMs just seem way to verbose to me.\n\nCan we ask students to fill out a form about what type of feedback they prefer? Do they want the “compliment sandwich”, or just straight to the point what is wrong? Is an LLM even capable of being straight to the point?\n\nI think we are assuming that goal of the LLM giving feedback that aligns with what is taught in the course is met, otherwise this would be an issue (e.g. the LLM says to use names_transform in pivot longer when in the course teaches students to modify names in a mutate statement).\nI could have misunderstood, but the start of the session, a point was made about using these LLMs for feedback, not grading. If the TAs are still grading, is the amount of time saved by the TAs in not providing feedback worth the faults that come with using an LLM? Does the grading align with the feedback?\n\nWhile I personally wouldn’t be ready to use LLMs for feedback if I was teaching, I am generally interested to learn more about how this evolves. The idea itself kind of blew my mind, and I think it could be quite useful (particularly for low stakes assignments in courses with hundreds of students). Mine mentioned that a few of the next steps were to continue model evaluation (e.g. cost, speed, accuracy) and tradeoffs as new LLMs are released, and to measure learning outcomes for students using the LLM feedback to understand the effectiveness of this approach."
  },
  {
    "objectID": "blog/uscots2025/index.html#topics-workshops",
    "href": "blog/uscots2025/index.html#topics-workshops",
    "title": "My Reflections from USCOTS 2025",
    "section": "Topics Workshops",
    "text": "Topics Workshops\nTwo other breakout sessions I attended were Jo Hardin and Nick Horton’s Leveraging data technologies to model bigger datasets and Paul Roback and Laura Boehm Vock’s Integrating Poisson regression into the undergraduate curriculum. Both of these were great! Personally I was not familiar with SQL and DuckDB together prior to the workshop, so it was fun to learn something new that could be really helpful when working with large data/databases. In the other workshop, I really liked the way Paul and Laura explained Poisson regression visually and with very minimal math. If I am able to co-instruct/instruct the CMU REU program at some point, I think it would be fun to have a couple sessions on a portion of the materials from both of these workshops."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html",
    "href": "teaching/llmwriting/llmwriting.html",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "",
    "text": "Large Language Models (LLMs) have become ubiquitous in academic settings, particularly for writing (Baek, Tate, and Warschauer 2024). Recently, Reinhart et al. (2024) identified systematic differences between LLM and human writing by leveraging Biber feature and lemma usage rates.\nThe overarching goal of our project is to identify whether (and if so, how) students’ statistics writing has systematically shifted toward being more similar to LLM academic writing since LLMs became widely accessible in 2022.\nOur data contains two corpora. The HAP-E Corpus contains 1,227 documents for which ChatGPT-4o (August 2024) was asked to generate the next 500 words (in the same tone and style) when prompted with a piece of academic writing (Brown 2024). The Student Corpus contains 2,353 student reports from three undergraduate statistics courses at Carnegie Mellon University. 36-202: Methods for Statistics & Data Science is typically the second statistics course students take. 36-401: Modern Regression and 36-402: Advanced Methods for Data Analysis are advanced courses students take in their junior or senior year. Students are given a dataset and asked to answer a domain question with a report in the IMRaD format. The average report length is 1,700 words. All reports are anonymized and collected under an IRB."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#comparing-rhetorical-features",
    "href": "teaching/llmwriting/llmwriting.html#comparing-rhetorical-features",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Comparing Rhetorical Features",
    "text": "Comparing Rhetorical Features\nWe start by extracting Biber feature rates (per 1,000 words) for each document in the HAP-E and Student Corpora. We then conduct linear discriminant analysis (LDA) on the ChatGPT and student reports from 2021 using standardized Biber feature rates across all documents. Throughout our analysis, we treat 2021 as a “pre-LLM era”, 2022 as an intermediate year, and 2023-2025 reports as being from a time when LLMs were utilized by and accesible to students. Using the standardized Biber feature rates, we then project 2022-2025 writing onto the first linear discriminant (LD1) space and observe the resulting distribution of LD1 scores. We do this process separately for introductory (36-202) and advanced classes (36-401/402). Due to the advanced classes having fewer reports—particularly none in 2024-2025—the remainder of our analysis focuses on 36-202.\n\n\n\nFigure 1: Distribution of LD1 scores by year for 36-202. The dashed line represents the mean LD1 score for each distribution.\n\n\nKey takeaways from Figure 1 include that 36-202 students are writing more like ChatGPT with each year, on average. We also observe more variability in LD1 scores in more recent years, suggesting that some students rely on ChatGPT for writing portion(s) of their report while other may not use it all.\nIn order to better understand what portions of the report students may use LLMs for, we break the report into five sections. We let the first 20% of sentences represent the introduction of the report, 20-80% represent the middle (EDA/methods) of the report, and the final 20% of sentences represent the conclusion. The results are displayed in Figure 2.\n\n\n\nFigure 2: Distribution of LD1 scores by section of the report, with 2021 reports in orange and 2023-2025 reports in blue.\n\n\nFigure 2 displays that across all sections of the report, students’ writing style is more similiar to ChatGPT’s writing style in 2023-2025 than it was in 2021, aligning with Figure 1. Breaking it down by section, even in 2021 (pre LLMs), the style of writing for the introduction and conclusion was more similar to ChatGPT’s style of writing than the methods section of the report. However, going from 2021 to 2023-2025, we see the distribution of LD1 scores shift toward ChatGPT’s LD1 distribution the most for the introduction and conclusion of the report. This implies these sections of the report are likely where students are using LLMs the most1.\nYou may be wondering what features of writing are more ChatGPT-like and what features are more human-like. Table 2 displays our top ten standardized Biber features that proved to be most helpful for classifying ChatGPT and student writing in our linear discriminant analysis. A negative LD1 coefficient indicates that the feature is more prevalent in ChatGPT writing than 2021 student writing, while a postive feature indicates the opposite. A few features that make sense to me: on average, ChatGPT uses longer words and more attributive adjectives, while students tend use be as a main verb and use first-person pronouns more often. Note that Biber features do not take punctuation into account, which I mention because LLMs like to use em dashes. This would be something additional to look into in the future.\n\n\n\nTable 2: Top 10 Biber features most important for classifying ChatGPT and and student writing in linear discriminant analysis. Some descriptions and examples are taken directly from Reinhart et al. (2024)."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#comparing-lemma-usage",
    "href": "teaching/llmwriting/llmwriting.html#comparing-lemma-usage",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Comparing Lemma Usage",
    "text": "Comparing Lemma Usage\nWe have looked at rhetorical features, but what about the words themselves? After hearing that LLMs at one point used words like delve, tapestry, or commraderie a lot, we were curious to see if our student writing saw an uptick in any of ChatGPT’s favorite verbs. To do so, we identify ChatGPT’s top 50 favorite verbs by doing a simple count of how many times each verb was used in the HAP-E corpus, removing auxiliary verbs. We then find the rate (per 1000 words) at which these verbs are used in the student corpus each year. Figure 3 displays how the frequency of usage of ChatGPT’s top ten favorite verbs have changed in student reports over recent years. Several of these verbs (e.g. provide, remain, ensure, reduce, address, enhance, offer) have seen noticeable increases.\n\n\n\nFigure 3: Frequency of ChatGPT’s top 10 favorite verbs in student writing since 2021.\n\n\nWe go on to perform a keyness analysis to identify which of the 50 verb lemmas have a significant positive frequency change from 2021 to 2023-2025 in the student writing. A keyness analysis is a special kind of chi-square test to compare observed and expected frequencies in text. We find that 75% of these 50 verb lemmas increased frequency in 36-202 student reports from 2021 to 2023-2025, and 33% of these positive changes were statistically significant after adjusting for multiple testing using the Bonferroni correction.\nWe want to make sure that the topic of the report does not play into student word choice (e.g. a prompt “analyze the effect of a reduction in required classes on…” may have the word reduce used very often). Luckily, in 36-202, students were given the same set of datasets and prompts to choose from each year. We additionally conduct a concordance analysis, pulling out the four words prior to and following each of top ten verbs in order to ensure the usage of these verbs is not context dependent. Figure 4 shows a sample of the ways in which provide was used in 36-202 reports in 2025. We see the usage does not appear to be context dependent, and we additionally observe repeated use cases such as “provide evidence-based recommendations”.\n\n\n\nFigure 4: Concordance analysis for provide."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#concluding-thoughts",
    "href": "teaching/llmwriting/llmwriting.html#concluding-thoughts",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nWe see there has been a systematic shift in both the style and vocabulary of students’ statistics reports toward ChatGPT in both lower and upper division courses at Carnegie Mellon in the era of LLMs. The writing style for students’ introductions and conclusions has particularly become more similar to ChatGPT’s writing style, on average.\nWe are still figuring out the next steps for this project, but it might be interesting to compose a ChatGPT corpus where each text is a report generated by the prompt of the assignment. This may be a more fair comparison to the student writing, as opposed to text generated by ChatGPT to meet the style and tone of expert academic writing. If this corpus was used in the linear discrminant analysis, I would expect an even larger shift of the distribution of LD1 scores for 2023-2025 reports toward to the ChatGPT distribution. Additionally, I would be curious to see if ChatGPT writing is “good”. If, using Biber features and linear discriminant analysis, we were to compare both student reports in the LLM and pre-LLM era to expert academic writing, would one perform better than the other? Finally, while it would be difficult to make happen, it would also be interesting to see how these results vary by institution (e.g. liberal arts versus R1 institutions).\nWhile the extensions above may help provide more information, that isn’t going to change what is clear—students are using LLMs for writing. Sara and I are curious about what this means. We both went to liberal arts colleges where writing (e.g. data analysis projects, reports, portfolios, etc) was at the center of our learning. Personally, writing-to-learn assignments were the place where I would process concepts and immediately figure out my pitfalls when I could not figure out how to explain something. The big question (that I think everyone has) is: do these assignments need modification in an era of LLMs? If so, how do we restructure these assignments to better support student learning?\nAt USCOTS, it was really interesting to hear ideas on this. Some thoughts:\n\nTurn to alternative forms of assessment, such as oral exams. See my USCOTS reflections for a discussion the pros and cons of oral exams.\n\nIn short, I think this could be a good option for advanced capstone classes at liberal arts colleges, but it still may not offer the all of the same benefits as take home data analysis projects (the student has less freedom in topic, cannot produce something they can share to network or look back on to learn from, may have oral exam anxiety).\n\nHave students write a data analysis report during classtime—or more realistically, part of a report. We observed that students are most likely to offload writing the introduction and conclusion to LLMs, so potentially we could provide some already done EDA plots, methods, and results to the student and have them write the introduction or conclusion during classtime on paper.\n\nThere could also be a class discussion where as a group you read some pieces of writing and critique them (kind of like a peer review) so students think more about carefully about their writing style and process. Again, this does not yield all the same benefits as a take home data analysis project, but it is an idea.\n\nTry some AI policy—I liked Nick Horton’s idea where he asked the students in his class to develop an AI policy on the first day of the semester, he refined it, then they discussed it as a group again and students were expected to adhere to it.\n\nI definitely think this could work well at a liberal arts college, where class and small group discussions are the norm and professors are typically the ones grading major assessments.\n\nFinally and optimistically, maybe some form of pitching the laundry list of reasons why we shouldn’t use LLMs for writing reports could help solve the problem.\n\nThese are all just thoughts. Right now I don’t have experience teaching a class or know what I’d do if I were a professor, but I am excited to keep learning! Thanks so much for reading, and please feel free to reach out to me at efranke@andrew.cmu.edu with feedback, questions, or ideas :)"
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#acknowledgements",
    "href": "teaching/llmwriting/llmwriting.html#acknowledgements",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks so much to Alex Reinhart for his support on this project. We are also grateful to the TeachStat working group at Carnegie Mellon for their valuable feedback and suggestions, and Eric Shau for pre-processing the student reports."
  },
  {
    "objectID": "teaching/llmwriting/llmwriting.html#footnotes",
    "href": "teaching/llmwriting/llmwriting.html#footnotes",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnecdotally, this makes sense. I have had at least four people tell me they are using LLMs for their introductions/conclusions in the last couple of months.↩︎"
  },
  {
    "objectID": "blog/rss/index.html",
    "href": "blog/rss/index.html",
    "title": "Sole searching: How super shoes have changed marathoning",
    "section": "",
    "text": "“All you need is a pair of shoes.” It’s a simple phrase—maybe one a friend has used either to encourage you to try running or to simply claim it as one of the world’s most accessible sports. But over the last eight years, it has become abundantly clear that for competitive runners, not all shoes are created equal. In 2016, Nike released the Vaporfly 4%, the world’s first super shoe, changing the trajectory of running forever(Aciman 2024). Most broadly, a super shoe is a lightweight running shoe with a carbon-fiber plate through the middle of the sole. The stiffness of the plate rolls your foot forward with each step, decreasing the amount you need to push off the ground. The result is better running economy—less energy is needed to run the same pace, meaning runners can conserve that energy to run farther, or expend it to run faster(Goldstein 2023).\nThe impact of super shoes has been most prevalent in long distance events, specifically the marathon, a popular 42.195 kilometer (26.2 mile) road race. The top 11 men’s and top 5 women’s marathon times have all come since the introduction of super shoes in 2016. Figure 1 displays the progression of the men’s and women’s world marathon record dating back to 1950 (“Marathon World Record Progression” 2025). Dennis Kimetto held the men’s world record prior to the introduction of super shoes, completing the 2014 Berlin Marathon in 2:02:57. Since then, the men’s world record has fallen three times and is currently 2:00:35, set by Kelvin Kiptum at the 2023 Chicago Marathon. That’s a pace of 2:51 per kilometer (4:36 per mile)! On the women’s side, Paula Radcliffe held the pre-super shoe marathon world record with a time of 2:15:25, set at the 2003 London Marathon. This record remained intact until 2019. Today, Ruth Chepng’etich sits atop the leaderboard with a time of 2:09:56, set at the 2024 Chicago Marathon.\nFigure 1: Progression of men’s (top) and women’s (bottom) world marathon record since 1950.\nIn addition to elite runners, the marathon is a popular event for those running as a hobby, whom we will refer to as everyday runners. The marathon’s popularity has grown noticeably since the COVID-19 pandemic, with many picking up running as a way to find community or routine. The 2025 London Marathon recently set the world record with a total 56,640 finishers—and over 800,000 individuals had applied to run the race(Pilastro 2025). Super shoes have become popular among the everyday runner, though their benefit is less certain, specifically at slower paces (such as above 6:00/km). In these cases, some runners may stop saving energy and instead need additional energy to get up and over the carbon plate, though more research is needed(Joubert, Dominy, and Burns 2023)."
  },
  {
    "objectID": "blog/rss/index.html#sweet-home-chicago",
    "href": "blog/rss/index.html#sweet-home-chicago",
    "title": "Sole searching: How super shoes have changed marathoning",
    "section": "Sweet Home Chicago",
    "text": "Sweet Home Chicago\nIn order to better understand the role that super shoes play for both elite and everyday runners, we use a publicly available dataset of all Chicago Marathon finishers from 1996-2023(Rock 2024). Home to both the men’s and women’s marathon world records, the Chicago Marathon is a flat and fast course. Elite runners often bring pacers to the race to help them strive for a goal time, whether that be a personal best or a world record. This makes the Chicago marathon ideal for an analysis on super shoes, as hillier races (such as the Boston Marathon or New York City Marathon) can end up being quite tactical with the finishing times varying greatly year to year. Another benefit is the worldwide presence of the Chicago Marathon. Figure 2 displays a map indicating the home countries of finishers from the 2023 Chicago Marathon. These finishers come from 137 different countries, with over 1,000 finishers from Mexico, the United Kingdom, Canada, Brazil, and China, in addition to the United States.\n\n\n\n\n\n\n\n\nFigure 2: Home countries of the 2023 Chicago Marathon finishers. Countries in gray had no participants. 137 countries were represented, making the Chicago Marathon an international event."
  },
  {
    "objectID": "blog/rss/index.html#super-shoes-elite-gains",
    "href": "blog/rss/index.html#super-shoes-elite-gains",
    "title": "Sole searching: How super shoes have changed marathoning",
    "section": "Super shoes, elite gains",
    "text": "Super shoes, elite gains\nWe start by looking at the progression of elite runners since the start of the super-shoe era. Figure 3 plots the average finishing time for the top ten finishers of the men’s and women’s division of the Chicago marathon each year, with the dashed vertical line denoting 2017, the year most elite runners began using super shoes. While there is a substantial amount of variation in the average finishing time year to year, there is evidence of elite times getting faster over the years, particularly in the women’s division. However, Figure 3 makes it clear there may be other factors at play for how quickly the top elite athletes finish. For example, there is a noticeable spike in average finishing time in both divisions in 2007. The Chicago marathon is held the second Sunday in October, and weather can be a major factor in results. In 2007, the temperature peaked at 31.1°C (88°F) raceday afternoon. While the elite race begins at 7:30 am, the temperature was already 24°C (75°F) mid-race at 9:00 am. For most runners, the optimal marathon temperature is roughly 7°C (45°F). To accurately quantify the impact of super shoes, we will need to account for weather differences year to year.\n\n\n\n\n\n\n\n\nFigure 3: Averaging finishing time of the top 10 athletes of the men’s and women’s division of the Chicago Marathon each year. The shaded portion represents one standard deviation, so in years with a smaller buffer, the top 10 runners finished closer together. The dashed line denotes the start of the elite super-shoe era in 2017.\n\n\n\n\n\nIn order to understand the impact of super shoes on elite marathon times, we take the elite marathon fields from 2011-2023. This gives an equal number of years in the pre-super-shoe era (2011-2016) and the super-shoe era (2017-2023, with no race in 2020). Across 338 athletes, there are 525 finishing times, as several elite runners competed at the Chicago Marathon multiple years. For instance, Ruth Chepng’etich of Kenya took first on the women’s side in 2021 and 2022, and second in 2023. For this reason, we use a mixed effects model to account for any repeated measures within participants(Meteyard and Davies 2020). The model includes random intercepts to account for differences in baseline athlete capabilities, recognizing that some elite runners are naturally faster or slower than others. In our model, we control for athlete age on race day, which is a binned variable with five-year age buckets (e.g. 24-28, 29-33, 34-38, etc). We also control for participant sex (male versus female) and the temperature at 9:00 am on race morning. Lastly, we include year in the model. Figure 1 displays that runners are getting better over time regardless of the shoes, as quite a few men’s and women’s world records were set in the decades leading up the super-shoe era. Including the year in the model attempts to isolate the super shoe effect. Our variable of interest is whether the race took place in the super-shoe era, and our outcome is the athlete’s finishing time in minutes.\nOur results show an estimate of -2.69 and a 95% confidence interval of (-4.38, -0.99) on the super-shoe era coefficient. This means we are 95% confident that, on average, an elite marathoner’s finishing time in the super-shoe era is between 59 seconds and 4 minutes and 23 seconds faster than the pre-super-shoe era. This estimate accounts for differences in weather and the composition of the elite field between the two time periods, as well as the overarching trajectory of running improvement over time.\nWhile the estimated reduction of roughly 2 minutes and 41 seconds with super shoes may not seem like much, this is substantial in the context of elite running. For instance, a current topic in the running world is the possibility of a man breaking two hours in the marathon. While Eliud Kipchoge ran 1:59:40 in 2019, it is not officially recognized as a world record because it was not done under race conditions(Keh 2019). Using our model, we can estimate the probability of Dennis Kimetto—the fastest man in the pre-super-shoe era—breaking two hours in the super-shoe era, had the conditions (his age and the raceday weather) been equivalent. Kimetto’s Chicago Marathon winning time in 2013 was 2:03:45. Our model estimates Kimetto would have a 12.2% chance of breaking two hours in 2023, as compared to 2.3% in the pre-super-shoe era. There is evidence super shoes are a key factor in achieving this goal that pushes the boundaries of human capability."
  },
  {
    "objectID": "blog/rss/index.html#boost-or-bust-for-everyday-runners",
    "href": "blog/rss/index.html#boost-or-bust-for-everyday-runners",
    "title": "Sole searching: How super shoes have changed marathoning",
    "section": "Boost or bust for everyday runners?",
    "text": "Boost or bust for everyday runners?\nWe have seen that super shoes have benefits for elite runners, but what about the everyday runner? Figure 4 displays the average finishing time over the years for runners in the men’s and women’s division that are not a part of the elite field. In both divisions, the average finishing time increases over time, until around 2018 when it starts to drop off. We note that 2021 was another hot year, with the temperature being 24°C (75°F) at noon when many non-elite runners were still out on the course. Figure 4 also includes a bar chart of the number of the finishers by year. We see the field size growing steadily over time, aside from 2021 and 2022 when field sizes were smaller due to the COVID-19 pandemic. This could be a major reason for the rise in the average finishing time over the early 21st century—the popularity of running has been growing, and more runners, of all skill levels, are participating in the marathon. In Figure 4 we choose to denote the super-shoe era as beginning in 2019 for everyday runners. This is to account for the price tag of super shoes, which average roughly $250 a pair(Aciman 2024). Many runners, especially those that are not hyper-competitive, likely waited to purchase the shoes until they became more popular.\n\n\n\n\n\n\n\n\nFigure 4: Average Chicago Marathon finishing time by year for the non-elite women’s and men’s divisions. The vertical dashed line at 2019 represents the start of the super-shoe era for everyday runners. The total number of finishers each year, which steadily increases over time, is displayed by the light gray bar chart.\n\n\n\n\n\nPart of the hypothesis around super shoes for non-elite runners is that there is a tapering effect, meaning slower runners will not benefit as much from the shoes as faster runners. In order to better understand this, we run a quantile regression, which allows us to model the relationship between our predictor variables across different locations in the distribution of our response variable(Koenker and Hallock 2001). In the context of our data, we estimate how the effect of super shoes differs for faster and slower runners. For instance, we can estimate the effect of super-shoe era for the 60% quantile of finishing time (around 4 hours and 40 minutes), given our additional covariates participant age, participant sex, and raceday temperature at 9:00 am. For these models, we use data from 2014-2023. We do not have any information on participant training, and therefore have nothing to account for the confounding variable of the potential compositional shift in runners from 1996-2023. Using a smaller number of years for the model will remove much of the period of growth in field size, hopefully making the skill levels of those in the pre-super-shoe era comparable to runners in the super-shoe era. We run separate regressions for each quantile from 10% to 90% in increments of 5%.\nThe results of our regressions are shown in Figure 5. We see evidence of the hypothesized tapering effect, with the top 30% of runners, or those running sub-4 hour marathons, seeing the most gain from the super shoes. Given equivalent age, sex, and raceday weather, we would estimate these runners to complete the race in about nine minutes less than the equivalent runners in the pre-super-shoe era, on average. On the other hand, runners in higher quantiles of finishing time see less of a gain. For instance, those in the 85% quantile of finishing times, or those running around a 5.5 hour marathon, are estimated to see only about a 3 minute time reduction in the super-shoe era, given equivalent age, sex, and raceday weather.\n\n\n\n\n\n\n\n\nFigure 5: Estimate on the reduction in finishing time (minutes) for each quantile regression model. The corresponding finishing time of each quantile is plotted in gray. There is evidence that those finishing in 3-4 hours see greater benefits of the super shoes than those finishing in 5-6 hours."
  },
  {
    "objectID": "blog/rss/index.html#not-so-fast",
    "href": "blog/rss/index.html#not-so-fast",
    "title": "Sole searching: How super shoes have changed marathoning",
    "section": "Not so fast",
    "text": "Not so fast\nBefore hanging up the shoes on this analysis, we must acknowledge a few of its limitations. While we can control for characteristics such as the runners’ age, sex, and raceday weather, a key limitation to our analysis is that we do not have information on their training leading up to the marathon or their baseline fitness level. Different runners of the same age and sex will have very different finishing times based on their baseline fitness and how their training went. Additionally, it is possible that the training and baseline fitness for the runners as a whole look different in the pre- and post-super-shoe era, making these confounding variables. For instance, in the elite field, there have likely been advancements to training theory in recent years, or better recovery technology that allows athletes to get in a higher volume or intensity of training. This could be part of what is contributing to the average reduction in elite finishing times in the post-super-shoe era. On the everyday runner side, the composition of runners in the Chicago Marathon may have changed. In order to run the Chicago Marathon, runners can qualify by time, raise money for charity, or enter a lottery. In both 2024 and 2025, the Chicago Marathon made the qualifying standards more difficult. This provides evidence that in recent years, the number of runners meeting the time standards has increased. Given that the popularity of running has surged since the COVID-19 pandemic, this leveling up is likely not entirely due to super shoes. A second limitation is that we are assuming all runners are wearing super shoes in the super-shoe era. While this is a fair assumption in the elite field, this will not hold in the everyday runner field.\nThere is still plenty of research to be done to understand the impact of super shoes on marathon finishing times, especially as super shoe technology continues to evolve. However, our results show promise for the benefits of super shoes, especially for faster athletes striving to break a 2, 3, or 4 hour marathon barrier. So, if you are considering a marathon, just remember: “All you need is a pair of shoes”—but not all shoes are created equal."
  },
  {
    "objectID": "teaching.html#teaching-related-research",
    "href": "teaching.html#teaching-related-research",
    "title": "Teaching",
    "section": "Teaching-Related Research",
    "text": "Teaching-Related Research\n\nTeaching Data Cleaning and Wrangling with R’s data.table Package. Presented in collaboration with Sara Colando as part of Posters and Beyond at USCOTS 2025. This work was generously funded by NSF Award Abstract # 2303612.\nAnalyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models. Presented in collaboration with Sara Colando as part of the Research Satellite poster session at USCOTS 2025. Manuscript currently in progress (as of December 2025)."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "This page is a hodgepodge of blog posts and notes on statistics concepts that interest me. More to come with time :)\n\nStatistics\n\nRegression Recipes (December 20, 2025)\nSole searching: How super shoes have changed marathoning (August 15, 2025)\n\n\n\nEducation\n\nReflections from USCOTS 2025 (July 21, 2025)"
  },
  {
    "objectID": "blog/regression/regression.html",
    "href": "blog/regression/regression.html",
    "title": "Regression Recipes",
    "section": "",
    "text": "My cohortmate, Sara Colando, and I created these recipes in preparation for the Data Analysis Exam at the end of our first year. The vast majority of these materials are pulled from Alex Reinhart’s 36-707 lecture notes.\nShow code\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(ggeffects)\nlibrary(regressinator)\nlibrary(sandwich)\nlibrary(car)\nlibrary(splines)\nlibrary(DHARMa)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(mgcv)\nlibrary(skimr)\nlibrary(mvtnorm)\nlibrary(glmnet)"
  },
  {
    "objectID": "blog/regression/regression.html#linear-regression",
    "href": "blog/regression/regression.html#linear-regression",
    "title": "Regression Recipes",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nAssumptions\nFor the regression model \\(\\mathbf{Y} = \\mathbf{X}\\beta + e\\), we have assumptions:\n\nThe errors have mean 0: \\(\\mathbb{E}[e] = 0\\)\n\nThe error variance is constant: \\(\\text{Var}(e|\\mathbf{X}) = \\sigma^2I\\)\n\nThe errors are uncorrelated (data points are iid): \\(\\text{Var}(e|\\mathbf{X}) = \\sigma^2I\\).\n\nThe errors are normally distributed. With a large sample size and if you are not doing prediction, non-normality is not a serious concern.\n\n\nWe also assume \\(\\mathbf{X}\\) is fixed, not random. Assumptions 1 and 2 can be checked with a plot of residuals vs. fitted values, while Assumption 4 can be checked with a quantile-quantile plot of the standardized or studentized residuals.\n\n\nHow a Q-Q plot works: if there are \\(n\\) residuals, calculate the expected value of the minimum of \\(n\\) draws from a standard normal distribution, the expected value of the second-smallest of \\(n\\) draws from a standard normal distribution, etc. These are the “theoretical quantiles”. For each residual, we plot its actual value against the expected value. In the ideal case where every residual is equal to its expected value, this will result in a diagonal line.\n\nTypes of residuals\n\nStandardized residuals: raw residual/estimated standard deviation\nStudentized residuals: raw residual/estimated standard deviation without that specific point. This makes studentized residuals more helpful for identifying outliers.\n\nThe Studentized residuals have constant variance, so the scatter of residuals above and below 0 should be similar for different values of \\(U\\), where \\(U\\) is a covariate or the fitted values.\n\nPartial residuals: allow us to visualize the marginal effect of covariate \\(X_k\\) on \\(Y\\), conditional on the other predictors being included in the model.\n\n\nNot standardized and hence have unequal variance, but when the true relationship is nonlinear, partial residual plots may suggest what kind of transformation or additional terms could better fit the data.\n\n\n\n\nRecipe\n\nFit model (consider interactions, nonlinear patterns from EDA, confounding variables, transformations).\n\nPartial residuals (look for nonlinearity, adjust model as needed)\n\nPlot residuals against fitted values\n\n\n\nLook for heteroskedasticity; use sandwich estimator if present.\n\nDiagnostic for constant error variance, errors have mean 0.\n\n\n\nQQ plot to check for any gross deviations from normality\n\n\n\nDiagnostic for normally distributed errors.\n\nNon-normality is not a serious concern if sample size is large\n\n\n\nCheck for outliers impact on coefficients (Cook’s distance)\n\nCheck for collinearity issues (based on EDA, variance inflation factor)\n\nIf we included spline, polynomial terms, or interactions, conduct an F test\n\nIf we included spline, polynomial terms, or interactions create effect plot\n\nFor linear terms, interpret with test statistic, p-value, confidence interval\n\nIf sample size is small, make a statement about power\n\nLimitations/Conclusion\n\n\n\nunmeasured confounders\n\ndichotomized confounders\n\nunaccounted for correlation, lack of independence (spatial, repeated measured)\n\nnon-random missingness\n\nsample demographics compared to population of interest\n\nstatistical vs. practical significance\n\n\n\n\nInterpretation\n\n\nShow code\nlibrary(palmerpenguins)\npenguin_fit &lt;- lm(\n  bill_length_mm ~ flipper_length_mm + species + flipper_length_mm:species, data = penguins)\n\n# tidy(penguin_fit) |&gt;\n#   knitr::kable(col.names = c(\"Term\", \"Estimate\", \"SE\", \"t\", \"p\"), digits = 3)\n\ntbl_regression(penguin_fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nflipper_length_mm\n0.13\n0.07, 0.20\n&lt;0.001\n\n\nspecies\n\n\n\n\n\n\n\n\n    Adelie\n—\n—\n\n\n\n\n    Chinstrap\n-8.0\n-29, 13\n0.4\n\n\n    Gentoo\n-34\n-54, -15\n&lt;0.001\n\n\nflipper_length_mm * species\n\n\n\n\n\n\n\n\n    flipper_length_mm * Chinstrap\n0.09\n-0.02, 0.19\n0.10\n\n\n    flipper_length_mm * Gentoo\n0.18\n0.09, 0.28\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nInteraction: For Adelie penguins (the baseline level), the association between bill length and flipper length is \\(0.13\\) mm of bill length per millimeter of flipper length, on average. But for chinstrap penguins, the association is \\(0.13 + 0.09 = 0.22\\) mm of bill length per millimeter of flipper length, on average.\nTest for a factor coefficient: For a given flipper length, gentoo penguins have a smaller mean bill length \\((M=47.5\\) mm, \\(SD = 3.1)\\) than Adelie penguins \\((M=38.8\\) mm, \\(SD = 2.7)\\), \\(t(336) = -3.5, p = 0.001\\). (use tidy() and summarize to get means/sd).\nConfidence interval for a factor coefficient: For a given flipper length, gentoo penguin bills are shorter than Adelie penguin bills by an average of 34.3 mm (95% CI [15.01, 53.64]).\nTest for a slope: In Adelie penguins, there was a statistically significant association between flipper length and bill length, \\(\\hat\\beta = 0.13, t(336) = 4.17, p&lt;0.001\\).\nDescribe a slope: Among Adelie penguins, each additional millimeter of flipper length is associated with 0.13 millimeters of additional bill length, on average (95% CI [0.07, 0.2]).\n\nDrawing causal claims\nIf we are confident in our causal model and can control for the necessary confounders, we can estimate a chosen causal path, such as the causal relationship \\(X \\to Y\\). But we should make clear the limitations of our causal claims:\n\nIf our regression model is misspecified or otherwise incorrect, our estimates may be wrong.\n\nIf our causal model is missing important confounders, or we have measured some confounders incompletely or inaccurately, our estimates may include some bias from confounding.\n\nIf our data comes from a specific sample or subset of a population, the causal claims may not generalize beyond it.\n\n\n\n\n\nCode\n\nPredictor Effect Plots\nEffect plots allow us to vary one predictor while holding others fixed to visualize the relationships. Other predictors are set at their average values.\n\n\nShow code\npredict_response(penguin_fit,\n                 terms = c(\"flipper_length_mm\", \"species\")) |&gt;\n  plot() +\n  labs(x = \"Flipper length (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       title = \"Flipper length effect plot\")+\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\nShow code\n# change depending on value of body mass and flipper length\npenguin_fit_2 &lt;- lm(\n  bill_length_mm ~ flipper_length_mm + species +\n    flipper_length_mm:species + body_mass_g,\n  data = penguins\n)\n\npredict_response(penguin_fit_2,\n  terms = c(\"flipper_length_mm\", \"species\", \"body_mass_g [3000, 4000, 5000, 6000]\")) |&gt;\n  plot() +\n  labs(x = \"Flipper length (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       title = \"Flipper length effect plot\",\n       subtitle = \"By body mass (g)\")+\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs. fitted\n\n\nShow code\naugment(penguin_fit) |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(x = \"Fitted value\", y = \"Residual\")+\n  geom_hline(linetype = \"dashed\", color = \"red\", yintercept = 0)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nQuantile-Quantile Plot\n\n\nShow code\naugment(penguin_fit) |&gt;\n  ggplot(aes(sample = .std.resid)) +\n  geom_qq() +\n  geom_qq_line(color = \"navy\") +\n  labs(x = \"Theoretical quantiles\",\n       y = \"Observed quantiles\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nPartial Residuals\n\n\nShow code\npartial_residuals(penguin_fit) |&gt;\n  mutate(.predictor_name = case_when(.predictor_name == \"flipper_length_mm\" ~ \"Flipper length (mm)\")) %&gt;%\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(alpha=0.7) +\n  geom_line(aes(y = .predictor_effect), color = \"forestgreen\") +\n  geom_smooth(se = FALSE, size = 0.8) +\n  facet_wrap(vars(.predictor_name), scales = \"free\") +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\nOther considerations\n\nHeteroskedasticity\nIf heteroskedasticity means our estimator \\(\\widehat{\\text{Var}}(\\hat \\beta) = S^2 (\\mathbf X^T\\mathbf X)^{-1}\\) is incorrect, we can use the sandwich estimator.\n\nThe sandwich estimator performs poorly for small sample sizes \\((n \\leq 250)\\)  \nA recommended adaptation is HC3, which performs well even in small sample sizes.\n\n\n\nShow code\n#vcovHC(penguin_fit) # uses HC3 by default\n\nConfint(penguin_fit, vcov = vcovHC(penguin_fit))\n\n\nStandard errors computed by vcovHC(penguin_fit) \n\n\n                                       Estimate        2.5 %      97.5 %\n(Intercept)                         13.58713949   2.60847282  24.5658062\nflipper_length_mm                    0.13268633   0.07463215   0.1907405\nspeciesChinstrap                    -7.99376340 -39.43674282  23.4492160\nspeciesGentoo                      -34.32335066 -54.07194712 -14.5747542\nflipper_length_mm:speciesChinstrap   0.08812701  -0.07155252   0.2478065\nflipper_length_mm:speciesGentoo      0.18151798   0.08605716   0.2769788\n\n\n\n\nOutliers\nObservations with large errors can pull our \\(\\hat \\beta\\) away from \\(\\beta\\), causing high variance in estimates and estimated regression lines that fit the data poorly.\nObservations with high leverage have the potential to pull the fitted regression line to be close to them (they could fall on the regression line and not affect things at all). Instead we use Cook’s distance which measures the difference in \\(\\hat \\beta\\) when the observation is excluded from the fit. It can be interpreted as a distance that is rescaled proportional to the variance of \\(\\text{Var}(\\hat \\beta)\\), so the scale is consistent. We can look for values \\(D_i \\geq 1\\) in augment().\n\n\nCollinearity\nCollinearity is not a violation of assumptions because there is no assumptions on the structure of \\(\\mathbf{X}\\) aside from it being full rank.\nWhy it is bad and implications:\n\n\nCan create high covariance between components of \\(\\hat \\beta\\) and increases the variance of the predictions.\n\nIt can be numerically difficult to calculate the least squares estimates. Tiny rounding errors produce large differences in estimates.\n\n\nThis should only be an issue in extreme cases. If the computer can calculate \\(\\hat \\beta\\) and Var\\((\\hat \\beta)\\), the reported least squares estimates are the correct ones given the model and predictors. There will just be large CIs.\n\n\nWe must be careful about interpretation. \\(\\beta_1\\) is the change in the mean of \\(Y\\) when increasing \\(X_1\\) and keeping \\(X_2\\) fixed. If \\(X_1\\) and \\(X_2\\) are highly correlated, that may be very different than the change in the mean of \\(Y\\) when increasing \\(X_1\\) without holding \\(X_2\\) fixed.\n\n\nSummary: If we have collinearity, ask if we have chosen the correct predictors for the research question. If so, there is little we can do. If we are interested in prediction rather than coefficients, collinearity is a problem as it creates high prediction variance, so we might use penalization to reduce variance.\n\n\nPartially observed predictors\nIn the presence of measurement error, we may experience regression dilution (the slope is attenuated to 0 and \\(\\hat\\beta\\) is biased). We might have measurement error in a number of instances, for instance:\n\n\\(X_i\\) involves physical quantities that have to be measured with equipment that has inherent error\n\\(X_i\\) is a quantity that fluctuates over time but has only been observed at one time, and the relationship of interest is with its long-term average\n\\(X_i\\) is some variable that cannot be measured and instead are observing an easy-to-measure variable that is only correlated with it\n\\(X_i\\) is obtained by surveying people who may give inaccurate or misremembered answers\n\nDiscretization or dichotomization causes similar issues: we lose information about a predictor, biasing our estimate for its coefficient. If that predictor is correlated with other variables or is a confounding variable, our estimates for other predictors may also become biased."
  },
  {
    "objectID": "blog/regression/regression.html#generalized-linear-models",
    "href": "blog/regression/regression.html#generalized-linear-models",
    "title": "Regression Recipes",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLogistic\n\n\nBinomial\n\n\nPoisson"
  },
  {
    "objectID": "blog/regression/regression.html#addressing-nonlinearity",
    "href": "blog/regression/regression.html#addressing-nonlinearity",
    "title": "Regression Recipes",
    "section": "Addressing nonlinearity",
    "text": "Addressing nonlinearity\nRegression splines model relationships as being piecewise polynomial where we choose knots, which are the fixed points between which the function is polynomial. Helpful to choose knots at quantiles (adapts to where there is more data).\nUsing natural splines allows the slope to remain constant instead of diverging to \\(\\pm \\infty\\), making mild extrapolation less wild and erroneous than regression splines.\n\n\nShow code\nsat_spline_fit &lt;- lm(SAT ~ ns(Takers, knots = quantile(Takers, c(0.1, 0.3, 0.6, 0.9))) + \n                       Income + Expend + \n                       ns(Rank, knots = quantile(Rank, c(0.1, 0.3, 0.6, 0.9))), data = alaska)\n\nmod &lt;- lm(log(price2007) ~ ns(distance, knots = 0.76) + ns(walkscore, knots=36) + squarefeet, \n          data = rail_trails)\n\n\nUse effect plots to interpret shape of overall relationship. To understand if shape differs by group, do an interaction between the factor and regression spline regressors, which amounts to allowing the spline coefficients to vary by factor level. We can then conduct tests to determine whether interaction terms are statistically significant, which amount to tests of the null hypothesis that the different groups have identical relationships with \\(Y\\)."
  },
  {
    "objectID": "blog/regression/regression.html#prediction",
    "href": "blog/regression/regression.html#prediction",
    "title": "Regression Recipes",
    "section": "Prediction",
    "text": "Prediction\nIf our goal is prediction, we are interested in predicting with minimal error. The bias-variance tradeoff says total error is a combination of bias and variance—and the optimal combination might have nonzero bias.\nIf \\(p\\) (# predictors) is not much smaller than \\(n\\), we can expect Var(\\(\\hat \\beta\\)) to be large, causing high variance in the predictions.\nWe can use penalization: estimator is penalized according to tuning parameter \\(\\lambda\\) and coefficients are shrunk to zero.\n\nRecipe\nUse ridge for high dimension problems and collinear predictors. Use lasso for sparsity (many variables that may not be related to the outcome).\nSteps\n1. Split data into test and train test, trying as much as possible to avoid data leakage.\n2. Create model matrix of covariates for testing and training data.\n3. Cross validate to select \\(\\lambda\\) using training data.\n4. Fit model on full training data using cross-validated penalization parameter \\(\\lambda\\). Make sure to specify the correct distribution family (binomial or poisson).\n4b. If classification, pick threshold for positive vs. negative class where you choose value of ROC curve closest to top left.\n5. Predict on test data, calculate RMSE or MSE to assess overall predictive performance. RMSE interpretation: the average magnitude of the error in our predictions between the model and true value. Make a plot between true and predicted values.\n6. If classification, also compute sensitivity and specificity.\n\n\n\nRidge\nRidge is good for high dimensional problems (\\(p &gt; n\\)) and good for collinear predictors (collinear predictors results in high predictor variance). The ridge penalty reduces the covariance of \\(\\hat \\beta\\) by encouraging effects to be “shared” between collinear predictors, and as a result reduces the variance of predictions as well.\n\n\nShow code\nset.seed(707)\ntrain_ind &lt;- sample(seq_len(nrow(sparse_samp)), size = (floor(nrow(sparse_samp)*0.8)))\n\ntrain &lt;- sparse_samp[train_ind, ]\ntest &lt;- sparse_samp[-train_ind, ]\n\ntestx &lt;- model.matrix(~ . - 1 - y, data = test)\n\nx &lt;- model.matrix(~ . - 1 - y, data = train)\nridge_fit &lt;- glmnet(x, train$y, alpha = 0)\n\nset.seed(707)\n# 10-fold CV to return penalty parameter with optimal error\ncv_results &lt;- cv.glmnet(x, train$y, alpha = 0)\n#cv_results$lambda.min # value that minimizes the error\n\nbest_ridge &lt;- glmnet(x, train$y, alpha=0, lambda = cv_results$lambda.min)\n\n# get model coefficients \ncoefs &lt;- coef(best_ridge)\ncoefs[order(abs(coefs[, 1]), decreasing = TRUE)[1:5], ]\n\n\n(Intercept)          x5          x2         x52         x43 \n  3.2251478   0.6941613  -0.3754062   0.2099840   0.2081895 \n\n\nShow code\npred &lt;- predict(best_ridge, s=cv_results$lambda.min, newx = testx)\nresult &lt;- cbind(test, pred)\n\n\n\n\nLasso\nLasso’s most useful property is that depending on the value of \\(\\lambda\\), it forces many entries in \\(\\hat \\beta_{\\text{lasso}}\\) to be exactly zero. This property is known as sparsity, and there are many real-world populations where we expect the true \\(\\beta\\) to be sparse (GWAS).\n\n\nShow code\nx &lt;- model.matrix(~ . - 1 - y, data = sparse_samp)\nlasso_fit &lt;- glmnet(x, sparse_samp$y, alpha = 1)\nplot(lasso_fit)\n\n\n\n\n\n\n\n\n\nShow code\n# choose optimal lambda\ncv_results &lt;- cv.glmnet(x, sparse_samp$y, alpha = 1)\ncv_results$lambda.min\n\n\n[1] 0.1897752\n\n\nShow code\ncoefs &lt;- coef(lasso_fit, s = cv_results$lambda.min)\ncoefs[order(abs(coefs[, 1]), decreasing = TRUE)[1:15], ]\n\n\n          x5           x2  (Intercept)           x3           x4          x43 \n 7.661103561 -4.958242093  3.842723187  0.850862318  0.195829286  0.151896218 \n         x51          x50           x1          x41          x18          x68 \n-0.118786722  0.082173685  0.054938058 -0.044567855  0.026517904 -0.024152974 \n         x96          x24           x6 \n-0.009186286 -0.002792418  0.000000000 \n\n\nIf we’re using the lasso in a population we believe is truly sparse, the choice of nonzero coefficients may be of as much interest as the actual values of the nonzero estimates. Identifying the genes associated with a disease, for example, might be just as medically interesting as knowing the actual numerical association. If that is our goal, we want a procedure with model selection consistency. Lasso is model selection consistent if we know the right value of \\(\\lambda\\), which we choose through cross-validation (so this property is not guaranteed)."
  },
  {
    "objectID": "blog/regression/regression.html#logistic-regression",
    "href": "blog/regression/regression.html#logistic-regression",
    "title": "Regression Recipes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nWhen the outcome is binary, logistic regression models the response as \\(\\text{logit}(\\Pr(Y = 1 \\mid X = x)) = \\beta^\\top x,\\) or equivalently, \\[\\begin{align*}\n\\Pr(Y = 1 \\mid X = x) &= \\text{logit}^{-1} (\\beta^\\top x)\\\\\n\\text{odds}(Y = 1 \\mid X = x) &= \\exp(\\beta^\\top x) \\\\\n\\log(\\text{odds}(Y = 1 \\mid X = x)) &= \\beta^\\top x.\n\\end{align*}\\]\nWe consider \\(\\hat y_i\\) to be the predicted probability: \\(\\hat y_i = \\widehat{\\Pr}(Y = 1 \\mid X = x_i) = \\text{logit}^{-1}(\\hat \\beta^\\top x_i).\\)\n\nAssumptions\n\nThe log-odds is linearly related to the regressors: \\(\\log(\\text{odds}(Y = 1 \\mid X= x)) = \\beta^\\top x\\)\n\nThe observations \\(Y_i\\) are conditionally independent given the covariates \\(X_i\\).\n\n\n\n\nRecipe\n\nFit model (consider interactions, nonlinear patterns from EDA, confounding variables, transformations).\n\nPartial residuals (look for remaining nonlinearity with log-odds, adjust model as needed). Diagnostic for nonlinearity between regressors and log-odds.\n\nPlot randomized quantile residuals against each predictor. Plot QQ plot (uniform distribution). This is a second diagnostic for nonlinearity.\n\nOptionally, create a calibration plot.\n\nCheck for outliers impact on coefficients (Cook’s distance)\n\nCheck for collinearity issues (based on EDA, variance inflation factor)\n\nIf we included spline or polynomial terms, conduct a deviance test\n\nIf we included spline or polynomial terms (or interactions), create effect plots. Ensure it is on response scale, not log-odds\n\nFor linear terms, interpret with test statistic, p-value, confidence interval. Exponentiate!\n\nIf sample size is small, make a statement about power\n\nLimitations/Conclusion\n\n\n\nunmeasured confounders\n\ndichotomized confounders\n\nunaccounted for correlation, lack of independence (spatial, repeated measured)\n\nnon-random missingness\n\nsample demographics compared to population of interest\n\nstatistical vs. practical significance\n\nIf it is a cohort study, consider which type (prospective, retrospective, case-control)\n\n\n\nInterpretation\n\n\nShow code\nlibrary(MASS)\n# ggeffects requires factors, not logicals\nPima.tr$pregnancy &lt;- factor(\n  ifelse(Pima.tr$npreg &gt; 0, \"Yes\", \"No\")\n)\npima_fit &lt;- glm(type ~ pregnancy + bp, data = Pima.tr,\n                family = binomial())\n\n# tidy(pima_fit) |&gt;\n#   mutate(`exp(Estimate)` = exp(estimate)) |&gt; \n#   dplyr::select(term, estimate, `exp(Estimate)`, std.error, statistic, p.value) %&gt;%\n#   knitr::kable(col.names = c(\"Term\", \"Estimate\", \"exp(Estimate)\", \"SE\", \"z\", \"p\"), digits = 3)\n\ntbl_regression(pima_fit, intercept = TRUE, exponentiate = TRUE) |&gt;\n  as_gt() |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\n(Intercept)\n0.04\n0.00, 0.33\n0.003\n\n\npregnancy\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.63\n0.27, 1.47\n0.3  \n\n\nbp\n1.04\n1.01, 1.07\n0.004\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nPregnancy: Table 1 gives the results of the logistic regression fit. Women with prior pregnancies were less likely to have diabetes \\((\\text{OR} = 0.626, 95\\% CI = [0.27, 1.5])\\), but this result was not significantly significant (\\(z = -1.1, p=0.27\\)). A larger sample may be necessary to determine if a relationship exists in the population.\nPregnancy: Pregnancy changes the log-odds of diabetes, relative to the baseline value, by -0.47. The odds ratio is 0.626, i.e. pregnancy multiplies the odds of diabetes by 0.626.\nBlood pressure: Each unit of increase in blood pressure (measured in mm Hg) is associated with an increase in the log-odds of diabetes of 0.04, or a multiplication of the odds of diabetes by 1.04 (95% CI [1.01, 1.07]).\n\n\nCode\n\nPredictor Effect Plot\n\n\nShow code\npredict_response(pima_fit, terms = c(\"bp\", \"pregnancy\")) |&gt;\n  plot() +\n  labs(x = \"Diastolic blood pressure (mm Hg)\",\n       y = \"Probability of diabetes\",\n       color = \"Prior pregnancy?\", title = NULL)\n\n\n\n\n\n\n\n\n\n\n\nEDA\nEmpirical Link Plot: Look for a linear relationship between the predictor and log-odds\n\n\nShow code\nrate_by_bp &lt;- Pima.tr |&gt;\n  mutate(diabetes = (type == \"Yes\")) |&gt;\n  bin_by_quantile(bp, breaks = 10) |&gt;\n  summarize(\n    mean_bp = mean(bp),\n    prob = mean(diabetes),\n    log_odds = empirical_link(\n      diabetes,\n      family = binomial(link = \"logit\")),\n    n = n()\n  )\n\nrate_by_bp %&gt;%\n  ggplot(aes(x=mean_bp, y=log_odds))+\n  geom_point()+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nContingency Table\n\n\nShow code\nlibrary(alr4)\n#xtabs(~ outcome + myopathy, data = Downer)\nDowner |&gt;\n  filter(!is.na(myopathy), !is.na(outcome)) |&gt;\n  group_by(myopathy, outcome) |&gt;\n  summarize(n = n()) |&gt;\n  pivot_wider(names_from = myopathy, values_from = n) |&gt;\n  knitr::kable(col.names = c(\"Outcome\", \"No myopathy\", \"Myopathy\"))\n\n\n\n\n\nOutcome\nNo myopathy\nMyopathy\n\n\n\n\ndied\n78\n89\n\n\nsurvived\n49\n6\n\n\n\n\n\n\n\nResiduals vs fitted\nOrdinary residuals for logistic regression exhibit this sort of grouping structure, as do standardized residuals (type = \"pearson\") and deviance residuals (type = \"deviance\"). These plots are not useful for interpretation.\n\n\nShow code\naugment(pima_fit) %&gt;%\n  ggplot(aes(x=bp, y=.resid))+\n  geom_point()+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nPartial residuals\nPartial residuals illustrate the shape of the relationship between the predictor and log-odds, subject to the accuracy of the linearization. The shape is locally accurate, but may not be so accurate if the response probabilities cover a wide range over which the logit is not very linear.\n\n\nShow code\npartial_residuals(pima_fit) |&gt;\n  # mutate(.predictor_name = case_when(.predictor_name == \"x1\" ~ \"Var 1\", \n  #                                    .predictor_name == \"x2\" ~ \"Var 2\")) %&gt;%\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(alpha=0.7) +\n  geom_line(aes(y = .predictor_effect), color = \"forestgreen\") +\n  geom_smooth(se = FALSE, size = 0.8) +\n  facet_wrap(vars(.predictor_name), scales = \"free\") +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nRandomized quantile residuals\nIdea: transform residuals to have a continuous, not discrete, distribution. This requires randomization, so the resulting residuals are called randomized quantile residuals. When the model is correct, the randomized quantile residuals are Uniform(0,1).\nWe can use plots of randomized quantile residuals against predictors and fitted values, as with any other residuals, to check the fit of our model. It is also useful to check that their distribution is indeed uniform (e.g. with a Q-Q plot). When the model is incorrectly specified, the distribution will not be uniform, producing patterns on the residual plots that can be interpreted (e.g. we can find nonlinearity that needs to be accounted for).\n\n\nShow code\n# get either using dHarma package\ndh &lt;- simulateResiduals(pima_fit)\n\npima_aug &lt;- augment(pima_fit) |&gt;\n  mutate(.quantile.resid = residuals(dh))\n\n# or, equivalently:\npima_aug &lt;- augment_quantile(pima_fit)\n\npima_aug %&gt;%\n  ggplot(aes(x=bp, y = .quantile.resid))+\n  geom_point()+\n  geom_smooth()+\n  geom_hline(yintercept= 0.5, linetype = \"dashed\", color = \"red\")+\n  theme_bw()+\n  labs(x=\"Blood pressure\", y=\"Randomized quantile residual\")\n\n\n\n\n\n\n\n\n\n\n\nQuantile-Quantile Plot\nUse to check whether the distribution of the randomized quantile residuals is uniform.\n\n\nShow code\npima_aug |&gt;\n  ggplot(aes(sample = .quantile.resid)) +\n  geom_qq(distribution = stats::qunif) +\n  geom_qq_line(distribution = stats::qunif) +\n  labs(x = \"Theoretical quantiles\",\n       y = \"Observed quantiles\",\n       title = \"Q-Q Plot: Diabetes\")+\n  theme_classic()+\n  theme(plot.title.positon = \"plot\",\n        plot.title = element_text(size = 9), \n        axis.title = element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n\nCalibration Plot\nA calibrated model is one whose predicted probabilities are accurate: if it predicts \\(\\text{Pr}(Y=1|X=x) = 0.8\\) for a particular \\(x\\), and we observe many responses with that \\(x\\), about 80% of those responses should be 1. A calibration plot allows us to approximate this.\n\n\nShow code\ncalibration_data &lt;- data.frame(\n  x = predict(pima_fit, type = \"response\"),\n  y = ifelse(Pima.tr$type == \"Yes\", 1, 0)\n)\n\nggplot(calibration_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted probability\", y = \"Observed fraction\") +\n  ylim(0, 1)+\n  theme_bw()"
  },
  {
    "objectID": "blog/regression/regression.html#binomial",
    "href": "blog/regression/regression.html#binomial",
    "title": "Regression Recipes",
    "section": "Binomial",
    "text": "Binomial\nBinomial regression is used with binary outcomes where there is a fixed and known total number of trials. Each observation consists of a number of successes and a total number of trials; the number of trials may differ between observations.\n\nAssumptions\n\nThe observations are conditionally independent, given \\(X\\).\n\nThe response variable follows the binomial distribution.\n\nThe mean of the response is related to the predictors through the choen link function; we want a linear relationship between the predictor and the log-odds of the rate \\(\\log\\left(\\frac{\\text{rate}}{1-\\text{rate}}\\right)\\).\n\n\n\n\nRecipe\n\nFit model (consider interactions, nonlinear patterns from EDA, confounding variables, transformations).\n\nPartial residuals (look for remaining nonlinearity with log-odds, adjust model as needed). Diagnostic for nonlinearity between regressors and log-odds.\n\nPlot randomized quantile residuals against each predictor. Plot QQ plot (uniform distribution). This is a second diagnostic for nonlinearity.\n\nCheck for overdispersion in the QQ plot. If there is evidence of overdisperson, go to quasi-land.\n\nCheck for outliers impact on coefficients (Cook’s distance)\n\nCheck for collinearity issues (based on EDA, variance inflation factor)\n\nIf we included spline or polynomial terms, conduct a deviance test\n\nIf we included spline or polynomial terms (or interactions), create effect plots. Ensure it is on response scale, not log-odds\n\nFor linear terms, interpret with test statistic, p-value, confidence interval. Exponentiate to get odds ratio!\n\nIf sample size is small, make a statement about power\n\nLimitations/Conclusion\n\n\n\nunmeasured confounders\n\ndichotomized confounders\n\nunaccounted for correlation, lack of independence between trials (success in one increases probability of success in another, spatio-temporal correlation)\n\nnon-random missingness\n\nsample demographics compared to population of interest\n\nstatistical vs. practical significance\n\nIf it is a cohort study, consider which type (prospective, retrospective, case-control)\n\n\n\n\nInterpretations\nFor this example, we have data based on two censuses of nesting birds taken on the Krunnit Islands archipelago in 1949 and 1959. Each row is one island in the archipelago, and we have the number of bird species present in 1949 (AtRisk) and the number of those that were extinct from the island by 1959 (Extinct). Theories predict that that extinction rates (Extinct / AtRisk) are higher in small, isolated communities, and lower in larger communities.\n\n\nShow code\nlibrary(Sleuth3)\nisland_fit &lt;- glm(cbind(Extinct, AtRisk-Extinct)~log10(Area), data=case2101, family = binomial())\n\ntbl_regression(island_fit, exponentiate = TRUE) |&gt;\n  as_gt() |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nlog10(Area)\n0.50\n0.39, 0.64\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nInterpretation: A 10-fold increase in the island size is associated with the odds of extinction being multiplied by 0.5 (95% CI [0.39, 0.64]).\n\n\nCode\n\nEDA\nCheck if predictor is linearly related to log-odds\n\n\nShow code\ng1 &lt;- case2101 %&gt;%\n  mutate(rate = Extinct/AtRisk) %&gt;%\n  ggplot(aes(y=rate, x=Area))+\n    geom_point()+\n  theme_classic()\ng2 &lt;- case2101 %&gt;%\n  mutate(rate = Extinct/AtRisk) %&gt;%\n  mutate(log_odds = log(rate/(1-rate))) %&gt;%\n  ggplot(aes(y=log_odds, x=log10(Area)))+\n    geom_point()+\n  theme_classic()\ng1|g2\n\n\n\n\n\n\n\n\n\n\n\nPartial Residuals\n\n\nShow code\npartial_residuals(island_fit) |&gt;\n  # mutate(.predictor_name = case_when(.predictor_name == \"x1\" ~ \"Var 1\", \n  #                                    .predictor_name == \"x2\" ~ \"Var 2\")) %&gt;%\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(alpha=0.7) +\n  geom_line(aes(y = .predictor_effect), color = \"forestgreen\") +\n  geom_smooth(se = TRUE, size = 0.8) +\n  facet_wrap(vars(.predictor_name), scales = \"free\") +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nRandomized Quantile Residuals\n\n\nShow code\naugment_quantile(island_fit) %&gt;%\n  ggplot(aes(x=`log10(Area)`, y = .quantile.resid))+\n  geom_point()+\n  theme_bw()+\n  labs(x=\"log(10) Area\", y=\"Randomized quantile residual\")+\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\nQQ Plot\n\n\nShow code\naugment_quantile(island_fit) |&gt;\n  ggplot(aes(sample = .quantile.resid)) +\n  geom_qq(distribution = stats::qunif) +\n  geom_qq_line(distribution = stats::qunif) +\n  labs(x = \"Theoretical quantiles\",\n       y = \"Observed quantiles\",\n       title = \"Q-Q Plot\")+\n  theme_classic()+\n  theme(plot.title.positon = \"plot\",\n        plot.title = element_text(size = 9), \n        axis.title = element_text(size = 9))"
  },
  {
    "objectID": "blog/regression/regression.html#poisson",
    "href": "blog/regression/regression.html#poisson",
    "title": "Regression Recipes",
    "section": "Poisson",
    "text": "Poisson\nIf a certain event (counts) occurs at a fixed rate and the events are independent, then the count of events over a fixed period of time will be Poisson-distributed.\n\nAssumptions\n\nThe observations are conditionally independent, given \\(X\\).\n\nThe response variable follows the poisson distribution.\n\nThe mean of the response is related to the predictors through the chosen link function: we want a linear relationship between the predictor and the log of the outcome (either rate or count).\n\n\n\n\nRecipe\n\nFit model (consider interactions, nonlinear patterns from EDA, confounding variables, transformations).\n1b. Consider whether an offset is needed. Used in cases where the observations were recorded for different population sizes or time period lengths.\n\nPartial residuals (look for remaining nonlinearity with log-outcome, adjust model as needed). Diagnostic for nonlinearity between regressors and log-outcome.\n\nPlot randomized quantile residuals against each predictor. Plot QQ plot (uniform distribution). This is a second diagnostic for nonlinearity.  \nCheck for overdispersion in the QQ plot. If there is evidence of overdisperson, go to quasi-land.\n\nCheck for outliers impact on coefficients (Cook’s distance)\n\nCheck for collinearity issues (based on EDA, variance inflation factor)\n\nIf we included splines, polynomial, or interactions terms, conduct a deviance test\n\nIf we included spline, polynomial, (or interactions), create effect plots. Ensure it is on response scale, not log-scale.  \nFor linear terms, interpret with test statistic, p-value, confidence interval. Exponentiate!\n\nIf sample size is small, make a statement about power  \nLimitations/Conclusion\n\n\n\nunmeasured confounders\n\ndichotomized confounders\n\nunaccounted for correlation, dependence of events, non-fixed rate of occurrence (success in one time period increases probability of success in another, spatio-temporal correlation)\n\nHaving insufficient data to included an offset term.\nunreasonable approximation of count data as Poisson (e.g. count assign nontrivial probability to impossible counts)\nnon-random missingness\n\nsample demographics compared to population of interest\n\nstatistical vs. practical significance\n\nIf it is a cohort study, consider which type (prospective, retrospective, case-control)\n\n\n\n\nInterpretations\n\n\nCode"
  },
  {
    "objectID": "blog/regression/regression.html#quasi-glm",
    "href": "blog/regression/regression.html#quasi-glm",
    "title": "Regression Recipes",
    "section": "Quasi-GLM",
    "text": "Quasi-GLM\nOverdispersion can happen when there is more variance in \\(Y\\) than the response distribution would predict. Can happen for several reasons:\n\n\\(X\\) is not sufficient. The fundamental assumption of a GLM is that each observation \\(Y\\) is independent and identically distributed given \\(X\\), but maybe there are other factors associated with \\(\\mathbb{E}[Y]\\) that we did not observe. For a fixed value of \\(X\\), these other factors may still vary and cause additional variation in the observed values of \\(Y\\).\nThere may be correlation we did not account for. For instance, a Binomial(\\(n\\), \\(p\\)) distribution assumes there are \\(n\\) independent trials—but what if the trials are dependent, and success in one is correlated with increased success rates in the others?\n\nWe can use quasi-likelihood estimation: we are now fudging it and using only the mean and variance of the distribution, and allowing the variance to be different from what an exponential dispersion family distribution would have. We are only sort of using a proper likelihood function. R supports quasi-likelihood estimation for both by using the quasibinomial() and quasipoisson() families in glm().\n\nRecipe\n\nFit the quasi-poisson model (same structure as poisson, just now quasi).\n\nConfirm goodness of fit with partial residuals plot (should be the same as above, perhaps just wider confidence band).\n\nBecause we no longer have a proper likelihood function, we cannot check our model with randomized quantile residuals.\n\nCheck for outliers impact on coefficients (Cook’s distance)\n\nCheck for collinearity issues (based on EDA, variance inflation factor)\n\nWe cannot conduct deviance tests for splines and polynomials.\n\nWe can still create effect plots with example interpretations including confidence intervals. Ensure it is on the response scale, not log-odds (binomial) or log (poisson).\n\nFor linear terms, interpret with test statistic, p-value, confidence interval. Exponentiate! The point estimate should be the same as non-quasi, but the confidence interval will be larger.\n\nIf sample size is small, make a statement about power.  \nLimitations/Conclusion\n\n\n\nunmeasured confounders\n\ndichotomized confounders\n\nunaccounted for correlation, dependence of events, non-fixed rate of occurrence (success in one time period increases probability of success in another, spatio-temporal correlation)\n\nHaving insufficient data to included an offset term.\nunreasonable approximation of count data as Poisson (e.g. count assign nontrivial probability to impossible counts)\n\ncannot test for significance of spline/polynomial terms.\n\nnon-random missingness\n\nsample demographics compared to population of interest\n\nstatistical vs. practical significance\n\nIf it is a cohort study, consider which type (prospective, retrospective, case-control)\n\n\n\n\nExample\nOverdispersed seeds: We get the same point estimates, but larger confidence intervals and higher p-values.\n\n\nShow code\nlibrary(agridat)\ndata(\"crowder.seeds\")\nseed_quasi &lt;- glm(cbind(germ, n - germ) ~\n                    extract * gen,\n                  data = crowder.seeds, family = quasibinomial())\n\ntbl_regression(seed_quasi, exponentiate = T) |&gt;\n  as_gt() |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nextract\n\n\n\n\n\n\n\n\n    bean\n—\n—\n\n\n\n\n    cucumber\n1.72\n0.88, 3.37\n0.13 \n\n\ngen\n\n\n\n\n\n\n\n\n    O73\n—\n—\n\n\n\n\n    O75\n0.86\n0.48, 1.58\n0.6  \n\n\nextract * gen\n\n\n\n\n\n\n\n\n    cucumber * O75\n2.18\n0.96, 4.94\n0.080\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nOverdispersion can be observed from a QQ plot, as well as by checking whether the overdispersion factor is greater than 1.\n\n\nShow code\nsummary(seed_quasi)$dispersion\n\n\n[1] 1.861832"
  },
  {
    "objectID": "blog/regression/regression.html#inference",
    "href": "blog/regression/regression.html#inference",
    "title": "Regression Recipes",
    "section": "Inference",
    "text": "Inference\n\nF-test\nOften used for testing null that\n\n\na relationship is linear rather than quadratic or cubic\n\na continuous predictor’s relationship with the response is identical regardless of the level of a factor predictor with several levels\n\nsimply testing null that several specific predictors are not associated with the response\n\n\nPartial \\(F\\) test: compares a full model (with \\(\\beta\\) unconstrained) against a reduced model, and \\(q\\) being the difference in the number of parameters between these two models.\nThe \\(F\\) statistic is exactly \\(F\\)-distributed when the model is correct and errors are normally distributed with common variance. The \\(F\\) test is, however, considered to be robust to nonnormality, meaning that its null distribution will still be approximately \\(F\\) even when the errors are not normal.\n\nExample Interpretation\n\n\nShow code\nmod &lt;- lm(log(price2007) ~ ns(distance, knots = 0.76) + ns(walkscore, knots=36) + squarefeet, \n          data = rail_trails)\nmod_nodist &lt;- lm(log(price2007) ~ ns(walkscore, knots=36) + squarefeet, \n          data = rail_trails)\nf_test &lt;- anova(mod_nodist, mod)\n\n\nThis relationship is unlikely to be significant, as an F-test comparing our model with a reduced model fit solely with square footage and the spline on walkability does not provide evidence that distance is associated with estimated home value \\((F(2, 98) = 0.201, p = 0.818)\\).\n\n\n\nDeviance Test\nSame thing as an F-test, but for GLMS.\nSuppose we have two varieties of seeds and two types of extract the seeds can be treated with, and we want to figure out which combination of seed and extract leads to the highest rate of germination of the seeds.\n\n\nShow code\nseed_fit_1 &lt;- glm(cbind(germ, n - germ) ~\n                    extract + gen,\n                  data = crowder.seeds, family = binomial())\nseed_fit_2 &lt;- glm(cbind(germ, n - germ) ~\n                    extract * gen,\n                  data = crowder.seeds, family = binomial())\n\nseed_test &lt;- anova(seed_fit_1, seed_fit_2, test = \"Chisq\")\nseed_test\n\n\nAnalysis of Deviance Table\n\nModel 1: cbind(germ, n - germ) ~ extract + gen\nModel 2: cbind(germ, n - germ) ~ extract * gen\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1        18     39.686                       \n2        17     33.278  1   6.4081  0.01136 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA deviance test shows that the extract effect differs by seed type, \\(\\chi^2(1) = 6.41, p=0.011\\).\nAnother example interpretation: The null hypothesis is that the coefficients for age and glucose level are exactly zero. We reject the null hypothesis and conclude the coefficients are not both zero, \\(\\chi^2(2) = 52, p=5.03 \\times 10^{-12}\\)."
  },
  {
    "objectID": "blog/regression/regression.html#binomial-regression",
    "href": "blog/regression/regression.html#binomial-regression",
    "title": "Regression Recipes",
    "section": "Binomial Regression",
    "text": "Binomial Regression\nBinomial regression is used with binary outcomes where there is a fixed and known total number of trials. Each observation consists of a number of successes and a total number of trials; the number of trials may differ between observations. The model can be specified as \\[n_i Y_i \\mid X_i = x_i \\sim \\text{Binomial}\\left(n_i, g^{-1}(\\beta^\\top x_i)\\right),\\] where \\(g\\) is the link function (logit).\n\nAssumptions\n\nThe observations are conditionally independent, given \\(X\\).\n\nThe response variable follows the binomial distribution.\n\nThe mean of the response is related to the predictors through the choen link function; we want a linear relationship between the predictor and the log-odds of the rate \\(\\log\\left(\\frac{\\text{rate}}{1-\\text{rate}}\\right)\\).\n\n\n\n\nRecipe\n\nFit model (consider interactions, nonlinear patterns from EDA, confounding variables, transformations).\n\nPartial residuals (look for remaining nonlinearity with log-odds, adjust model as needed). Diagnostic for nonlinearity between regressors and log-odds.\n\nPlot randomized quantile residuals against each predictor. Plot QQ plot (uniform distribution). This is a second diagnostic for nonlinearity.\n\nCheck for overdispersion in the QQ plot. If there is evidence of overdisperson, go to quasi-land.\n\nCheck for outliers impact on coefficients (Cook’s distance)\n\nCheck for collinearity issues (based on EDA, variance inflation factor)\n\nIf we included spline or polynomial terms, conduct a deviance test\n\nIf we included spline or polynomial terms (or interactions), create effect plots. Ensure it is on response scale, not log-odds\n\nFor linear terms, interpret with test statistic, p-value, confidence interval. Exponentiate to get odds ratio!\n\nIf sample size is small, make a statement about power\n\nLimitations/Conclusion\n\n\n\nunmeasured confounders\n\ndichotomized confounders\n\nunaccounted for correlation, lack of independence between trials (success in one increases probability of success in another, spatio-temporal correlation)\n\nnon-random missingness\n\nsample demographics compared to population of interest\n\nstatistical vs. practical significance\n\nIf it is a cohort study, consider which type (prospective, retrospective, case-control)\n\n\n\n\nInterpretations\nFor this example, we have data based on two censuses of nesting birds taken on the Krunnit Islands archipelago in 1949 and 1959. Each row is one island in the archipelago, and we have the number of bird species present in 1949 (AtRisk) and the number of those that were extinct from the island by 1959 (Extinct). Theories predict that that extinction rates (Extinct / AtRisk) are higher in small, isolated communities, and lower in larger communities.\n\n\nShow code\nlibrary(Sleuth3)\nisland_fit &lt;- glm(cbind(Extinct, AtRisk-Extinct)~log10(Area), data=case2101, family = binomial())\n\ntbl_regression(island_fit, exponentiate = TRUE) |&gt;\n  as_gt() |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nlog10(Area)\n0.50\n0.39, 0.64\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nInterpretation: A 10-fold increase in the island size is associated with the odds of extinction being multiplied by 0.5 (95% CI [0.39, 0.64]).\n\n\nCode\n\nEDA\nCheck if predictor is linearly related to log-odds\n\n\nShow code\ng1 &lt;- case2101 %&gt;%\n  mutate(rate = Extinct/AtRisk) %&gt;%\n  ggplot(aes(y=rate, x=Area))+\n    geom_point()+\n  theme_classic()\ng2 &lt;- case2101 %&gt;%\n  mutate(rate = Extinct/AtRisk) %&gt;%\n  mutate(log_odds = log(rate/(1-rate))) %&gt;%\n  ggplot(aes(y=log_odds, x=log10(Area)))+\n    geom_point()+\n  theme_classic()\ng1|g2\n\n\n\n\n\n\n\n\n\n\n\nPartial Residuals\n\n\nShow code\npartial_residuals(island_fit) |&gt;\n  # mutate(.predictor_name = case_when(.predictor_name == \"x1\" ~ \"Var 1\", \n  #                                    .predictor_name == \"x2\" ~ \"Var 2\")) %&gt;%\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(alpha=0.7) +\n  geom_line(aes(y = .predictor_effect), color = \"forestgreen\") +\n  geom_smooth(se = TRUE, size = 0.8) +\n  facet_wrap(vars(.predictor_name), scales = \"free\") +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nRandomized Quantile Residuals\n\n\nShow code\naugment_quantile(island_fit) %&gt;%\n  ggplot(aes(x=`log10(Area)`, y = .quantile.resid))+\n  geom_point()+\n  theme_bw()+\n  labs(x=\"log(10) Area\", y=\"Randomized quantile residual\")+\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\nQQ Plot\n\n\nShow code\naugment_quantile(island_fit) |&gt;\n  ggplot(aes(sample = .quantile.resid)) +\n  geom_qq(distribution = stats::qunif) +\n  geom_qq_line(distribution = stats::qunif) +\n  labs(x = \"Theoretical quantiles\",\n       y = \"Observed quantiles\",\n       title = \"Q-Q Plot\")+\n  theme_classic()+\n  theme(plot.title.positon = \"plot\",\n        plot.title = element_text(size = 9), \n        axis.title = element_text(size = 9))"
  },
  {
    "objectID": "blog/regression/regression.html#poisson-regression",
    "href": "blog/regression/regression.html#poisson-regression",
    "title": "Regression Recipes",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nIf a certain event (counts) occurs at a fixed rate and the events are independent, then the count of events over a fixed period of time will be Poisson-distributed. The model can be specified as \\[Y \\mid X = x \\sim \\text{Poisson}(\\exp(\\beta_1 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3))\\]\n\nAssumptions\n\nThe observations are conditionally independent, given \\(X\\).\n\nThe response variable follows the poisson distribution.\n\nThe mean of the response is related to the predictors through the chosen link function: we want a linear relationship between the predictor and the log of the outcome (either rate or count).\n\n\n\n\nRecipe\n\nFit model (consider interactions, nonlinear patterns from EDA, confounding variables, transformations).\n1b. Consider whether an offset is needed. Used in cases where the observations were recorded for different population sizes or time period lengths.\n\nPartial residuals (look for remaining nonlinearity with log-outcome, adjust model as needed). Diagnostic for nonlinearity between regressors and log-outcome.\n\nPlot randomized quantile residuals against each predictor. Plot QQ plot (uniform distribution). This is a second diagnostic for nonlinearity.  \nCheck for overdispersion in the QQ plot. If there is evidence of overdisperson, go to quasi-land.\n\nCheck for outliers impact on coefficients (Cook’s distance)\n\nCheck for collinearity issues (based on EDA, variance inflation factor)\n\nIf we included splines, polynomial, or interactions terms, conduct a deviance test\n\nIf we included spline, polynomial, (or interactions), create effect plots. Ensure it is on response scale, not log-scale.  \nFor linear terms, interpret with test statistic, p-value, confidence interval. Exponentiate!\n\nIf sample size is small, make a statement about power  \nLimitations/Conclusion\n\n\n\nunmeasured confounders\n\ndichotomized confounders\n\nunaccounted for correlation, dependence of events, non-fixed rate of occurrence (success in one time period increases probability of success in another, spatio-temporal correlation)\n\nHaving insufficient data to included an offset term.\nunreasonable approximation of count data as Poisson (e.g. count assign nontrivial probability to impossible counts)\nnon-random missingness\n\nsample demographics compared to population of interest\n\nstatistical vs. practical significance\n\nIf it is a cohort study, consider which type (prospective, retrospective, case-control)\n\n\n\n\nInterpretations\nFor this example, we have data on the number of ant species observed in 64-square-meter areas in various bogs and forests in New England.\n\n\nShow code\nants &lt;- read.csv(\"ants.csv\")\nants_fit &lt;- glm(Srich ~ Latitude + Elevation + Habitat, data = ants,\n                family = poisson()) # Srich: count of ants in square area\n\ntbl_regression(ants_fit, exponentiate = T) |&gt;\n  as_gt() |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nLatitude\n0.79\n0.70, 0.89\n&lt;0.001\n\n\nElevation\n1.00\n1.00, 1.00\n 0.002\n\n\nHabitat\n\n\n\n\n\n\n\n\n    Bog\n—\n—\n\n\n\n\n    Forest\n1.89\n1.50, 2.39\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nIntepretation: One degree of latitude is associated with the mean number of ants being multiplied by 0.79 [95% CI(0.7, 0.89)].\n\n\nOffset\n\n\nShow code\nsmokers &lt;- read_csv(\"smokers.csv\")\nhead(smokers)\n\n\n# A tibble: 6 × 4\n    age deaths    py smoke\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1    40     32 52407 yes  \n2    40      2 18790 no   \n3    50    104 43248 yes  \n4    50     12 10673 no   \n5    60    206 28612 yes  \n6    60     28  5710 no   \n\n\nThe count of a particular unit might vary based on population or amount of time. In this case, we can use an offset. For example, Doll and Hill (1954) follow a number of smokers and non-smokers for many years and observed their death rates and causes of death.\nWe could choose to model the number of deaths as approximately Poisson, given the death rate, including \\(P\\) as an offset term in order to account for the different group sizes. The choice of Poisson is approximate, because there is an upper bound to the number of deaths in any age bucket based on the number of people in the study in that age bucket, but as long as the death rate is low, the Poisson will assign very little probability to impossible death numbers.\n\n\nShow code\nsmoke_fit &lt;- glm(deaths ~ (age + I(age^2)) * smoke, \n                 offset = log(py), # include with coefficient 1\n                 data = smokers, family = poisson(link = \"log\"))\n\npredict_response(smoke_fit, terms = c(\"age\", \"smoke\")) |&gt;\n  plot() +\n  labs(x = \"Age (years)\",\n       y = \"Death rate\",\n       title = \"Predicted counts of deaths\",\n       color = \"Smoker?\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nEDA\nCheck to see if predictor is linearly related with log of the rate. We see that the relationship between age and log(death proportion) is non-linear and varies with smoking status.\n\n\nShow code\na &lt;- smokers %&gt;%\n  mutate(deaths_py = deaths/py) %&gt;%\n  ggplot(aes(x=age, y=deaths_py))+\n  geom_point(aes(color = smoke, shape = smoke))+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\nb &lt;- smokers %&gt;%\n  mutate(deaths_py_log = log10(deaths/py)) %&gt;%\n  ggplot(aes(x=age, y=deaths_py_log))+\n  geom_point(aes(color = smoke, shape = smoke))+\n  theme_minimal()+\n  theme(legend.position = \"bottom\")\n\na+b\n\n\n\n\n\n\n\n\n\n\n\nPartial Residuals\n\n\nShow code\npartial_residuals(ants_fit) |&gt;\n  # mutate(.predictor_name = case_when(.predictor_name == \"x1\" ~ \"Var 1\", \n  #                                    .predictor_name == \"x2\" ~ \"Var 2\")) %&gt;%\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(alpha=0.7) +\n  geom_line(aes(y = .predictor_effect), color = \"forestgreen\") +\n  geom_smooth(se = TRUE, size = 0.8) +\n  facet_wrap(vars(.predictor_name), scales = \"free\") +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nRandomized Quantile Residuals\n\n\nShow code\naugment_quantile(ants_fit) %&gt;%\n  ggplot(aes(x=`Elevation`, y = .quantile.resid))+\n  geom_point()+\n  theme_bw()+\n  labs(x=\"Elevation\", y=\"Randomized quantile residual\")+\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\nQQ Plot\n\n\nShow code\naugment_quantile(ants_fit) |&gt;\n  ggplot(aes(sample = .quantile.resid)) +\n  geom_qq(distribution = stats::qunif) +\n  geom_qq_line(distribution = stats::qunif) +\n  labs(x = \"Theoretical quantiles\",\n       y = \"Observed quantiles\",\n       title = \"Q-Q Plot\")+\n  theme_classic()+\n  theme(plot.title.positon = \"plot\",\n        plot.title = element_text(size = 9), \n        axis.title = element_text(size = 9))"
  }
]